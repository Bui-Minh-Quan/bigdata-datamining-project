{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "249853b0",
   "metadata": {},
   "source": [
    "## Import necessary libraries and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0fca505",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f41fdb",
   "metadata": {},
   "source": [
    "## Clean and Process Fireant News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e4ac669",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>postID</th>\n",
       "      <th>date</th>\n",
       "      <th>userid</th>\n",
       "      <th>username</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>newsType</th>\n",
       "      <th>postGroupName</th>\n",
       "      <th>postSourceName</th>\n",
       "      <th>postSourceUrl</th>\n",
       "      <th>originalContent</th>\n",
       "      <th>link</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>totalLikes</th>\n",
       "      <th>totalReplies</th>\n",
       "      <th>totalShares</th>\n",
       "      <th>totalImages</th>\n",
       "      <th>replyToPostID</th>\n",
       "      <th>referToPostID</th>\n",
       "      <th>taggedSymbols</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3751931</td>\n",
       "      <td>2021-10-31T21:22:00+07:00</td>\n",
       "      <td>266ed7a4-0c22-4683-86d3-dfe616343731</td>\n",
       "      <td>Mister Mạnh</td>\n",
       "      <td>Lâm sản, thủy sản nỗ lực hồi phục sau giãn cách</td>\n",
       "      <td>Ngành nông nghiệp trong tháng 10/2021 chứng ki...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Kinh tế</td>\n",
       "      <td>{VnEconomy - Nguồn không hợp lệ}</td>\n",
       "      <td>https://vneconomy.vn/</td>\n",
       "      <td>&lt;p&gt;&lt;strong&gt;Ng&amp;agrave;nh n&amp;ocirc;ng nghiệp tron...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    postID                       date                                userid  \\\n",
       "0  3751931  2021-10-31T21:22:00+07:00  266ed7a4-0c22-4683-86d3-dfe616343731   \n",
       "\n",
       "      username                                            title  \\\n",
       "0  Mister Mạnh  Lâm sản, thủy sản nỗ lực hồi phục sau giãn cách   \n",
       "\n",
       "                                         description newsType postGroupName  \\\n",
       "0  Ngành nông nghiệp trong tháng 10/2021 chứng ki...      NaN       Kinh tế   \n",
       "\n",
       "                     postSourceName          postSourceUrl  \\\n",
       "0  {VnEconomy - Nguồn không hợp lệ}  https://vneconomy.vn/   \n",
       "\n",
       "                                     originalContent link sentiment  \\\n",
       "0  <p><strong>Ng&agrave;nh n&ocirc;ng nghiệp tron...  NaN         0   \n",
       "\n",
       "  totalLikes totalReplies totalShares totalImages replyToPostID referToPostID  \\\n",
       "0          7            6           0           1           NaN           NaN   \n",
       "\n",
       "  taggedSymbols  \n",
       "0            []  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_demo = pd.read_csv('fireant_data/news_2021-10.csv')\n",
    "news_demo.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc87d81e",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8803add4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import html \n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "# Clean HTML content from text\n",
    "def clean_html(text):\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    # Remove HTML tags\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    cleaned_text = soup.get_text()\n",
    "    # Unescape HTML entities\n",
    "    cleaned_text = html.unescape(cleaned_text)\n",
    "    return cleaned_text\n",
    "\n",
    "# Extract symbols from taggedSymbols column\n",
    "def extract_symbols(tagged_symbols_str):\n",
    "    # if tagged_symbols_str is a list skip processing\n",
    "    if isinstance(tagged_symbols_str, list):\n",
    "        return tagged_symbols_str\n",
    "    if pd.isna(tagged_symbols_str) or tagged_symbols_str.strip() == \"\":\n",
    "        return []\n",
    "    try:\n",
    "        data = json.loads(tagged_symbols_str)\n",
    "        # handle both single dict and list of dicts\n",
    "        if isinstance(data, dict):\n",
    "            return [data.get(\"symbol\")]\n",
    "        elif isinstance(data, list):\n",
    "            return [item.get(\"symbol\") for item in data if isinstance(item, dict) and \"symbol\" in item]\n",
    "        else:\n",
    "            return []\n",
    "    except json.JSONDecodeError:\n",
    "        # In case malformed JSON (e.g. missing brackets)\n",
    "        return []\n",
    "\n",
    "# Combine multiple fields into a single text field\n",
    "def combine_content(row):\n",
    "    # combine title, description, originalContent, sentiment, totalLikes, totalReplies, totalShares into on text field \n",
    "    \"\"\"  \n",
    "    format:\n",
    "    Title: {title}\n",
    "    Description: {description}\n",
    "    Content: {originalContent}\n",
    "    Sentiment: {sentiment} # -1 for negative, 0 for neutral, 1 for positive\n",
    "    Likes: {totalLikes}\n",
    "    Replies: {totalReplies}\n",
    "    Shares: {totalShares}\n",
    "    \"\"\"\n",
    "    content_parts = []\n",
    "    if pd.notna(row['title']):\n",
    "        content_parts.append(f\"Title: {row['title']}\")\n",
    "    if pd.notna(row['description']):\n",
    "        content_parts.append(f\"Description: {row['description']}\")\n",
    "    if pd.notna(row['originalContent']):\n",
    "        content_parts.append(f\"Content: {row['originalContent']}\")\n",
    "        \n",
    "    # sentiment mapping\n",
    "    sentiment_map = {-1: \"negative\", 0: \"neutral\", 1: \"positive\"}\n",
    "    sentiment_str = sentiment_map.get(row['sentiment'], \"unknown\")\n",
    "    content_parts.append(f\"Sentiment: {sentiment_str}\")\n",
    "    \n",
    "    content_parts.append(f\"Likes: {row['totalLikes']}\")\n",
    "    content_parts.append(f\"Replies: {row['totalReplies']}\")\n",
    "    content_parts.append(f\"Shares: {row['totalShares']}\")\n",
    "    return \"\\n\".join(content_parts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b6ffc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function clean and preprocess fireant news data\n",
    "def clean_fireant_news(news_paths):\n",
    "    for path in news_paths:\n",
    "        # Check if path exists else continue\n",
    "        try:\n",
    "            with open(path, 'r', encoding='utf-8', errors=\"replace\") as file:\n",
    "                news_df = pd.read_csv(\n",
    "                    file,\n",
    "                    header=0,\n",
    "                    quotechar='\"',\n",
    "                    escapechar='\\\\',\n",
    "                    encoding='utf-8',\n",
    "                    on_bad_lines='skip')\n",
    "                print(f\"Processing file: {path} with {len(news_df)} records.\")\n",
    "                \n",
    "                print(f\"Initial number of records: {len(news_df)}\")\n",
    "                \n",
    "                # Convert and clean numeric columns\n",
    "                # --- 1. Drop invalid postID rows (anything not all digits) ---\n",
    "                news_df[\"postID\"] = news_df[\"postID\"].astype(str)\n",
    "                news_df = news_df[news_df[\"postID\"].str.fullmatch(r\"\\d+\")]\n",
    "\n",
    "                # --- 2. Convert sentiment to numeric and keep only -1,0,1 ---\n",
    "                news_df[\"sentiment\"] = pd.to_numeric(news_df[\"sentiment\"], errors=\"coerce\")\n",
    "                news_df = news_df[news_df[\"sentiment\"].isin([-1,0,1])]\n",
    "                \n",
    "                # --- 3. Convert counts to integers ---\n",
    "                for col in [\"totalLikes\", \"totalReplies\", \"totalShares\"]:\n",
    "                    news_df[col] = pd.to_numeric(news_df[col], errors=\"coerce\").fillna(0).astype(int)\n",
    "                    news_df = news_df[news_df[col] >= 0]\n",
    "                    \n",
    "             \n",
    "                # --- 4. Drop rows with missing essential text fields ---\n",
    "                news_df = news_df.dropna(subset=[\"title\", \"description\", \"originalContent\"])\n",
    "                \n",
    "                \n",
    "                # --- 5. Process date column --- (ex: 2021-10-31T21:22:00+07:00 to 2021-10-31)\n",
    "                news_df['date'] = pd.to_datetime(news_df['date'], errors='coerce').dt.date\n",
    "                news_df = news_df.dropna(subset=['date'])\n",
    "\n",
    "                \n",
    "                # --- 6. Clean HTML fields ---\n",
    "                text_fields = [\"title\", \"description\", \"originalContent\"]\n",
    "                for field in text_fields:\n",
    "                    news_df[field] = news_df[field].apply(clean_html)\n",
    "                \n",
    "                # --- 7. Extract taggedSymbols ---\n",
    "                news_df['taggedSymbols'] = news_df['taggedSymbols'].apply(extract_symbols)\n",
    "                news_df[\"taggedSymbols\"] = news_df[\"taggedSymbols\"].apply(json.dumps)\n",
    "                \n",
    "                # --- 8. Combine content ---\n",
    "                news_df['combinedContent'] = news_df.apply(combine_content, axis=1)\n",
    "                \n",
    "                # --- 9. Drop unnecessary columns ---\n",
    "                columns_to_drop = ['userid', 'username', 'title', 'description',\n",
    "                                   'originalContent', 'postGroupName', 'postSourceName', 'postSourceUrl',\n",
    "                                   'link', 'sentiment', 'totalLikes', 'totalReplies', 'totalShares', 'totalImages', \n",
    "                                   'replyToPostID',\t'referToPostID']\n",
    "                news_df = news_df.drop(columns=columns_to_drop, errors='ignore')\n",
    "                \n",
    "                \n",
    "                print(f\"Final number of records after cleaning: {len(news_df)}\")\n",
    "                \n",
    "                \n",
    "                # --- 10. Save cleaned CSV ---\n",
    "                folder_path = \"fireant_data/cleaned_news\"\n",
    "                if not os.path.exists(folder_path):\n",
    "                    os.makedirs(folder_path)\n",
    "                # Save to cleaned_news folder with same filename, example: fireant_data/cleaned_news/news_2021-10.csv\n",
    "                cleaned_path = os.path.join(folder_path, os.path.basename(path))\n",
    "                news_df.to_csv(cleaned_path, index=False, encoding='utf-8')\n",
    "                print(f\"Cleaned data saved to: {cleaned_path}\\n\")\n",
    "                print(\"--------------------------------------------------\")\n",
    "                                   \n",
    "                \n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {path}. Skipping.\")\n",
    "            continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "946aa614",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_fireant_news(news_paths):\n",
    "    combined_df = []  # list to store all cleaned monthly DataFrames\n",
    "\n",
    "    for path in news_paths:\n",
    "        try:\n",
    "            with open(path, 'r', encoding='utf-8', errors=\"replace\") as file:\n",
    "                news_df = pd.read_csv(\n",
    "                    file,\n",
    "                    header=0,\n",
    "                    quotechar='\"',\n",
    "                    escapechar='\\\\',\n",
    "                    encoding='utf-8',\n",
    "                    on_bad_lines='skip'\n",
    "                )\n",
    "                print(f\"Processing file: {path} with {len(news_df)} records.\")\n",
    "\n",
    "                # --- 1. Drop invalid postID rows (anything not all digits) ---\n",
    "                news_df[\"postID\"] = news_df[\"postID\"].astype(str)\n",
    "                news_df = news_df[news_df[\"postID\"].str.fullmatch(r\"\\d+\")]\n",
    "\n",
    "                # --- 2. Convert sentiment to numeric and keep only -1,0,1 ---\n",
    "                news_df[\"sentiment\"] = pd.to_numeric(news_df[\"sentiment\"], errors=\"coerce\")\n",
    "                news_df = news_df[news_df[\"sentiment\"].isin([-1,0,1])]\n",
    "\n",
    "                # --- 3. Convert counts to integers ---\n",
    "                for col in [\"totalLikes\", \"totalReplies\", \"totalShares\"]:\n",
    "                    news_df[col] = pd.to_numeric(news_df[col], errors=\"coerce\").fillna(0).astype(int)\n",
    "                    news_df = news_df[news_df[col] >= 0]\n",
    "\n",
    "                # --- 4. Drop rows with missing essential text fields ---\n",
    "                news_df = news_df.dropna(subset=[\"title\", \"description\", \"originalContent\"])\n",
    "\n",
    "                # --- 5. Process date column ---\n",
    "                news_df['date'] = pd.to_datetime(news_df['date'], errors='coerce').dt.date\n",
    "                news_df = news_df.dropna(subset=['date'])\n",
    "\n",
    "                # --- 6. Clean HTML fields ---\n",
    "                text_fields = [\"title\", \"description\", \"originalContent\"]\n",
    "                for field in text_fields:\n",
    "                    news_df[field] = news_df[field].apply(clean_html)\n",
    "\n",
    "                # --- 7. Extract taggedSymbols ---\n",
    "                news_df['taggedSymbols'] = news_df['taggedSymbols'].apply(extract_symbols)\n",
    "                news_df[\"taggedSymbols\"] = news_df[\"taggedSymbols\"].apply(json.dumps)\n",
    "\n",
    "                # --- 8. Combine content ---\n",
    "                news_df['combinedContent'] = news_df.apply(combine_content, axis=1)\n",
    "\n",
    "                # --- 9. Drop unnecessary columns ---\n",
    "                columns_to_drop = ['userid', 'username', 'title', 'description',\n",
    "                                   'originalContent', 'postGroupName', 'postSourceName', 'postSourceUrl',\n",
    "                                   'link', 'sentiment', 'totalLikes', 'totalReplies', 'totalShares', 'totalImages', \n",
    "                                   'replyToPostID', 'referToPostID']\n",
    "                news_df = news_df.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "                print(f\"Final number of records after cleaning: {len(news_df)}\")\n",
    "\n",
    "                # Append cleaned monthly DF to list\n",
    "                combined_df.append(news_df)\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {path}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "    # --- Combine all months into one DataFrame ---\n",
    "    if combined_df:\n",
    "        final_df = pd.concat(combined_df, ignore_index=True)\n",
    "        print(f\"Total combined records: {len(final_df)}\")\n",
    "\n",
    "        # --- Save combined CSV ---\n",
    "        folder_path = \"fireant_data/cleaned_news\"\n",
    "        os.makedirs(folder_path, exist_ok=True)\n",
    "        combined_path = os.path.join(folder_path, \"all_news.csv\")\n",
    "        final_df.to_csv(combined_path, index=False, encoding='utf-8')\n",
    "        print(f\"Combined cleaned data saved to: {combined_path}\")\n",
    "    else:\n",
    "        print(\"No data processed. Combined CSV not created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf314cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not found: fireant_data/news_2021-01.csv. Skipping.\n",
      "File not found: fireant_data/news_2021-02.csv. Skipping.\n",
      "File not found: fireant_data/news_2021-03.csv. Skipping.\n",
      "File not found: fireant_data/news_2021-04.csv. Skipping.\n",
      "File not found: fireant_data/news_2021-05.csv. Skipping.\n",
      "File not found: fireant_data/news_2021-06.csv. Skipping.\n",
      "File not found: fireant_data/news_2021-07.csv. Skipping.\n",
      "File not found: fireant_data/news_2021-08.csv. Skipping.\n",
      "Processing file: fireant_data/news_2021-09.csv with 3063 records.\n",
      "Final number of records after cleaning: 2957\n",
      "Processing file: fireant_data/news_2021-10.csv with 3093 records.\n",
      "Final number of records after cleaning: 3029\n",
      "Processing file: fireant_data/news_2021-11.csv with 3190 records.\n",
      "Final number of records after cleaning: 2967\n",
      "Processing file: fireant_data/news_2021-12.csv with 2994 records.\n",
      "Final number of records after cleaning: 2653\n",
      "Processing file: fireant_data/news_2022-01.csv with 2648 records.\n",
      "Final number of records after cleaning: 2074\n",
      "Processing file: fireant_data/news_2022-02.csv with 2209 records.\n",
      "Final number of records after cleaning: 1759\n",
      "Processing file: fireant_data/news_2022-03.csv with 2714 records.\n",
      "Final number of records after cleaning: 2129\n",
      "Processing file: fireant_data/news_2022-04.csv with 2443 records.\n",
      "Final number of records after cleaning: 1974\n",
      "Processing file: fireant_data/news_2022-05.csv with 2454 records.\n",
      "Final number of records after cleaning: 1849\n",
      "Processing file: fireant_data/news_2022-06.csv with 2731 records.\n",
      "Final number of records after cleaning: 1996\n",
      "Processing file: fireant_data/news_2022-07.csv with 2625 records.\n",
      "Final number of records after cleaning: 2012\n",
      "Processing file: fireant_data/news_2022-08.csv with 2761 records.\n",
      "Final number of records after cleaning: 2393\n",
      "Processing file: fireant_data/news_2022-09.csv with 2577 records.\n",
      "Final number of records after cleaning: 2104\n",
      "Processing file: fireant_data/news_2022-10.csv with 2800 records.\n",
      "Final number of records after cleaning: 2066\n",
      "Processing file: fireant_data/news_2022-11.csv with 3172 records.\n",
      "Final number of records after cleaning: 2522\n",
      "Processing file: fireant_data/news_2022-12.csv with 3232 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_16772\\2962457169.py:13: MarkupResemblesLocatorWarning: The input passed in on this line looks more like a URL than HTML or XML.\n",
      "\n",
      "If you meant to use Beautiful Soup to parse the web page found at a certain URL, then something has gone wrong. You should use an Python package like 'requests' to fetch the content behind the URL. Once you have the content as a string, you can feed that string into Beautiful Soup.\n",
      "\n",
      "However, if you want to parse some data that happens to look like a URL, then nothing has gone wrong: you are using Beautiful Soup correctly, and this warning is spurious and can be filtered. To make this warning go away, run this code before calling the BeautifulSoup constructor:\n",
      "\n",
      "    from bs4 import MarkupResemblesLocatorWarning\n",
      "    import warnings\n",
      "\n",
      "    warnings.filterwarnings(\"ignore\", category=MarkupResemblesLocatorWarning)\n",
      "    \n",
      "  soup = BeautifulSoup(text, \"html.parser\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final number of records after cleaning: 2594\n",
      "Processing file: fireant_data/news_2023-01.csv with 3927 records.\n",
      "Final number of records after cleaning: 3556\n",
      "Processing file: fireant_data/news_2023-02.csv with 4231 records.\n",
      "Final number of records after cleaning: 3671\n",
      "Processing file: fireant_data/news_2023-03.csv with 4281 records.\n",
      "Final number of records after cleaning: 3669\n",
      "Processing file: fireant_data/news_2023-04.csv with 4893 records.\n",
      "Final number of records after cleaning: 4415\n",
      "Processing file: fireant_data/news_2023-05.csv with 5133 records.\n",
      "Final number of records after cleaning: 4796\n",
      "Processing file: fireant_data/news_2023-06.csv with 4447 records.\n",
      "Final number of records after cleaning: 3813\n",
      "Processing file: fireant_data/news_2023-07.csv with 4459 records.\n",
      "Final number of records after cleaning: 3600\n",
      "Processing file: fireant_data/news_2023-08.csv with 5416 records.\n",
      "Final number of records after cleaning: 4709\n",
      "Processing file: fireant_data/news_2023-09.csv with 5162 records.\n",
      "Final number of records after cleaning: 4655\n",
      "Processing file: fireant_data/news_2023-10.csv with 5637 records.\n",
      "Final number of records after cleaning: 5118\n",
      "Processing file: fireant_data/news_2023-11.csv with 5360 records.\n",
      "Final number of records after cleaning: 4933\n",
      "Processing file: fireant_data/news_2023-12.csv with 5984 records.\n",
      "Final number of records after cleaning: 5643\n",
      "Processing file: fireant_data/news_2024-01.csv with 6287 records.\n",
      "Final number of records after cleaning: 5814\n",
      "Processing file: fireant_data/news_2024-02.csv with 4919 records.\n",
      "Final number of records after cleaning: 4624\n",
      "Processing file: fireant_data/news_2024-03.csv with 5878 records.\n",
      "Final number of records after cleaning: 5454\n",
      "Processing file: fireant_data/news_2024-04.csv with 5670 records.\n",
      "Final number of records after cleaning: 5329\n",
      "Processing file: fireant_data/news_2024-05.csv with 6899 records.\n",
      "Final number of records after cleaning: 6411\n",
      "Processing file: fireant_data/news_2024-06.csv with 6271 records.\n",
      "Final number of records after cleaning: 5678\n",
      "Processing file: fireant_data/news_2024-07.csv with 7338 records.\n",
      "Final number of records after cleaning: 6601\n",
      "Processing file: fireant_data/news_2024-08.csv with 8675 records.\n",
      "Final number of records after cleaning: 8161\n",
      "Processing file: fireant_data/news_2024-09.csv with 8967 records.\n",
      "Final number of records after cleaning: 8608\n",
      "Processing file: fireant_data/news_2024-10.csv with 9060 records.\n",
      "Final number of records after cleaning: 8534\n",
      "Processing file: fireant_data/news_2024-11.csv with 8464 records.\n",
      "Final number of records after cleaning: 8046\n",
      "Processing file: fireant_data/news_2024-12.csv with 8617 records.\n",
      "Final number of records after cleaning: 8015\n",
      "Processing file: fireant_data/news_2025-01.csv with 7785 records.\n",
      "Final number of records after cleaning: 7332\n",
      "Processing file: fireant_data/news_2025-02.csv with 7267 records.\n",
      "Final number of records after cleaning: 6709\n",
      "Processing file: fireant_data/news_2025-03.csv with 7490 records.\n",
      "Final number of records after cleaning: 7008\n",
      "Processing file: fireant_data/news_2025-04.csv with 7507 records.\n",
      "Final number of records after cleaning: 7010\n",
      "Processing file: fireant_data/news_2025-05.csv with 7507 records.\n",
      "Final number of records after cleaning: 7096\n",
      "Processing file: fireant_data/news_2025-06.csv with 7353 records.\n",
      "Final number of records after cleaning: 6855\n",
      "Processing file: fireant_data/news_2025-07.csv with 8425 records.\n",
      "Final number of records after cleaning: 7929\n",
      "Processing file: fireant_data/news_2025-08.csv with 7494 records.\n",
      "Final number of records after cleaning: 7115\n",
      "Processing file: fireant_data/news_2025-09.csv with 7188 records.\n",
      "Final number of records after cleaning: 6842\n",
      "Processing file: fireant_data/news_2025-10.csv with 7624 records.\n",
      "Final number of records after cleaning: 7163\n",
      "File not found: fireant_data/news_2025-11.csv. Skipping.\n",
      "File not found: fireant_data/news_2025-12.csv. Skipping.\n",
      "Total combined records: 241990\n",
      "Combined cleaned data saved to: fireant_data/cleaned_news\\all_news.csv\n"
     ]
    }
   ],
   "source": [
    "years = [2021, 2022, 2023, 2024, 2025]\n",
    "news_paths = [f\"fireant_data/news_{year}-{month:02d}.csv\" for year in years for month in range(1, 13)]\n",
    "clean_fireant_news(news_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46c205b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
