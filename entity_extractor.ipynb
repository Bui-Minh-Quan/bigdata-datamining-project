{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "734e43e5",
   "metadata": {},
   "source": [
    "# ü§ñ Entity Extractor - TRR Step 1: Tr√≠ch xu·∫•t th·ª±c th·ªÉ\n",
    "\n",
    "## M·ª•c ti√™u:\n",
    "Tr√≠ch xu·∫•t c√°c th·ª±c th·ªÉ (m√£ c·ªï phi·∫øu, ng√†nh ngh·ªÅ, c√¥ng ty, th·ªã tr∆∞·ªùng) t·ª´ tin t·ª©c ƒë√£ t√≥m t·∫Øt.\n",
    "\n",
    "## Input:\n",
    "- File CSV: `summarized_news_with_stocks_merged.csv`\n",
    "- Columns: `postID, stockCodes, title, description, date`\n",
    "\n",
    "## Output:\n",
    "- `entities_extracted.csv`: DataFrame ch·ª©a c√°c entity ƒë∆∞·ª£c tr√≠ch xu·∫•t\n",
    "- `knowledge_graph.pkl`: NetworkX DiGraph\n",
    "- `canonical_entities.pkl`: Set c√°c entity canonical\n",
    "- `graph_tuples_step1.csv`: Tuples (date, source, impact, target) ƒë·ªÉ ph√¢n t√≠ch\n",
    "\n",
    "## Prompt Strategy:\n",
    "- **Focus m√£ c·ªï phi·∫øu:** ∆Øu ti√™n 10 m√£ trong portfolio\n",
    "- **Focus ng√†nh VN:** Ghi r√µ \"Ng√†nh [t√™n] Vi·ªát Nam\"\n",
    "- **Focus th·ªã tr∆∞·ªùng:** Ph√¢n bi·ªát VN/n∆∞·ªõc ngo√†i\n",
    "- **Chi ti·∫øt entity:** B·∫Øt bu·ªôc tr√≠ch d·∫´n s·ªë li·ªáu c·ª• th·ªÉ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c679e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "import google.generativeai as genai\n",
    "import networkx as nx\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "print(\"‚úì ƒê√£ import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c32e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class APIKeyManager:\n",
    "    \"\"\"\n",
    "    Qu·∫£n l√Ω nhi·ªÅu Google API keys v√† t·ª± ƒë·ªông chuy·ªÉn ƒë·ªïi khi g·∫∑p l·ªói.\n",
    "    \n",
    "    Quy t·∫Øc:\n",
    "    - M·ªói key ƒë∆∞·ª£c th·ª≠ t·ªëi ƒëa 2 l·∫ßn\n",
    "    - Sau 2 l·∫ßn l·ªói ‚Üí t·ª± ƒë·ªông chuy·ªÉn key ti·∫øp theo\n",
    "    - H·∫øt key ‚Üí b√°o l·ªói\n",
    "    \"\"\"\n",
    "    \n",
    "    MAX_RETRIES_PER_KEY = 2  # S·ªë l·∫ßn th·ª≠ t·ªëi ƒëa cho m·ªói key\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Kh·ªüi t·∫°o v√† load t·∫•t c·∫£ API keys t·ª´ .env\"\"\"\n",
    "        # Load c√°c API keys t·ª´ environment\n",
    "        self.keys = [\n",
    "            (\"GOOGLE_API_KEY\", os.getenv(\"GOOGLE_API_KEY\")),\n",
    "            (\"GOOGLE_API_KEY_2\", os.getenv(\"GOOGLE_API_KEY_2\")),\n",
    "            (\"GOOGLE_API_KEY_3\", os.getenv(\"GOOGLE_API_KEY_3\")),\n",
    "            (\"GOOGLE_API_KEY_4\", os.getenv(\"GOOGLE_API_KEY_4\")),\n",
    "        ]\n",
    "        \n",
    "        # Ch·ªâ gi·ªØ l·∫°i c√°c key h·ª£p l·ªá (kh√¥ng None)\n",
    "        self.keys = [(name, key) for name, key in self.keys if key]\n",
    "        \n",
    "        if not self.keys:\n",
    "            raise ValueError(\"‚ùå Kh√¥ng t√¨m th·∫•y API key! Ki·ªÉm tra file .env\")\n",
    "        \n",
    "        # Kh·ªüi t·∫°o tr·∫°ng th√°i\n",
    "        self.current_index = 0\n",
    "        self.error_counts = {name: 0 for name, _ in self.keys}  # ƒê·∫øm l·ªói m·ªói key\n",
    "        \n",
    "        print(f\"‚úì Ph√°t hi·ªán {len(self.keys)} API keys\")\n",
    "        self._activate_key(0)\n",
    "    \n",
    "    def _activate_key(self, index):\n",
    "        \"\"\"K√≠ch ho·∫°t API key t·∫°i v·ªã tr√≠ index\"\"\"\n",
    "        if index >= len(self.keys):\n",
    "            raise Exception(\"‚ùå ƒê√£ h·∫øt t·∫•t c·∫£ API keys!\")\n",
    "        \n",
    "        self.current_index = index\n",
    "        key_name, key_value = self.keys[index]\n",
    "        \n",
    "        # C·∫•u h√¨nh Google AI v·ªõi key m·ªõi\n",
    "        genai.configure(api_key=key_value)\n",
    "        \n",
    "        print(f\"üîë ƒêang s·ª≠ d·ª•ng: {key_name} (Key {index + 1}/{len(self.keys)})\")\n",
    "    \n",
    "    def get_current_key(self):\n",
    "        \"\"\"L·∫•y API key hi·ªán t·∫°i\"\"\"\n",
    "        return self.keys[self.current_index][1]\n",
    "    \n",
    "    def get_models(self):\n",
    "        \"\"\"\n",
    "        T·∫°o c√°c model AI v·ªõi API key hi·ªán t·∫°i.\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (model, model_more_temp, model_pro)\n",
    "        \"\"\"\n",
    "        current_key = self.get_current_key()\n",
    "        \n",
    "        model = ChatGoogleGenerativeAI(\n",
    "            model=\"gemini-2.0-flash-lite\", \n",
    "            temperature=0.02,\n",
    "            google_api_key=current_key\n",
    "        )\n",
    "        \n",
    "        model_more_temp = ChatGoogleGenerativeAI(\n",
    "            model=\"gemini-2.0-flash-lite\", \n",
    "            temperature=0.1,\n",
    "            google_api_key=current_key\n",
    "        )\n",
    "        \n",
    "        model_pro = ChatGoogleGenerativeAI(\n",
    "            model=\"gemini-2.5-pro-exp-03-25\", \n",
    "            temperature=0.1,\n",
    "            google_api_key=current_key\n",
    "        )\n",
    "        \n",
    "        return model, model_more_temp, model_pro\n",
    "    \n",
    "    def on_error(self):\n",
    "        \"\"\"\n",
    "        X·ª≠ l√Ω khi g·∫∑p l·ªói API.\n",
    "        \n",
    "        Returns:\n",
    "            bool: True n·∫øu c√≤n key kh√°c ƒë·ªÉ th·ª≠, False n·∫øu h·∫øt key\n",
    "        \"\"\"\n",
    "        key_name = self.keys[self.current_index][0]\n",
    "        self.error_counts[key_name] += 1\n",
    "        \n",
    "        error_count = self.error_counts[key_name]\n",
    "        print(f\"‚ö† L·ªói l·∫ßn {error_count} v·ªõi {key_name}\")\n",
    "        \n",
    "        # N·∫øu ƒë√£ ƒë·∫°t gi·ªõi h·∫°n retry cho key n√†y\n",
    "        if error_count >= self.MAX_RETRIES_PER_KEY:\n",
    "            print(f\"‚õî {key_name} ƒë√£ l·ªói {error_count}/{self.MAX_RETRIES_PER_KEY} l·∫ßn\")\n",
    "            \n",
    "            # Th·ª≠ chuy·ªÉn sang key ti·∫øp theo\n",
    "            next_index = self.current_index + 1\n",
    "            if next_index < len(self.keys):\n",
    "                print(f\"üîÑ Chuy·ªÉn sang key ti·∫øp theo...\")\n",
    "                self._activate_key(next_index)\n",
    "                return True\n",
    "            else:\n",
    "                print(f\"‚ùå ƒê√£ h·∫øt t·∫•t c·∫£ {len(self.keys)} keys!\")\n",
    "                return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def reset_errors(self):\n",
    "        \"\"\"Reset error counters cho t·∫•t c·∫£ keys\"\"\"\n",
    "        self.error_counts = {name: 0 for name, _ in self.keys}\n",
    "        print(\"‚ôª ƒê√£ reset error counters\")\n",
    "\n",
    "print(\"‚úì ƒê√£ ƒë·ªãnh nghƒ©a APIKeyManager\")\n",
    "\n",
    "# Kh·ªüi t·∫°o API Manager\n",
    "api_manager = APIKeyManager()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851c0369",
   "metadata": {},
   "source": [
    "## üìù Prompt Templates\n",
    "\n",
    "### Entity Extraction Prompt:\n",
    "- **ƒê√£ c·∫£i ti·∫øn:** B·ªè tr∆∞·ªùng `{group}` kh√¥ng t·ªìn t·∫°i trong dataset\n",
    "- **Th√™m tr∆∞·ªùng:** `{stockCodes}` - m√£ c·ªï phi·∫øu t·ª´ dataset\n",
    "- **Focus:** M√£ CP trong portfolio, ng√†nh VN, th·ªã tr∆∞·ªùng VN/ngo·∫°i\n",
    "- **Chi ti·∫øt:** B·∫Øt bu·ªôc tr√≠ch d·∫´n s·ªë li·ªáu c·ª• th·ªÉ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af71d02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PROMPT TEMPLATES - Tr√≠ch xu·∫•t th·ª±c th·ªÉ v√† m·ªëi quan h·ªá\n",
    "# ============================================================\n",
    "\n",
    "# Template tr√≠ch xu·∫•t th·ª±c th·ªÉ t·ª´ tin t·ª©c\n",
    "entity_extraction_template = PromptTemplate.from_template(\"\"\"B·∫°n ƒëang ph√¢n t√≠ch tin t·ª©c kinh t·∫ø Vi·ªát Nam ƒë·ªÉ d·ª± ƒëo√°n bi·∫øn ƒë·ªông th·ªã tr∆∞·ªùng ch·ª©ng kho√°n.\n",
    "B·∫°n ƒë∆∞·ª£c cho m·ªôt b√†i b√°o v·ªõi t·ª±a ƒë·ªÅ, m√¥ t·∫£ ng·∫Øn g·ªçn, v√† ng√†y xu·∫•t b·∫£n.\n",
    "\n",
    "**M·ª•c ti√™u:** T√¨m c√°c th·ª±c th·ªÉ (m√£ c·ªï phi·∫øu, ng√†nh ngh·ªÅ, c√¥ng ty, th·ªã tr∆∞·ªùng, qu·ªëc gia, t·ªânh th√†nh) b·ªã ·∫£nh h∆∞·ªüng tr·ª±c ti·∫øp b·ªüi tin t·ª©c, theo h∆∞·ªõng T√çCH C·ª∞C ho·∫∑c TI√äU C·ª∞C.\n",
    "\n",
    "**Nguy√™n t·∫Øc ƒë·∫∑t t√™n th·ª±c th·ªÉ:**\n",
    "1. **∆Øu ti√™n t·ªëi ƒëa c√°c m√£ c·ªï phi·∫øu trong danh m·ª•c:** {portfolio}\n",
    "   - N·∫øu tin t·ª©c ƒë·ªÅ c·∫≠p c√¥ng ty/ng√†nh c·ªßa c√°c m√£ n√†y, B·∫ÆT BU·ªòC ghi r√µ m√£ c·ªï phi·∫øu\n",
    "   - V√≠ d·ª•: \"FPT-C√¥ng ngh·ªá\", \"VCB-Ng√¢n h√†ng\", \"HPG-Th√©p\"\n",
    "   \n",
    "2. **V·ªõi ng√†nh ngh·ªÅ Vi·ªát Nam:** Ghi r√µ \"Ng√†nh [t√™n ng√†nh] Vi·ªát Nam\"\n",
    "   - V√≠ d·ª•: \"Ng√†nh b√°n l·∫ª Vi·ªát Nam\", \"Ng√†nh c√¥ng ngh·ªá Vi·ªát Nam\", \"Ng√†nh th√©p Vi·ªát Nam\"\n",
    "   \n",
    "3. **V·ªõi th·ªã tr∆∞·ªùng/khu v·ª±c:** Ghi r√µ ƒë·ªãa danh c·ª• th·ªÉ\n",
    "   - V√≠ d·ª•: \"Th·ªã tr∆∞·ªùng ch·ª©ng kho√°n Vi·ªát Nam\", \"Th·ªã tr∆∞·ªùng M·ªπ\", \"Khu v·ª±c ƒê√¥ng Nam √Å\"\n",
    "   \n",
    "4. **V·ªõi c√¥ng ty kh√¥ng trong portfolio:** Ghi \"C√¥ng ty [t√™n]-[ng√†nh]\"\n",
    "   - V√≠ d·ª•: \"C√¥ng ty Meta-C√¥ng ngh·ªá\", \"C√¥ng ty Vinamilk-Ti√™u d√πng\"\n",
    "\n",
    "5. **H·∫°n ch·∫ø t·∫°o m·ªõi:** Ch·ªâ t·∫°o t·ªëi ƒëa 5 th·ª±c th·ªÉ. ∆Øu ti√™n li√™n k·∫øt v·ªõi c√°c th·ª±c th·ªÉ ƒë√£ c√≥: {existing_entities}\n",
    "\n",
    "6. **Tr√°nh k√Ω t·ª± ph·ª©c t·∫°p:** Kh√¥ng d√πng d·∫•u ch·∫•m, g·∫°ch ngang (tr·ª´ sau m√£ c·ªï phi·∫øu), d·∫•u &, d·∫•u ch·∫•m ph·∫©y\n",
    "\n",
    "**Nguy√™n t·∫Øc gi·∫£i th√≠ch:**\n",
    "- B·∫ÆT BU·ªòC tr√≠ch d·∫´n s·ªë li·ªáu c·ª• th·ªÉ t·ª´ b√†i b√°o (tƒÉng/gi·∫£m bao nhi√™u, g·∫•p m·∫•y l·∫ßn, %)\n",
    "- ƒê√°nh gi√° t√°c ƒë·ªông d·ª±a tr√™n d·ªØ li·ªáu th·ª±c t·∫ø, kh√¥ng suy di·ªÖn qu√° xa\n",
    "- Kh√¥ng t·ª± th√™m s·ªë li·ªáu kh√¥ng c√≥ trong b√†i b√°o\n",
    "- Kh√¥ng d√πng d·∫•u hai ch·∫•m trong ph·∫ßn gi·∫£i th√≠ch\n",
    "\n",
    "**ƒê·ªãnh d·∫°ng output:**\n",
    "[[POSITIVE]]\n",
    "[Entity 1]: [Explanation v·ªõi s·ªë li·ªáu c·ª• th·ªÉ]\n",
    "...\n",
    "\n",
    "[[NEGATIVE]]\n",
    "[Entity A]: [Explanation v·ªõi s·ªë li·ªáu c·ª• th·ªÉ]\n",
    "...\n",
    "\n",
    "---\n",
    "\n",
    "**V√ç D·ª§ MINH H·ªåA:**\n",
    "\n",
    "(B·∫ÆT ƒê·∫¶U V√ç D·ª§)\n",
    "\n",
    "Ng√†y ƒëƒÉng: 2025-01-01T00:00:00+07:00\n",
    "M√£ c·ªï phi·∫øu li√™n quan: (kh√¥ng c√≥)\n",
    "T·ª±a ƒë·ªÅ: S·ªë l∆∞·ª£ng h√≥a ƒë∆°n kh·ªüi t·∫°o t·ª´ m√°y t√≠nh ti·ªÅn tƒÉng g·∫•p 13 l·∫ßn nƒÉm 2023\n",
    "\n",
    "M√¥ t·∫£: T·ª∑ l·ªá c∆° s·ªü kinh doanh s·ª≠ d·ª•ng h√≥a ƒë∆°n ƒëi·ªán t·ª≠ tƒÉng m·∫°nh, v·ªõi s·ªë l∆∞·ª£ng h√≥a ƒë∆°n t·ª´ m√°y t√≠nh ti·ªÅn tƒÉng g·∫•p 13 l·∫ßn so v·ªõi nƒÉm tr∆∞·ªõc. Ng√†nh b√°n l·∫ª v√† d·ªãch v·ª• h∆∞·ªüng l·ª£i l·ªõn t·ª´ chuy·ªÉn ƒë·ªïi s·ªë n√†y.\n",
    "\n",
    "Danh s√°ch th·ª±c th·ªÉ s·∫Ω b·ªã ·∫£nh h∆∞·ªüng:\n",
    "\n",
    "[[POSITIVE]]\n",
    "Ng√†nh b√°n l·∫ª Vi·ªát Nam: S·ªë l∆∞·ª£ng h√≥a ƒë∆°n ƒëi·ªán t·ª≠ t·ª´ m√°y t√≠nh ti·ªÅn tƒÉng g·∫•p 13 l·∫ßn trong nƒÉm 2023, gi√∫p tƒÉng hi·ªáu qu·∫£ qu·∫£n l√Ω v√† gi·∫£m chi ph√≠ v·∫≠n h√†nh\n",
    "MWG-B√°n l·∫ª: L√† chu·ªói b√°n l·∫ª l·ªõn, h∆∞·ªüng l·ª£i tr·ª±c ti·∫øp t·ª´ vi·ªác s·ªë h√≥a h√≥a ƒë∆°n tƒÉng 13 l·∫ßn, c·∫£i thi·ªán kh·∫£ nƒÉng qu·∫£n l√Ω t·ªìn kho v√† d√≤ng ti·ªÅn\n",
    "Ng√†nh c√¥ng ngh·ªá Vi·ªát Nam: Cung c·∫•p gi·∫£i ph√°p h√≥a ƒë∆°n ƒëi·ªán t·ª≠ v√† m√°y t√≠nh ti·ªÅn cho h√†ng ngh√¨n c∆° s·ªü kinh doanh, doanh thu d·ª± ki·∫øn tƒÉng m·∫°nh\n",
    "FPT-C√¥ng ngh·ªá: L√† nh√† cung c·∫•p gi·∫£i ph√°p chuy·ªÉn ƒë·ªïi s·ªë h√†ng ƒë·∫ßu, h∆∞·ªüng l·ª£i t·ª´ nhu c·∫ßu tri·ªÉn khai h√≥a ƒë∆°n ƒëi·ªán t·ª≠ tƒÉng ƒë·ªôt bi·∫øn\n",
    "\n",
    "[[NEGATIVE]]\n",
    "(Kh√¥ng c√≥ th·ª±c th·ªÉ b·ªã ·∫£nh h∆∞·ªüng ti√™u c·ª±c r√µ r√†ng t·ª´ b√†i b√°o n√†y)\n",
    "\n",
    "(K·∫æT TH√öC V√ç D·ª§)\n",
    "\n",
    "---\n",
    "\n",
    "**B√ÄI B√ÅO C·∫¶N PH√ÇN T√çCH:**\n",
    "\n",
    "Ng√†y ƒëƒÉng: {date}\n",
    "M√£ c·ªï phi·∫øu li√™n quan: {stockCodes}\n",
    "T·ª±a ƒë·ªÅ: {title}\n",
    "\n",
    "M√¥ t·∫£: {description}\n",
    "\n",
    "Danh s√°ch th·ª±c th·ªÉ s·∫Ω b·ªã ·∫£nh h∆∞·ªüng:\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úì ƒê√£ ƒë·ªãnh nghƒ©a entity_extraction_template\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bcc5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "PORTFOLIO_STOCKS = [\"FPT\", \"SSI\", \"VCB\", \"VHM\", \"HPG\", \"GAS\", \"MSN\", \"MWG\", \"GVR\", \"VIC\"]\n",
    "PORTFOLIO_SECTOR = [\"C√¥ng ngh·ªá\", \"Ch·ª©ng kho√°n\", \"Ng√¢n h√†ng\", \"B·∫•t ƒë·ªông s·∫£n\", \"V·∫≠t li·ªáu c∆° b·∫£n\", \n",
    "                     \"D·ªãch v·ª• H·∫° t·∫ßng\", \"Ti√™u d√πng c∆° b·∫£n\", \"B√°n l·∫ª\", \"Ch·∫ø bi·∫øn\", \"B·∫•t ƒë·ªçng s·∫£n\"]\n",
    "BASE_DELAY = 30\n",
    "MAX_RETRIES = 3\n",
    "\n",
    "def create_chains(api_manager):\n",
    "    \"\"\"\n",
    "    T·∫°o chains v·ªõi models t·ª´ APIKeyManager\n",
    "    \"\"\"\n",
    "    model, model_more_temp, model_pro = api_manager.get_models()\n",
    "    \n",
    "    # T·∫°o chain tr√≠ch xu·∫•t th·ª±c th·ªÉ\n",
    "    chain_entity = entity_extraction_template | model\n",
    "    \n",
    "    return chain_entity\n",
    "\n",
    "print(\"‚úì ƒê√£ ƒë·ªãnh nghƒ©a portfolio v√† constants\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c320c01e",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Utility Functions\n",
    "\n",
    "C√°c h√†m h·ªó tr·ª£:\n",
    "1. `invoke_chain_with_retry()` - G·ªçi API v·ªõi retry t·ª± ƒë·ªông\n",
    "2. `parse_entity_response()` - Parse response t·ª´ LLM\n",
    "3. `merge_entity()` - Chu·∫©n h√≥a t√™n entity\n",
    "4. `graph_entities_to_str()` - Convert graph entities th√†nh string cho prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b81888",
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_chain_with_retry(chain, prompt, api_manager, base_delay=BASE_DELAY):\n",
    "    \"\"\"\n",
    "    G·ªçi chain v·ªõi c∆° ch·∫ø retry t·ª± ƒë·ªông v√† t√≠ch h·ª£p APIKeyManager\n",
    "    \"\"\"\n",
    "    total_attempts = 0\n",
    "    max_total_attempts = len(api_manager.keys) * api_manager.MAX_RETRIES_PER_KEY\n",
    "    \n",
    "    while total_attempts < max_total_attempts:\n",
    "        try:\n",
    "            # Th·ª≠ g·ªçi API\n",
    "            response = chain.invoke(prompt)\n",
    "            \n",
    "            # Th√†nh c√¥ng -> reset error count\n",
    "            api_manager.reset_error_count()\n",
    "            return response\n",
    "            \n",
    "        except Exception as e:\n",
    "            total_attempts += 1\n",
    "            error_msg = str(e)\n",
    "            \n",
    "            # B√°o l·ªói cho API manager\n",
    "            switched = api_manager.on_error()\n",
    "            \n",
    "            if total_attempts >= max_total_attempts:\n",
    "                print(f\"‚ùå ƒê√£ th·ª≠ t·∫•t c·∫£ {len(api_manager.keys)} API keys ({total_attempts} l·∫ßn) nh∆∞ng v·∫´n l·ªói\")\n",
    "                print(f\"   L·ªói cu·ªëi: {error_msg}\")\n",
    "                return None\n",
    "            \n",
    "            # Ch·ªù tr∆∞·ªõc khi retry\n",
    "            if switched:\n",
    "                delay = base_delay\n",
    "                print(f\"‚è≥ ƒê·ª£i {delay}s tr∆∞·ªõc khi th·ª≠ key m·ªõi...\")\n",
    "            else:\n",
    "                retry_num = api_manager.error_counts.get(api_manager.current_index, 0)\n",
    "                delay = base_delay * (1.5 ** (retry_num - 1))\n",
    "                print(f\"‚è≥ ƒê·ª£i {delay:.0f}s tr∆∞·ªõc khi retry ({retry_num}/{api_manager.MAX_RETRIES_PER_KEY})...\")\n",
    "            \n",
    "            time.sleep(delay)\n",
    "\n",
    "def parse_entity_response(response):\n",
    "    \"\"\"\n",
    "    Ph√¢n t√≠ch response t·ª´ entity extraction prompt\n",
    "    \n",
    "    Returns:\n",
    "        dict: {\"POSITIVE\": [(entity, explanation), ...], \"NEGATIVE\": [(entity, explanation), ...]}\n",
    "    \"\"\"\n",
    "    if response is None:\n",
    "        print(\"Response is None\")\n",
    "        return {\"POSITIVE\": [], \"NEGATIVE\": []}\n",
    "        \n",
    "    sections = {\"POSITIVE\": [], \"NEGATIVE\": []}\n",
    "    current_section = None\n",
    "    str_resp = response.content\n",
    "    \n",
    "    for line in str(str_resp).splitlines():\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        if \"[[POSITIVE]]\" in line.upper():\n",
    "            current_section = \"POSITIVE\"\n",
    "            continue\n",
    "        if \"[[NEGATIVE]]\" in line.upper():\n",
    "            current_section = \"NEGATIVE\"\n",
    "            continue\n",
    "        if current_section and ':' in line:\n",
    "            entity = line.split(\":\", 1)[0].strip()\n",
    "            # Skip invalid entities\n",
    "            if not entity or \"kh√¥ng c√≥ th·ª±c th·ªÉ n√†o\" in entity.lower():\n",
    "                continue\n",
    "            # content = all line except entity\n",
    "            content = line.split(entity, 1)[-1].strip(':').strip()\n",
    "            sections[current_section].append((entity, content))\n",
    "\n",
    "    return sections\n",
    "\n",
    "def merge_entity(entity, canonical_set):\n",
    "    \"\"\"\n",
    "    Tr·∫£ v·ªÅ phi√™n b·∫£n canonical c·ªßa entity n·∫øu ƒë√£ t·ªìn t·∫°i (case-insensitive),\n",
    "    n·∫øu kh√¥ng th√¨ th√™m v√† tr·∫£ v·ªÅ entity m·ªõi.\n",
    "    \"\"\"\n",
    "    normalized_entity = str(entity).strip('[').strip(']').strip(' ').lower()\n",
    "    for exist in canonical_set:\n",
    "        if exist.lower() == normalized_entity:\n",
    "            return exist\n",
    "    canonical_set.add(normalized_entity)\n",
    "    return normalized_entity\n",
    "\n",
    "def graph_entities_to_str(G, max_entities=50):\n",
    "    \"\"\"\n",
    "    Chuy·ªÉn ƒë·ªïi c√°c entities trong graph th√†nh chu·ªói ƒë·ªÉ ƒë∆∞a v√†o prompt\n",
    "    \"\"\"\n",
    "    entities = [node for node in G.nodes() if not node.startswith(\"Article_\")]\n",
    "    # Gi·ªõi h·∫°n s·ªë l∆∞·ª£ng ƒë·ªÉ kh√¥ng l√†m prompt qu√° d√†i\n",
    "    entities = entities[:max_entities]\n",
    "    return \", \".join(entities) if entities else \"Ch∆∞a c√≥ th·ª±c th·ªÉ n√†o\"\n",
    "\n",
    "print(\"‚úì ƒê√£ ƒë·ªãnh nghƒ©a c√°c h√†m ti·ªán √≠ch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf93f87",
   "metadata": {},
   "source": [
    "## üéØ Main Function: Extract Entities from News\n",
    "\n",
    "H√†m ch√≠nh tr√≠ch xu·∫•t entities t·ª´ CSV tin t·ª©c ƒë√£ t√≥m t·∫Øt.\n",
    "\n",
    "**Features:**\n",
    "- ‚úÖ Validation dataset tr∆∞·ªõc khi ch·∫°y\n",
    "- ‚úÖ Error handling v·ªõi th√¥ng b√°o chi ti·∫øt\n",
    "- ‚úÖ Export to tuples CSV cho ph√¢n t√≠ch\n",
    "- ‚úÖ Progress tracking v√† logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec082aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities_from_news(\n",
    "    csv_path=\"summarized_news_with_stocks_merged.csv\",\n",
    "    output_path=\"entities_extracted.csv\",\n",
    "    start_date=None,\n",
    "    end_date=None,\n",
    "    max_articles=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Tr√≠ch xu·∫•t th·ª±c th·ªÉ t·ª´ tin t·ª©c ƒë√£ t√≥m t·∫Øt\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    csv_path : str\n",
    "        ƒê∆∞·ªùng d·∫´n ƒë·∫øn file CSV ch·ª©a tin t·ª©c ƒë√£ t√≥m t·∫Øt\n",
    "    output_path : str\n",
    "        ƒê∆∞·ªùng d·∫´n file CSV k·∫øt qu·∫£\n",
    "    start_date : str, optional\n",
    "        Ng√†y b·∫Øt ƒë·∫ßu (format: YYYY-MM-DD)\n",
    "    end_date : str, optional\n",
    "        Ng√†y k·∫øt th√∫c (format: YYYY-MM-DD)\n",
    "    max_articles : int, optional\n",
    "        S·ªë l∆∞·ª£ng b√†i b√°o t·ªëi ƒëa ƒë·ªÉ x·ª≠ l√Ω (ƒë·ªÉ test)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple: (entities_df, graph, canonical_entities)\n",
    "        - entities_df: DataFrame ch·ª©a c√°c th·ª±c th·ªÉ ƒë√£ tr√≠ch xu·∫•t\n",
    "        - graph: NetworkX graph ch·ª©a m·ªëi quan h·ªá\n",
    "        - canonical_entities: Set c√°c th·ª±c th·ªÉ canonical\n",
    "    \"\"\"\n",
    "    print(f\"üìñ ƒêang ƒë·ªçc d·ªØ li·ªáu t·ª´ {csv_path}...\")\n",
    "    \n",
    "    # ƒê·ªçc d·ªØ li·ªáu tin t·ª©c ƒë√£ t√≥m t·∫Øt v·ªõi error handling\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        print(f\"‚úì ƒê√£ ƒë·ªçc {len(df)} tin t·ª©c\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå Kh√¥ng t√¨m th·∫•y file: {csv_path}\")\n",
    "        print(f\"üí° C√°c file CSV c√≥ s·∫µn:\")\n",
    "        csv_files = [f for f in os.listdir('.') if f.endswith('.csv') and 'summarized' in f]\n",
    "        for f in csv_files:\n",
    "            print(f\"   - {f}\")\n",
    "        raise\n",
    "    \n",
    "    # Validate required columns\n",
    "    required_cols = ['postID', 'stockCodes', 'title', 'description', 'date']\n",
    "    missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        print(f\"‚ùå Dataset thi·∫øu c√°c c·ªôt: {missing_cols}\")\n",
    "        print(f\"üìã C√°c c·ªôt hi·ªán c√≥: {list(df.columns)}\")\n",
    "        raise ValueError(f\"Dataset kh√¥ng h·ª£p l·ªá: thi·∫øu c·ªôt {missing_cols}\")\n",
    "    \n",
    "    print(f\"‚úì Dataset h·ª£p l·ªá v·ªõi c√°c c·ªôt: {list(df.columns)}\")\n",
    "    \n",
    "    # Chuy·ªÉn ƒë·ªïi c·ªôt date sang datetime\n",
    "    df['parsed_date'] = pd.to_datetime(df['date'])\n",
    "    df['only_date'] = df['parsed_date'].dt.date\n",
    "    \n",
    "    # L·ªçc theo kho·∫£ng th·ªùi gian\n",
    "    if start_date:\n",
    "        start_dt = pd.to_datetime(start_date).date()\n",
    "        df = df[df['only_date'] >= start_dt]\n",
    "        print(f\"‚úì L·ªçc t·ª´ ng√†y {start_date}: c√≤n {len(df)} tin\")\n",
    "    \n",
    "    if end_date:\n",
    "        end_dt = pd.to_datetime(end_date).date()\n",
    "        df = df[df['only_date'] <= end_dt]\n",
    "        print(f\"‚úì L·ªçc ƒë·∫øn ng√†y {end_date}: c√≤n {len(df)} tin\")\n",
    "    \n",
    "    # Gi·ªõi h·∫°n s·ªë l∆∞·ª£ng n·∫øu c·∫ßn (ƒë·ªÉ test)\n",
    "    if max_articles:\n",
    "        df = df.head(max_articles)\n",
    "        print(f\"‚úì Gi·ªõi h·∫°n xu·ªëng {len(df)} tin ƒë·ªÉ x·ª≠ l√Ω\")\n",
    "    \n",
    "    # S·∫Øp x·∫øp theo th·ªùi gian\n",
    "    df = df.sort_values('date')\n",
    "    \n",
    "    # Kh·ªüi t·∫°o graph v√† canonical entities\n",
    "    G = nx.DiGraph()\n",
    "    canonical_entities = set()\n",
    "    \n",
    "    # Build portfolio string\n",
    "    portfolio_str_full = \", \".join([f\"{stock}-{sector}\" for stock, sector in zip(PORTFOLIO_STOCKS, PORTFOLIO_SECTOR)])\n",
    "    \n",
    "    # Kh·ªüi t·∫°o chain v·ªõi API manager\n",
    "    chain_entity = create_chains(api_manager)\n",
    "    \n",
    "    # K·∫øt qu·∫£\n",
    "    all_entities = []\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üîç B·∫ÆT ƒê·∫¶U TR√çCH XU·∫§T TH·ª∞C TH·ªÇ\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # X·ª≠ l√Ω t·ª´ng b√†i b√°o\n",
    "    for idx, row in df.iterrows():\n",
    "        article_idx = idx + 1\n",
    "        article_node = f\"Article_{article_idx}: {row['title']}\"\n",
    "        article_timestamp = row['parsed_date']\n",
    "        \n",
    "        # Th√™m node b√†i b√°o v√†o graph\n",
    "        if not G.has_node(article_node):\n",
    "            G.add_node(article_node, type=\"article\", timestamp=article_timestamp)\n",
    "        \n",
    "        print(f\"[{article_idx}/{len(df)}] üì∞ {row['title'][:60]}...\")\n",
    "        \n",
    "        # L·∫•y th√¥ng tin m√£ c·ªï phi·∫øu t·ª´ dataset\n",
    "        stock_codes = row.get('stockCodes', '')\n",
    "        if not stock_codes or pd.isna(stock_codes):\n",
    "            stock_codes = \"(kh√¥ng c√≥)\"\n",
    "        \n",
    "        # Phase 1: Extract initial entities\n",
    "        max_entity_retries = MAX_RETRIES\n",
    "        entity_retry_count = 0\n",
    "        entities_dict = {\"POSITIVE\": [], \"NEGATIVE\": []}\n",
    "        \n",
    "        while entity_retry_count < max_entity_retries:\n",
    "            prompt_text = {\n",
    "                \"portfolio\": portfolio_str_full,\n",
    "                \"date\": row['date'],\n",
    "                \"stockCodes\": stock_codes,\n",
    "                \"title\": row['title'],\n",
    "                \"description\": row['description'],\n",
    "                \"existing_entities\": graph_entities_to_str(G)\n",
    "            }\n",
    "            \n",
    "            response_text = invoke_chain_with_retry(chain_entity, prompt_text, api_manager)\n",
    "            time.sleep(1)  # Rate limiting\n",
    "            \n",
    "            if response_text is None:\n",
    "                print(f\"   ‚ùå B·ªè qua tin {article_idx} do l·ªói API\")\n",
    "                break\n",
    "            \n",
    "            entities_dict = parse_entity_response(response_text)\n",
    "            \n",
    "            # Check if we got any entities\n",
    "            total_entities = len(entities_dict.get(\"POSITIVE\", [])) + len(entities_dict.get(\"NEGATIVE\", []))\n",
    "            if total_entities > 0:\n",
    "                print(f\"   ‚úì Tr√≠ch xu·∫•t ƒë∆∞·ª£c {total_entities} th·ª±c th·ªÉ\")\n",
    "                break\n",
    "                \n",
    "            entity_retry_count += 1\n",
    "            print(f\"   ‚ö† Kh√¥ng c√≥ th·ª±c th·ªÉ. Th·ª≠ l·∫°i {entity_retry_count}/{max_entity_retries}\")\n",
    "            time.sleep(BASE_DELAY)\n",
    "        \n",
    "        if entity_retry_count == max_entity_retries and total_entities == 0:\n",
    "            print(f\"   ‚ùå Th·∫•t b·∫°i sau {max_entity_retries} l·∫ßn th·ª≠\")\n",
    "            continue\n",
    "        \n",
    "        # Process entities\n",
    "        for impact in [\"POSITIVE\", \"NEGATIVE\"]:\n",
    "            for ent, content in entities_dict.get(impact, []):\n",
    "                # Skip invalid entities\n",
    "                if not ent or \"kh√¥ng c√≥ th·ª±c th·ªÉ n√†o\" in ent.lower():\n",
    "                    continue\n",
    "                \n",
    "                # Normalize entity\n",
    "                canon_ent = merge_entity(ent, canonical_entities)\n",
    "                \n",
    "                # Determine node type\n",
    "                node_type = \"stock\" if any(str(canon_ent).lower().find(stock.lower()) != -1 for stock in PORTFOLIO_STOCKS) else \"entity\"\n",
    "                \n",
    "                # Add node to graph\n",
    "                if not G.has_node(canon_ent):\n",
    "                    G.add_node(canon_ent, type=node_type, timestamp=article_timestamp)\n",
    "                \n",
    "                # Add edge from article to entity\n",
    "                if not G.has_edge(article_node, canon_ent):\n",
    "                    G.add_edge(article_node, canon_ent, impact=impact, timestamp=article_timestamp)\n",
    "                \n",
    "                # L∆∞u v√†o k·∫øt qu·∫£\n",
    "                all_entities.append({\n",
    "                    \"article_id\": article_idx,\n",
    "                    \"article_title\": row['title'],\n",
    "                    \"date\": row['date'],\n",
    "                    \"entity\": canon_ent,\n",
    "                    \"entity_type\": node_type,\n",
    "                    \"impact\": impact,\n",
    "                    \"explanation\": content\n",
    "                })\n",
    "    \n",
    "    # T·∫°o DataFrame k·∫øt qu·∫£\n",
    "    entities_df = pd.DataFrame(all_entities)\n",
    "    \n",
    "    # L∆∞u file\n",
    "    entities_df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    # Export to tuples CSV for analysis\n",
    "    print(f\"\\nüì¶ ƒêang export graph tuples...\")\n",
    "    tuples_data = []\n",
    "    for u, v, data in G.edges(data=True):\n",
    "        if not u.startswith(\"Article_\"):  # Only export non-article edges\n",
    "            tuples_data.append({\n",
    "                'date': data.get('timestamp', ''),\n",
    "                'source': u,\n",
    "                'impact': data.get('impact', ''),\n",
    "                'target': v\n",
    "            })\n",
    "    \n",
    "    if tuples_data:\n",
    "        tuples_df = pd.DataFrame(tuples_data)\n",
    "        tuples_path = output_path.replace('.csv', '_tuples.csv')\n",
    "        tuples_df.to_csv(tuples_path, index=False, encoding='utf-8-sig')\n",
    "        print(f\"‚úì ƒê√£ export {len(tuples_df)} tuples v√†o: {tuples_path}\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"‚úÖ HO√ÄN TH√ÄNH!\")\n",
    "    print(f\"üìä T·ªïng s·ªë entities: {len(entities_df)}\")\n",
    "    print(f\"üîπ Unique entities: {len(canonical_entities)}\")\n",
    "    print(f\"üìà Graph nodes: {len(G.nodes())}\")\n",
    "    print(f\"üîó Graph edges: {len(G.edges())}\")\n",
    "    print(f\"üíæ ƒê√£ l∆∞u v√†o: {output_path}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    return entities_df, G, canonical_entities\n",
    "\n",
    "print(\"‚úì ƒê√£ ƒë·ªãnh nghƒ©a h√†m extract_entities_from_news()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea251fe5",
   "metadata": {},
   "source": [
    "## üöÄ Ch·∫°y tr√≠ch xu·∫•t th·ª±c th·ªÉ\n",
    "\n",
    "### H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng:\n",
    "\n",
    "1. **Test v·ªõi s·ªë l∆∞·ª£ng nh·ªè:** Th·ª≠ v·ªõi `max_articles=10` ƒë·ªÉ ki·ªÉm tra\n",
    "2. **Ch·∫°y ƒë·∫ßy ƒë·ªß:** B·ªè `max_articles` ƒë·ªÉ x·ª≠ l√Ω to√†n b·ªô\n",
    "3. **L·ªçc theo th·ªùi gian:** D√πng `start_date` v√† `end_date`\n",
    "\n",
    "### V√≠ d·ª•:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb6e205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test v·ªõi 10 tin t·ª©c ƒë·∫ßu ti√™n\n",
    "entities_df, G, canonical_entities = extract_entities_from_news(\n",
    "    csv_path=\"summarized_news_with_stocks_merged.csv\",  # File merged\n",
    "    output_path=\"entities_extracted.csv\",\n",
    "    max_articles=10  # Test v·ªõi 10 tin tr∆∞·ªõc\n",
    ")\n",
    "\n",
    "# ƒê·ªÉ ch·∫°y ƒë·∫ßy ƒë·ªß:\n",
    "# entities_df, G, canonical_entities = extract_entities_from_news(\n",
    "#     csv_path=\"summarized_news_with_stocks_merged.csv\",\n",
    "#     output_path=\"entities_extracted.csv\",\n",
    "#     start_date=\"2025-01-01\",  # T√πy ch·ªçn\n",
    "#     end_date=None\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a79ca8e",
   "metadata": {},
   "source": [
    "## üìä Ph√¢n t√≠ch k·∫øt qu·∫£\n",
    "\n",
    "Xem c√°c th·ª±c th·ªÉ ƒë√£ tr√≠ch xu·∫•t:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6611160e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xem t·ªïng quan\n",
    "print(f\"üìä T·ªîNG QUAN K·∫æT QU·∫¢\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "print(f\"T·ªïng s·ªë entities tr√≠ch xu·∫•t: {len(entities_df)}\")\n",
    "print(f\"Unique entities: {len(canonical_entities)}\")\n",
    "print(f\"Graph nodes: {len(G.nodes())}\")\n",
    "print(f\"Graph edges: {len(G.edges())}\")\n",
    "\n",
    "print(f\"\\nüìà PH√ÇN LO·∫†I THEO IMPACT:\\n\")\n",
    "print(entities_df['impact'].value_counts())\n",
    "\n",
    "print(f\"\\nüìå PH√ÇN LO·∫†I THEO ENTITY TYPE:\\n\")\n",
    "print(entities_df['entity_type'].value_counts())\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"M·∫™U 10 ENTITIES ƒê·∫¶U TI√äN:\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "for idx, row in entities_df.head(10).iterrows():\n",
    "    print(f\"[{row['article_id']}] üì∞ {row['article_title'][:50]}...\")\n",
    "    print(f\"üè∑Ô∏è  Entity: {row['entity']} ({row['entity_type']})\")\n",
    "    print(f\"{'‚úÖ' if row['impact'] == 'POSITIVE' else '‚ùå'} Impact: {row['impact']}\")\n",
    "    print(f\"üìù {row['explanation'][:100]}...\")\n",
    "    print(f\"{'-'*60}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c143bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xem c√°c entity ƒë∆∞·ª£c ƒë·ªÅ c·∫≠p nhi·ªÅu nh·∫•t\n",
    "from collections import Counter\n",
    "\n",
    "entity_counts = Counter(entities_df['entity'])\n",
    "\n",
    "print(f\"üî• TOP 15 ENTITIES ƒê∆Ø·ª¢C ƒê·ªÄ C·∫¨P NHI·ªÄU NH·∫§T:\\n\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "for i, (entity, count) in enumerate(entity_counts.most_common(15), 1):\n",
    "    # ƒê·∫øm positive v√† negative\n",
    "    pos_count = len(entities_df[(entities_df['entity'] == entity) & (entities_df['impact'] == 'POSITIVE')])\n",
    "    neg_count = len(entities_df[(entities_df['entity'] == entity) & (entities_df['impact'] == 'NEGATIVE')])\n",
    "    \n",
    "    print(f\"{i:2d}. {entity}\")\n",
    "    print(f\"    T·ªïng: {count} l·∫ßn | ‚úÖ {pos_count} | ‚ùå {neg_count}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2d87c6",
   "metadata": {},
   "source": [
    "## üîç T√¨m ki·∫øm v√† ph√¢n t√≠ch\n",
    "\n",
    "### T√¨m th√¥ng tin v·ªÅ m·ªôt entity c·ª• th·ªÉ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2a08a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_entity(entity_name, entities_df):\n",
    "    \"\"\"\n",
    "    T√¨m ki·∫øm t·∫•t c·∫£ th√¥ng tin v·ªÅ m·ªôt entity\n",
    "    \"\"\"\n",
    "    # T√¨m ki·∫øm case-insensitive\n",
    "    results = entities_df[entities_df['entity'].str.lower().str.contains(entity_name.lower(), na=False)]\n",
    "    \n",
    "    if len(results) == 0:\n",
    "        print(f\"‚ùå Kh√¥ng t√¨m th·∫•y entity: {entity_name}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"üîç T√åM TH·∫§Y {len(results)} MENTIONS V·ªÄ '{entity_name.upper()}'\\n\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    for idx, row in results.iterrows():\n",
    "        print(f\"[{row['article_id']}] üì∞ {row['article_title']}\")\n",
    "        print(f\"üìÖ {row['date']}\")\n",
    "        print(f\"üè∑Ô∏è  {row['entity']} ({row['entity_type']})\")\n",
    "        print(f\"{'‚úÖ' if row['impact'] == 'POSITIVE' else '‚ùå'} {row['impact']}\")\n",
    "        print(f\"üìù {row['explanation']}\")\n",
    "        print(f\"{'-'*60}\\n\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# V√≠ d·ª•: T√¨m th√¥ng tin v·ªÅ FPT\n",
    "# search_entity(\"FPT\", entities_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505062d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ph√¢n t√≠ch Graph: Xem c√°c entities k·∫øt n·ªëi v·ªõi nhau\n",
    "def analyze_graph(G):\n",
    "    \"\"\"\n",
    "    Ph√¢n t√≠ch knowledge graph ƒë√£ x√¢y d·ª±ng\n",
    "    \"\"\"\n",
    "    print(f\"üìä PH√ÇN T√çCH KNOWLEDGE GRAPH\\n\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Th·ªëng k√™ c∆° b·∫£n\n",
    "    print(f\"T·ªïng s·ªë nodes: {len(G.nodes())}\")\n",
    "    print(f\"T·ªïng s·ªë edges: {len(G.edges())}\")\n",
    "    \n",
    "    # Ph√¢n lo·∫°i nodes\n",
    "    node_types = {}\n",
    "    for node, data in G.nodes(data=True):\n",
    "        node_type = data.get('type', 'unknown')\n",
    "        node_types[node_type] = node_types.get(node_type, 0) + 1\n",
    "    \n",
    "    print(f\"\\nüìå Ph√¢n lo·∫°i nodes:\")\n",
    "    for ntype, count in node_types.items():\n",
    "        print(f\"   {ntype}: {count}\")\n",
    "    \n",
    "    # T√¨m nodes c√≥ nhi·ªÅu k·∫øt n·ªëi nh·∫•t (degree centrality)\n",
    "    entity_nodes = [n for n, d in G.nodes(data=True) if d.get('type') != 'article']\n",
    "    \n",
    "    if entity_nodes:\n",
    "        # In-degree: s·ªë l∆∞·ª£ng b√†i b√°o li√™n k·∫øt ƒë·∫øn entity n√†y\n",
    "        in_degrees = [(node, G.in_degree(node)) for node in entity_nodes]\n",
    "        in_degrees.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        print(f\"\\nüî• TOP 10 ENTITIES ƒê∆Ø·ª¢C NH·∫ÆC ƒê·∫æN NHI·ªÄU NH·∫§T (b·ªüi c√°c b√†i b√°o):\\n\")\n",
    "        for i, (node, degree) in enumerate(in_degrees[:10], 1):\n",
    "            print(f\"{i:2d}. {node}: {degree} b√†i b√°o\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "\n",
    "# Ch·∫°y ph√¢n t√≠ch\n",
    "analyze_graph(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b70144",
   "metadata": {},
   "source": [
    "## üíæ L∆∞u Graph ƒë·ªÉ s·ª≠ d·ª•ng sau\n",
    "\n",
    "L∆∞u graph v√†o file pickle ƒë·ªÉ s·ª≠ d·ª•ng cho c√°c b∆∞·ªõc ti·∫øp theo (relation extraction, attention mechanism...):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcc08b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# L∆∞u graph\n",
    "with open('knowledge_graph.pkl', 'wb') as f:\n",
    "    pickle.dump(G, f)\n",
    "print(\"‚úì ƒê√£ l∆∞u knowledge graph v√†o 'knowledge_graph.pkl'\")\n",
    "\n",
    "# L∆∞u canonical entities\n",
    "with open('canonical_entities.pkl', 'wb') as f:\n",
    "    pickle.dump(canonical_entities, f)\n",
    "print(\"‚úì ƒê√£ l∆∞u canonical_entities v√†o 'canonical_entities.pkl'\")\n",
    "\n",
    "# ƒê·ªÉ load l·∫°i sau:\n",
    "# with open('knowledge_graph.pkl', 'rb') as f:\n",
    "#     G = pickle.load(f)\n",
    "# with open('canonical_entities.pkl', 'rb') as f:\n",
    "#     canonical_entities = pickle.load(f)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
