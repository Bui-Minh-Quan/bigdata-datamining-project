{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "734e43e5",
   "metadata": {},
   "source": [
    "# ü§ñ Entity Extractor - TRR Step 1: Tr√≠ch xu·∫•t th·ª±c th·ªÉ & M·ªëi quan h·ªá\n",
    "\n",
    "## ‚ú® PHI√äN B·∫¢N M·ªöI - HO√ÄN CH·ªàNH GI·ªêNG TRR_MODEL\n",
    "\n",
    "Notebook n√†y ƒë√£ ƒë∆∞·ª£c c·∫£i ti·∫øn ƒë·ªÉ c√≥ c·∫•u tr√∫c graph y h·ªát TRR_model v·ªõi:\n",
    "\n",
    "### üéØ 3 Prompt Templates:\n",
    "1. **Entity Extraction** - Tr√≠ch xu·∫•t entities ban ƒë·∫ßu\n",
    "2. **Relation Extraction** - Tr√≠ch xu·∫•t m·ªëi quan h·ªá cho 1 entity\n",
    "3. **Batch Relation Extraction** - X·ª≠ l√Ω nhi·ªÅu entities c√πng l√∫c\n",
    "\n",
    "### üîÑ Workflow ƒê·∫ßy ƒê·ªß:\n",
    "- **Phase 1:** Entity Extraction t·ª´ b√†i b√°o\n",
    "- **Phase 2:** Relation Extraction v·ªõi iterations (max 2 rounds)\n",
    "- **Batch Processing:** X·ª≠ l√Ω nhi·ªÅu entities trong m·ªôt API call\n",
    "\n",
    "### üì¶ Output Files:\n",
    "1. `entities_extracted.csv` - DataFrame ch·ª©a entities v·ªõi iteration info\n",
    "2. `knowledge_graph.pkl` - NetworkX DiGraph\n",
    "3. `canonical_entities.pkl` - Set c√°c entity canonical\n",
    "4. `graph_tuples_step1.csv` - **CSV tuples (date, source, impact, target)**\n",
    "\n",
    "## Input:\n",
    "- File CSV: `summarized_news_with_stocks_merged.csv`\n",
    "- Columns: `postID`, `stockCodes`, `title`, `description`, `date`\n",
    "- Start date: `2025-01-01`\n",
    "\n",
    "## Thay ƒê·ªïi So V·ªõi Phi√™n B·∫£n C≈©:\n",
    "- ‚úÖ Th√™m 2 prompt templates m·ªõi (relation extraction, batch relation)\n",
    "- ‚úÖ Th√™m workflow iterations gi·ªëng TRR\n",
    "- ‚úÖ Th√™m batch processing ƒë·ªÉ t·ªëi ∆∞u API calls\n",
    "- ‚úÖ Export CSV tuples v·ªõi format chu·∫©n `(date, source, impact, target)`\n",
    "- ‚úÖ S·ª≠ d·ª•ng `stockCodes` t·ª´ dataset thay v√¨ `group`\n",
    "- ‚úÖ L∆∞u iteration info cho m·ªói entity\n",
    "- ‚úÖ C·∫•u tr√∫c graph ho√†n ch·ªânh v·ªõi article nodes, entity nodes, stock nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c679e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì ƒê√£ import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt (bao g·ªìm multithreading)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "import google.generativeai as genai\n",
    "import networkx as nx\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from threading import Lock\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "print(\"‚úì ƒê√£ import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt (bao g·ªìm multithreading)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68c32e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì ƒê√£ ƒë·ªãnh nghƒ©a APIKeyManager\n",
      "‚úì Ph√°t hi·ªán 4 API keys\n",
      "üîë ƒêang s·ª≠ d·ª•ng: GOOGLE_API_KEY (Key 1/4)\n"
     ]
    }
   ],
   "source": [
    "class APIKeyManager:\n",
    "    \"\"\"\n",
    "    Qu·∫£n l√Ω nhi·ªÅu Google API keys v√† t·ª± ƒë·ªông chuy·ªÉn ƒë·ªïi khi g·∫∑p l·ªói.\n",
    "    \n",
    "    Quy t·∫Øc:\n",
    "    - M·ªói key ƒë∆∞·ª£c th·ª≠ t·ªëi ƒëa 2 l·∫ßn\n",
    "    - Sau 2 l·∫ßn l·ªói ‚Üí t·ª± ƒë·ªông chuy·ªÉn key ti·∫øp theo\n",
    "    - H·∫øt key ‚Üí b√°o l·ªói\n",
    "    \"\"\"\n",
    "    \n",
    "    MAX_RETRIES_PER_KEY = 2  # S·ªë l·∫ßn th·ª≠ t·ªëi ƒëa cho m·ªói key\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Kh·ªüi t·∫°o v√† load t·∫•t c·∫£ API keys t·ª´ .env\"\"\"\n",
    "        # Load c√°c API keys t·ª´ environment\n",
    "        self.keys = [\n",
    "            (\"GOOGLE_API_KEY\", os.getenv(\"GOOGLE_API_KEY\")),\n",
    "            (\"GOOGLE_API_KEY_2\", os.getenv(\"GOOGLE_API_KEY_2\")),\n",
    "            (\"GOOGLE_API_KEY_3\", os.getenv(\"GOOGLE_API_KEY_3\")),\n",
    "            (\"GOOGLE_API_KEY_4\", os.getenv(\"GOOGLE_API_KEY_4\")),\n",
    "        ]\n",
    "        \n",
    "        # Ch·ªâ gi·ªØ l·∫°i c√°c key h·ª£p l·ªá (kh√¥ng None)\n",
    "        self.keys = [(name, key) for name, key in self.keys if key]\n",
    "        \n",
    "        if not self.keys:\n",
    "            raise ValueError(\"‚ùå Kh√¥ng t√¨m th·∫•y API key! Ki·ªÉm tra file .env\")\n",
    "        \n",
    "        # Kh·ªüi t·∫°o tr·∫°ng th√°i\n",
    "        self.current_index = 0\n",
    "        self.error_counts = {name: 0 for name, _ in self.keys}  # ƒê·∫øm l·ªói m·ªói key\n",
    "        \n",
    "        print(f\"‚úì Ph√°t hi·ªán {len(self.keys)} API keys\")\n",
    "        self._activate_key(0)\n",
    "    \n",
    "    def _activate_key(self, index):\n",
    "        \"\"\"K√≠ch ho·∫°t API key t·∫°i v·ªã tr√≠ index\"\"\"\n",
    "        if index >= len(self.keys):\n",
    "            raise Exception(\"‚ùå ƒê√£ h·∫øt t·∫•t c·∫£ API keys!\")\n",
    "        \n",
    "        self.current_index = index\n",
    "        key_name, key_value = self.keys[index]\n",
    "        \n",
    "        # C·∫•u h√¨nh Google AI v·ªõi key m·ªõi\n",
    "        genai.configure(api_key=key_value)\n",
    "        \n",
    "        print(f\"üîë ƒêang s·ª≠ d·ª•ng: {key_name} (Key {index + 1}/{len(self.keys)})\")\n",
    "    \n",
    "    def get_current_key(self):\n",
    "        \"\"\"L·∫•y API key hi·ªán t·∫°i\"\"\"\n",
    "        return self.keys[self.current_index][1]\n",
    "    \n",
    "    def get_models(self):\n",
    "        \"\"\"\n",
    "        T·∫°o c√°c model AI v·ªõi API key hi·ªán t·∫°i.\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (model, model_more_temp, model_pro)\n",
    "        \"\"\"\n",
    "        current_key = self.get_current_key()\n",
    "        \n",
    "        model = ChatGoogleGenerativeAI(\n",
    "            model=\"gemini-2.5-flash-lite\", \n",
    "            temperature=0.15,\n",
    "            google_api_key=current_key\n",
    "        )\n",
    "        \n",
    "        model_more_temp = ChatGoogleGenerativeAI(\n",
    "            model=\"gemini-2.5-flash-lite\", \n",
    "            temperature=0.2,\n",
    "            google_api_key=current_key\n",
    "        )\n",
    "        \n",
    "        model_pro = ChatGoogleGenerativeAI(\n",
    "            model=\"gemini-2.5-pro\", \n",
    "            temperature=0.1,\n",
    "            google_api_key=current_key\n",
    "        )\n",
    "        \n",
    "        return model, model_more_temp, model_pro\n",
    "    \n",
    "    def on_error(self):\n",
    "        \"\"\"\n",
    "        X·ª≠ l√Ω khi g·∫∑p l·ªói API.\n",
    "        \n",
    "        Returns:\n",
    "            bool: True n·∫øu c√≤n key kh√°c ƒë·ªÉ th·ª≠, False n·∫øu h·∫øt key\n",
    "        \"\"\"\n",
    "        key_name = self.keys[self.current_index][0]\n",
    "        self.error_counts[key_name] += 1\n",
    "        \n",
    "        error_count = self.error_counts[key_name]\n",
    "        print(f\"‚ö† L·ªói l·∫ßn {error_count} v·ªõi {key_name}\")\n",
    "        \n",
    "        # N·∫øu ƒë√£ ƒë·∫°t gi·ªõi h·∫°n retry cho key n√†y\n",
    "        if error_count >= self.MAX_RETRIES_PER_KEY:\n",
    "            print(f\"‚õî {key_name} ƒë√£ l·ªói {error_count}/{self.MAX_RETRIES_PER_KEY} l·∫ßn\")\n",
    "            \n",
    "            # Th·ª≠ chuy·ªÉn sang key ti·∫øp theo\n",
    "            next_index = self.current_index + 1\n",
    "            if next_index < len(self.keys):\n",
    "                print(f\"üîÑ Chuy·ªÉn sang key ti·∫øp theo...\")\n",
    "                self._activate_key(next_index)\n",
    "                return True\n",
    "            else:\n",
    "                print(f\"‚ùå ƒê√£ h·∫øt t·∫•t c·∫£ {len(self.keys)} keys!\")\n",
    "                return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def reset_errors(self):\n",
    "        \"\"\"Reset error counters cho t·∫•t c·∫£ keys\"\"\"\n",
    "        self.error_counts = {name: 0 for name, _ in self.keys}\n",
    "        print(\"‚ôª ƒê√£ reset error counters\")\n",
    "\n",
    "print(\"‚úì ƒê√£ ƒë·ªãnh nghƒ©a APIKeyManager\")\n",
    "\n",
    "# Kh·ªüi t·∫°o API Manager\n",
    "api_manager = APIKeyManager()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851c0369",
   "metadata": {},
   "source": [
    "## üìù Prompt Templates (Ho√†n ch·ªânh nh∆∞ TRR Model)\n",
    "\n",
    "Notebook n√†y ƒë√£ ƒë∆∞·ª£c c·∫£i ti·∫øn ƒë·ªÉ c√≥ c·∫•u tr√∫c graph y h·ªát TRR_model v·ªõi 3 prompt templates:\n",
    "\n",
    "### 1. Entity Extraction Prompt\n",
    "- **M·ª•c ƒë√≠ch:** Tr√≠ch xu·∫•t c√°c th·ª±c th·ªÉ ban ƒë·∫ßu t·ª´ b√†i b√°o\n",
    "- **Input:** `date`, `stockCodes`, `title`, `description`, `portfolio`, `existing_entities`\n",
    "- **Output:** `[[POSITIVE]]` v√† `[[NEGATIVE]]` entities v·ªõi gi·∫£i th√≠ch\n",
    "- **ƒê·∫∑c ƒëi·ªÉm:** \n",
    "  - S·ª≠ d·ª•ng `stockCodes` t·ª´ dataset thay v√¨ `group`\n",
    "  - Focus v√†o 10 m√£ c·ªï phi·∫øu trong portfolio\n",
    "  - B·∫Øt bu·ªôc tr√≠ch d·∫´n s·ªë li·ªáu c·ª• th·ªÉ\n",
    "\n",
    "### 2. Relation Extraction Prompt\n",
    "- **M·ª•c ƒë√≠ch:** Tr√≠ch xu·∫•t m·ªëi quan h·ªá hi·ªáu ·ª©ng d√¢y chuy·ªÅn t·ª´ m·ªôt th·ª±c th·ªÉ\n",
    "- **Input:** `entities` (th·ª±c th·ªÉ g·ªëc), `description`, `portfolio`, `existing_entities`\n",
    "- **Output:** C√°c th·ª±c th·ªÉ b·ªã ·∫£nh h∆∞·ªüng b·ªüi th·ª±c th·ªÉ g·ªëc\n",
    "- **Gi·ªõi h·∫°n:** T·ªëi ƒëa 2 th·ª±c th·ªÉ m·ªõi, li√™n k·∫øt t·ªõi 4 th·ª±c th·ªÉ kh√°c\n",
    "\n",
    "### 3. Batch Relation Extraction Prompt\n",
    "- **M·ª•c ƒë√≠ch:** X·ª≠ l√Ω nhi·ªÅu th·ª±c th·ªÉ c√πng l√∫c (hi·ªáu qu·∫£ h∆°n)\n",
    "- **Input:** `input_entities` (nhi·ªÅu th·ª±c th·ªÉ g·ªëc), `portfolio`, `existing_entities`\n",
    "- **Output:** M·ªëi quan h·ªá cho t·ª´ng th·ª±c th·ªÉ g·ªëc theo ƒë·ªãnh d·∫°ng `[[SOURCE:...]]`\n",
    "- **Gi·ªõi h·∫°n:** 2 th·ª±c th·ªÉ m·ªõi/th·ª±c th·ªÉ g·ªëc, li√™n k·∫øt t·ªõi 3 th·ª±c th·ªÉ/th·ª±c th·ªÉ g·ªëc\n",
    "\n",
    "## üîÑ Workflow (Gi·ªëng TRR Model)\n",
    "\n",
    "1. **Phase 1: Entity Extraction** - Tr√≠ch xu·∫•t entities ban ƒë·∫ßu t·ª´ b√†i b√°o\n",
    "2. **Phase 2: Relation Extraction** - L·∫∑p qua `max_iterations` ƒë·ªÉ x√¢y d·ª±ng m·ªëi quan h·ªá\n",
    "   - M·ªói iteration x·ª≠ l√Ω entities trong batches\n",
    "   - Entities m·ªõi ƒë∆∞·ª£c th√™m v√†o frontier cho iteration ti·∫øp theo\n",
    "3. **Output:** \n",
    "   - `entities_extracted.csv` - DataFrame ch·ª©a t·∫•t c·∫£ entities\n",
    "   - `knowledge_graph.pkl` - NetworkX DiGraph\n",
    "   - `canonical_entities.pkl` - Set c√°c entity canonical\n",
    "   - `graph_tuples_step1.csv` - CSV tuples `(date, source, impact, target)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af71d02d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì ƒê√£ ƒë·ªãnh nghƒ©a 3 prompt templates: entity_extraction, relation_extraction, batch_relation_extraction\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# PROMPT TEMPLATES - Tr√≠ch xu·∫•t th·ª±c th·ªÉ v√† m·ªëi quan h·ªá\n",
    "# ============================================================\n",
    "\n",
    "# Template tr√≠ch xu·∫•t th·ª±c th·ªÉ t·ª´ tin t·ª©c\n",
    "entity_extraction_template = PromptTemplate.from_template(\"\"\"B·∫°n ƒë∆∞·ª£c cho m·ªôt ho·∫∑c nhi·ªÅu b√†i b√°o, bao g·ªìm t·ª±a ƒë·ªÅ v√† m√¥ t·∫£ ng·∫Øn g·ªçn v·ªÅ b√†i b√°o ƒë√≥, ngo√†i ra b·∫°n c√≥\n",
    "th√¥ng tin v·ªÅ ng√†y xu·∫•t b·∫£n c·ªßa b√†i b√°o, v√† lo·∫°i ch·ªß ƒë·ªÅ m√† b√†i b√°o ƒëang ƒë·ªÅ c·∫≠p t·ªõi.\n",
    "\n",
    "L∆∞u √Ω [QUAN TR·ªåNG, kh√¥ng ƒë∆∞·ª£c b·ªè qua]: \n",
    "   - H·∫°n ch·∫ø t·∫°o m·ªõi m·ªôt th·ª±c th·ªÉ, ch·ªâ t·∫°o li√™n k·∫øt t·ªõi 5 th·ª±c th·ªÉ. \n",
    "   - Lu√¥n ∆∞u ti√™n li√™n k·∫øt v·ªõi c√°c th·ª±c th·ªÉ ƒë√£ c√≥: {existing_entities}\n",
    "\n",
    "B·∫°n c·∫ßn ph√¢n t√≠ch b√†i b√°o, ƒë∆∞a ra t√™n c·ªßa nh·ªØng th·ª±c th·ªÉ (v√≠ d·ª• nh∆∞ c·ªï phi·∫øu, ng√†nh ngh·ªÅ, c√¥ng ty, qu·ªëc gia, t·ªânh th√†nh...)\n",
    "s·∫Ω b·ªã ·∫£nh h∆∞·ªüng tr·ª±c ti·∫øp b·ªüi th√¥ng tin c·ªßa b√†i b√°o, theo h∆∞·ªõng t√≠ch c·ª±c ho·∫∑c ti√™u c·ª±c.\n",
    "\n",
    "V·ªõi m·ªói th·ª±c th·ªÉ, ·ªü ph·∫ßn T√™n th·ª±c th·ªÉ, h·∫°n ch·∫ø d√πng d·∫•u ch·∫•m, g·∫°ch ngang, d·∫•u v√† &, d·∫•u ch·∫•m ph·∫©y ;. V√† c·∫ßn ghi th√™m qu·ªëc gia, ƒë·ªãa ph∆∞∆°ng c·ª• th·ªÉ v√† ng√†nh ngh·ªÅ c·ªßa n√≥ (n·∫øu c√≥).\n",
    "T√™n ch·ªâ n√≥i t·ªõi m·ªôt th·ª±c th·ªÉ duy nh·∫•t. Ph·∫ßn T√™n kh√¥ng ƒë∆∞·ª£c qu√° ph·ª©c t·∫°p, ƒë∆°n gi·∫£n nh·∫•t c√≥ th·ªÉ.\n",
    "N·∫øu th·ª±c th·ªÉ n√†o thu·ªôc danh m·ª•c c·ªï phi·∫øu sau: {portfolio}, h√£y ghi r√µ t√™n c·ªï phi·∫øu.\n",
    "V√≠ d·ª•: SSI Ch·ª©ng kho√°n; Ng√†nh c√¥ng nghi·ªáp Vi·ªát Nam; Ng∆∞·ªùi d√πng M·ªπ; Ng√†nh th√©p Ch√¢u √Å; Ng√†nh du l·ªãch H·∫° Long, ...\n",
    "\n",
    "Ghi nh·ªõ, H·∫°n ch·∫ø t·∫°o m·ªõi m·ªôt th·ª±c th·ªÉ, ch·ªâ t·∫°o li√™n k·∫øt t·ªõi 5 th·ª±c th·ªÉ. Lu√¥n c·ªë li√™n k·∫øt v·ªõi c√°c th·ª±c th·ªÉ ƒë√£ c√≥.\n",
    "\n",
    "Ph·∫ßn gi·∫£i th√≠ch m·ªói th·ª±c th·ªÉ, b·∫Øt bu·ªôc ƒë√°nh gi√° s·ªë li·ªáu ƒë∆∞·ª£c ghi, nhi·ªÅu ho·∫∑c √≠t, tƒÉng ho·∫∑c gi·∫£m, g·∫•p bao nhi√™u l·∫ßn, ...\n",
    "C·∫ßn c·ªë g·∫Øng li√™n k·∫øt v·ªõi nhi·ªÅu th·ª±c th·ªÉ kh√°c. Tuy nhi√™n kh√¥ng suy ngo√†i ph·∫°m vi b√†i b√°o. Kh√¥ng t·ª± ch√®n s·ªë li·ªáu ngo√†i b√†i b√°o.\n",
    "Kh√¥ng d√πng d·∫•u hai ch·∫•m trong ph·∫ßn gi·∫£i th√≠ch, ch·ªâ d√πng hai ch·∫•m : ƒë·ªÉ t√°ch gi·ªØa T√™n th·ª±c th·ªÉ v√† ph·∫ßn gi·∫£i th√≠ch.\n",
    "                                                          \n",
    "ƒê∆∞a ra theo ƒë·ªãnh d·∫°ng sau:\n",
    "[[POSITIVE]]\n",
    "[Entity 1]: [Explanation]\n",
    "...\n",
    "[Entity N]: [Explanation]\n",
    "\n",
    "[[NEGATIVE]]\n",
    "[Entity A]: [Explanation]\n",
    "..\n",
    "[Entity Z]: [Explanation]\n",
    "                                                          \n",
    "M·ªôt v√≠ d·ª• cho b√†i b√°o:\n",
    "\n",
    "(B·∫ÆT ƒê·∫¶U V√ç D·ª§)\n",
    "\n",
    "Ng√†y ƒëƒÉng: 2025-01-01T00:00:00+07:00\n",
    "M√£ c·ªï phi·∫øu li√™n quan: (kh√¥ng c√≥)\n",
    "T·ª±a ƒë·ªÅ: S·ªë l∆∞·ª£ng h√≥a ƒë∆°n kh·ªüi t·∫°o t·ª´ m√°y t√≠nh ti·ªÅn tƒÉng g·∫•p 13 l·∫ßn nƒÉm 2023\n",
    "\n",
    "M√¥ t·∫£: T·ª∑ l·ªá c∆° s·ªü kinh doanh s·ª≠ d·ª•ng h√≥a ƒë∆°n ƒëi·ªán t·ª≠ tƒÉng m·∫°nh, v·ªõi s·ªë l∆∞·ª£ng h√≥a ƒë∆°n t·ª´ m√°y t√≠nh ti·ªÅn tƒÉng g·∫•p 13 l·∫ßn so v·ªõi nƒÉm tr∆∞·ªõc. Ng√†nh b√°n l·∫ª v√† d·ªãch v·ª• h∆∞·ªüng l·ª£i l·ªõn t·ª´ chuy·ªÉn ƒë·ªïi s·ªë n√†y.\n",
    "\n",
    "Danh s√°ch th·ª±c th·ªÉ s·∫Ω b·ªã ·∫£nh h∆∞·ªüng:\n",
    "\n",
    "[[POSITIVE]]\n",
    "Ng√†nh b√°n l·∫ª Vi·ªát Nam: S·ªë l∆∞·ª£ng h√≥a ƒë∆°n ƒëi·ªán t·ª≠ t·ª´ m√°y t√≠nh ti·ªÅn tƒÉng g·∫•p 13 l·∫ßn trong nƒÉm 2023, gi√∫p tƒÉng hi·ªáu qu·∫£ qu·∫£n l√Ω v√† gi·∫£m chi ph√≠ v·∫≠n h√†nh\n",
    "MWG B√°n l·∫ª: L√† chu·ªói b√°n l·∫ª l·ªõn, h∆∞·ªüng l·ª£i tr·ª±c ti·∫øp t·ª´ vi·ªác s·ªë h√≥a h√≥a ƒë∆°n tƒÉng 13 l·∫ßn, c·∫£i thi·ªán kh·∫£ nƒÉng qu·∫£n l√Ω t·ªìn kho v√† d√≤ng ti·ªÅn\n",
    "Ng√†nh c√¥ng ngh·ªá Vi·ªát Nam: Cung c·∫•p gi·∫£i ph√°p h√≥a ƒë∆°n ƒëi·ªán t·ª≠ v√† m√°y t√≠nh ti·ªÅn cho h√†ng ngh√¨n c∆° s·ªü kinh doanh, doanh thu d·ª± ki·∫øn tƒÉng m·∫°nh\n",
    "FPT C√¥ng ngh·ªá: L√† nh√† cung c·∫•p gi·∫£i ph√°p chuy·ªÉn ƒë·ªïi s·ªë h√†ng ƒë·∫ßu, h∆∞·ªüng l·ª£i t·ª´ nhu c·∫ßu tri·ªÉn khai h√≥a ƒë∆°n ƒëi·ªán t·ª≠ tƒÉng ƒë·ªôt bi·∫øn\n",
    "\n",
    "[[NEGATIVE]]\n",
    "(Kh√¥ng c√≥ th·ª±c th·ªÉ b·ªã ·∫£nh h∆∞·ªüng ti√™u c·ª±c r√µ r√†ng t·ª´ b√†i b√°o n√†y)\n",
    "\n",
    "(K·∫æT TH√öC V√ç D·ª§)\n",
    "\n",
    "Ng√†y ƒëƒÉng: {date}\n",
    "M√£ c·ªï phi·∫øu li√™n quan: {stockCodes}\n",
    "T·ª±a ƒë·ªÅ: {title}\n",
    "\n",
    "M√¥ t·∫£: {description}\n",
    "\n",
    "Danh s√°ch th·ª±c th·ªÉ s·∫Ω b·ªã ·∫£nh h∆∞·ªüng:\n",
    "\"\"\")\n",
    "\n",
    "# Template tr√≠ch xu·∫•t m·ªëi quan h·ªá (relation extraction)\n",
    "relation_extraction_template = PromptTemplate.from_template(\"\"\"B·∫°n ƒëang l√†m vi·ªác d∆∞·ªõi b·ªëi c·∫£nh ph√¢n t√≠ch kinh t·∫ø.                                                            \n",
    "H·∫°n ch·∫ø t·∫°o m·ªõi m·ªôt th·ª±c th·ªÉ, ch·ªâ ƒë∆∞·ª£c t·∫°o m·ªõi t·ªëi ƒëa 2 th·ª±c th·ªÉ m·ªõi. Ch·ªâ ƒë∆∞·ª£c li√™n k·∫øt t·ªõi 4 th·ª±c th·ªÉ kh√°c. Lu√¥n ∆∞u ti√™n li√™n k·∫øt v·ªõi c√°c th·ª±c th·ªÉ ƒë√£ c√≥: {existing_entities}\n",
    "\n",
    "D·ª±a tr√™n t√°c ƒë·ªông ƒë·∫øn m·ªôt th·ª±c th·ªÉ, h√£y li·ªát k√™ c√°c th·ª±c th·ªÉ s·∫Ω b·ªã ·∫£nh h∆∞·ªüng ti√™u c·ª±c v√† ·∫£nh h∆∞·ªüng t√≠ch c·ª±c do hi·ªáu ·ª©ng d√¢y chuy·ªÅn.\n",
    "H√£y suy lu·∫≠n xem th·ª±c th·ªÉ hi·ªán t·∫°i n√†y c√≥ th·ªÉ ·∫£nh h∆∞·ªüng ti·∫øp ƒë·∫øn nh·ªØng th·ª±c th·ªÉ kh√°c n√†o, theo h∆∞·ªõng t√≠ch c·ª±c ho·∫∑c ti√™u c·ª±c.\n",
    "                                                            \n",
    "V·ªõi m·ªói th·ª±c th·ªÉ, ·ªü ph·∫ßn T√™n th·ª±c th·ªÉ, h·∫°n ch·∫ø d√πng d·∫•u ch·∫•m, g·∫°ch ngang, d·∫•u v√† &, d·∫•u ch·∫•m ph·∫©y ;. C·∫ßn ghi th√™m qu·ªëc gia, ƒë·ªãa ph∆∞∆°ng c·ª• th·ªÉ v√† ng√†nh ngh·ªÅ c·ªßa n√≥ (n·∫øu c√≥). \n",
    "T√™n ch·ªâ n√≥i t·ªõi m·ªôt th·ª±c th·ªÉ duy nh·∫•t. Ph·∫ßn T√™n kh√¥ng ƒë∆∞·ª£c qu√° ph·ª©c t·∫°p, ƒë∆°n gi·∫£n nh·∫•t c√≥ th·ªÉ.\n",
    "N·∫øu th·ª±c th·ªÉ n√†o thu·ªôc danh m·ª•c c·ªï phi·∫øu sau: {portfolio}, h√£y ghi r√µ t√™n c·ªï phi·∫øu.\n",
    "V√≠ d·ª•: SSI Ch·ª©ng kho√°n; Ng√†nh c√¥ng nghi·ªáp Vi·ªát Nam; Ng∆∞·ªùi d√πng M·ªπ; Ng√†nh th√©p Ch√¢u √Å; Ng√†nh du l·ªãch H·∫° Long, ...\n",
    "\n",
    "Ghi nh·ªõ, H·∫°n ch·∫ø t·∫°o m·ªõi th·ª±c th·ªÉ, ch·ªâ ƒë∆∞·ª£c t·∫°o m·ªõi t·ªëi ƒëa 2 th·ª±c th·ªÉ m·ªõi. Ch·ªâ ƒë∆∞·ª£c li√™n k·∫øt t·ªõi 4 th·ª±c th·ªÉ kh√°c. Lu√¥n c·ªë li√™n k·∫øt v·ªõi c√°c th·ª±c th·ªÉ ƒë√£ c√≥.\n",
    "\n",
    "Ph·∫ßn gi·∫£i th√≠ch m·ªói th·ª±c th·ªÉ, b·∫Øt bu·ªôc ƒë√°nh gi√° s·ªë li·ªáu ƒë∆∞·ª£c ghi, nhi·ªÅu ho·∫∑c √≠t, tƒÉng ho·∫∑c gi·∫£m, g·∫•p bao nhi√™u l·∫ßn, ...\n",
    "C·∫ßn c·ªë g·∫Øng li√™n k·∫øt v·ªõi nhi·ªÅu th·ª±c th·ªÉ kh√°c. Tuy nhi√™n kh√¥ng suy ngo√†i ph·∫°m vi b√†i b√°o. Kh√¥ng t·ª± ch√®n s·ªë li·ªáu ngo√†i b√†i b√°o.\n",
    "Kh√¥ng d√πng d·∫•u hai ch·∫•m trong ph·∫ßn gi·∫£i th√≠ch, ch·ªâ d√πng hai ch·∫•m : ƒë·ªÉ t√°ch gi·ªØa T√™n th·ª±c th·ªÉ v√† ph·∫ßn gi·∫£i th√≠ch.\n",
    "\n",
    "ƒê∆∞a ra theo ƒë·ªãnh d·∫°ng sau:\n",
    "[[POSITIVE]]\n",
    "[Entity 1]: [Explanation]\n",
    "...\n",
    "[Entity N]: [Explanation]\n",
    "\n",
    "[[NEGATIVE]]\n",
    "[Entity A]: [Explanation]\n",
    "..\n",
    "[Entity Z]: [Explanation]\n",
    "\n",
    "(B·∫ÆT ƒê·∫¶U V√ç D·ª§)\n",
    "\n",
    "Th·ª±c th·ªÉ g·ªëc: B·ªô X√¢y d·ª±ng Vi·ªát Nam\n",
    "\n",
    "·∫¢nh h∆∞·ªüng: √Åp l·ª±c qu·∫£n l√Ω 28 d·ª± √°n v·ªõi t·ªïng chi·ªÅu d√†i 1188 km, nh·∫±m hi·ªán th·ª±c h√≥a m·ª•c ti√™u ƒë·∫°t 3000 km cao t·ªëc v√†o nƒÉm 2025. S·ªë l∆∞·ª£ng d·ª± √°n tƒÉng g·∫•p nhi·ªÅu l·∫ßn so v·ªõi giai ƒëo·∫°n tr∆∞·ªõc, ƒë√≤i h·ªèi ƒëi·ªÅu ph·ªëi ngu·ªìn l·ª±c v√† ki·ªÉm so√°t ti·∫øn ƒë·ªô ch·∫∑t ch·∫Ω h∆°n.\n",
    "\n",
    "Danh s√°ch th·ª±c th·ªÉ s·∫Ω b·ªã ·∫£nh h∆∞·ªüng b·ªüi hi·ªáu ·ª©ng d√¢y chuy·ªÅn:\n",
    "\n",
    "[[POSITIVE]]\n",
    "Doanh nghi·ªáp x√¢y d·ª±ng Vi·ªát Nam: C√≥ c∆° h·ªôi m·ªü r·ªông h·ª£p ƒë·ªìng thi c√¥ng, tƒÉng doanh thu nh·ªù s·ªë l∆∞·ª£ng d·ª± √°n cao t·ªëc l·ªõn ƒëang tri·ªÉn khai ƒë·ªìng lo·∫°t.\n",
    "Ng∆∞·ªùi lao ƒë·ªông Vi·ªát Nam: C√≥ th√™m nhi·ªÅu c∆° h·ªôi vi·ªác l√†m t·ª´ c√°c d·ª± √°n thi c√¥ng tr·∫£i d√†i kh·∫Øp c·∫£ n∆∞·ªõc.\n",
    "\n",
    "[[NEGATIVE]]\n",
    "B·ªô Giao th√¥ng V·∫≠n t·∫£i Vi·ªát Nam: Ch·ªãu √°p l·ª±c ph·ªëi h·ª£p v√† gi√°m s√°t hi·ªáu qu·∫£ gi·ªØa c√°c b√™n li√™n quan, c√≥ nguy c∆° b·ªã ch·ªâ tr√≠ch n·∫øu d·ª± √°n ch·∫≠m ti·∫øn ƒë·ªô.\n",
    "Doanh nghi·ªáp x√¢y d·ª±ng Vi·ªát Nam: C√≥ th·ªÉ ch·ªãu √°p l·ª±c tƒÉng gi√° nguy√™n v·∫≠t li·ªáu v√† thi·∫øu h·ª•t ngu·ªìn cung do nhu c·∫ßu tƒÉng ƒë·ªôt bi·∫øn.\n",
    "\n",
    "(K·∫æT TH√öC V√ç D·ª§)\n",
    "\n",
    "Th·ª±c th·ªÉ g·ªëc: {entities}\n",
    "\n",
    "·∫¢nh h∆∞·ªüng: {description}\n",
    "\n",
    "Danh s√°ch th·ª±c th·ªÉ s·∫Ω b·ªã ·∫£nh h∆∞·ªüng b·ªüi hi·ªáu ·ª©ng d√¢y chuy·ªÅn:\n",
    "\"\"\")\n",
    "\n",
    "# Template batch relation extraction (x·ª≠ l√Ω nhi·ªÅu th·ª±c th·ªÉ c√πng l√∫c)\n",
    "batch_relation_extraction_template = PromptTemplate.from_template(\"\"\"B·∫°n ƒëang l√†m vi·ªác d∆∞·ªõi b·ªëi c·∫£nh ph√¢n t√≠ch kinh t·∫ø.\n",
    "H·∫°n ch·∫ø t·∫°o m·ªõi th·ª±c th·ªÉ, ch·ªâ ƒë∆∞·ª£c t·∫°o m·ªõi t·ªëi ƒëa 2 th·ª±c th·ªÉ m·ªõi cho m·ªói th·ª±c th·ªÉ g·ªëc. Ch·ªâ ƒë∆∞·ª£c li√™n k·∫øt t·ªëi ƒëa 3 th·ª±c th·ªÉ kh√°c cho m·ªói th·ª±c th·ªÉ g·ªëc. Lu√¥n ∆∞u ti√™n li√™n k·∫øt v·ªõi c√°c th·ª±c th·ªÉ ƒë√£ c√≥: {existing_entities}\n",
    "\n",
    "D·ª±a tr√™n t√°c ƒë·ªông ƒë·∫øn c√°c th·ª±c th·ªÉ ƒë·∫ßu v√†o, h√£y ph√¢n t√≠ch hi·ªáu ·ª©ng d√¢y chuy·ªÅn. \n",
    "H√£y suy lu·∫≠n xem m·ªói th·ª±c th·ªÉ hi·ªán t·∫°i c√≥ th·ªÉ ·∫£nh h∆∞·ªüng ti·∫øp ƒë·∫øn nh·ªØng th·ª±c th·ªÉ kh√°c n√†o, theo h∆∞·ªõng t√≠ch c·ª±c ho·∫∑c ti√™u c·ª±c.\n",
    "\n",
    "V·ªõi m·ªói th·ª±c th·ªÉ, ·ªü ph·∫ßn T√™n th·ª±c th·ªÉ, h·∫°n ch·∫ø d√πng d·∫•u ch·∫•m, g·∫°ch ngang, d·∫•u v√† &, d·∫•u ch·∫•m ph·∫©y ;. C·∫ßn ghi th√™m qu·ªëc gia, ƒë·ªãa ph∆∞∆°ng c·ª• th·ªÉ v√† ng√†nh ngh·ªÅ c·ªßa n√≥ (n·∫øu c√≥).\n",
    "T√™n ch·ªâ n√≥i t·ªõi m·ªôt th·ª±c th·ªÉ duy nh·∫•t. Ph·∫ßn T√™n kh√¥ng ƒë∆∞·ª£c qu√° ph·ª©c t·∫°p, ƒë∆°n gi·∫£n nh·∫•t c√≥ th·ªÉ.\n",
    "N·∫øu th·ª±c th·ªÉ n√†o thu·ªôc danh m·ª•c c·ªï phi·∫øu sau: {portfolio}, h√£y ghi r√µ t√™n c·ªï phi·∫øu.\n",
    "V√≠ d·ª•: SSI Ch·ª©ng kho√°n; Ng√†nh c√¥ng nghi·ªáp Vi·ªát Nam; Ng∆∞·ªùi d√πng M·ªπ; Ng√†nh th√©p Ch√¢u √Å; Ng√†nh du l·ªãch H·∫° Long, ...\n",
    "\n",
    "Ph·∫ßn gi·∫£i th√≠ch m·ªói th·ª±c th·ªÉ, b·∫Øt bu·ªôc ƒë√°nh gi√° s·ªë li·ªáu ƒë∆∞·ª£c ghi, nhi·ªÅu ho·∫∑c √≠t, tƒÉng ho·∫∑c gi·∫£m, g·∫•p bao nhi√™u l·∫ßn...\n",
    "C·∫ßn c·ªë g·∫Øng li√™n k·∫øt v·ªõi nhi·ªÅu th·ª±c th·ªÉ kh√°c. Tuy nhi√™n kh√¥ng suy ngo√†i ph·∫°m vi b√†i b√°o. Kh√¥ng t·ª± ch√®n s·ªë li·ªáu ngo√†i b√†i b√°o.\n",
    "Kh√¥ng d√πng d·∫•u hai ch·∫•m trong ph·∫ßn gi·∫£i th√≠ch, ch·ªâ d√πng hai ch·∫•m : ƒë·ªÉ t√°ch gi·ªØa T√™n th·ª±c th·ªÉ v√† ph·∫ßn gi·∫£i th√≠ch.\n",
    "\n",
    "ƒê∆∞a ra theo ƒë·ªãnh d·∫°ng sau cho m·ªói th·ª±c th·ªÉ ngu·ªìn:\n",
    "\n",
    "[[SOURCE: T√™n th·ª±c th·ªÉ ngu·ªìn]]\n",
    "[[IMPACT: POSITIVE/NEGATIVE]]\n",
    "\n",
    "[[POSITIVE]]\n",
    "[Th·ª±c th·ªÉ ·∫£nh h∆∞·ªüng 1]: [Gi·∫£i th√≠ch]\n",
    "[Th·ª±c th·ªÉ ·∫£nh h∆∞·ªüng 2]: [Gi·∫£i th√≠ch]\n",
    "[Th·ª±c th·ªÉ ·∫£nh h∆∞·ªüng 3]: [Gi·∫£i th√≠ch]\n",
    "\n",
    "[[NEGATIVE]]\n",
    "[Th·ª±c th·ªÉ ·∫£nh h∆∞·ªüng A]: [Gi·∫£i th√≠ch]\n",
    "[Th·ª±c th·ªÉ ·∫£nh h∆∞·ªüng B]: [Gi·∫£i th√≠ch]\n",
    "[Th·ª±c th·ªÉ ·∫£nh h∆∞·ªüng C]: [Gi·∫£i th√≠ch]\n",
    "\n",
    "\n",
    "L∆ØU √ù [R·∫§T QUAN TR·ªåNG]:\n",
    "   - C√≥ th·ªÉ c√≥ R·∫§T NHI·ªÄU th·ª±c th·ªÉ ƒë·∫ßu v√†o, h√£y ph√¢n t√≠ch C·∫®N TH·∫¨N t·ª´ng th·ª±c th·ªÉ ƒë·ªÉ kh√¥ng b·ªè s√≥t. Kh√¥ng ƒë∆∞·ª£c t·∫°o th√™m th·ª±c th·ªÉ g·ªëc. \n",
    "   - B·∫°n s·∫Ω ph√¢n t√≠ch nhi·ªÅu th·ª±c th·ªÉ g·ªëc m·ªôt l√∫c. V·ªõi T·ª™NG th·ª±c th·ªÉ, ch·ªâ ch·ªçn CH√çNH X√ÅC 2-3 th·ª±c th·ªÉ ·∫£nh h∆∞·ªüng t√≠ch c·ª±c nh·∫•t v√† 2-3 th·ª±c th·ªÉ ·∫£nh h∆∞·ªüng ti√™u c·ª±c quan tr·ªçng nh·∫•t.\n",
    "   - Th·ª±c th·ªÉ ngu·ªìn trong [[SOURCE: ...]] CH·ªà ch·ª©a T√äN TH·ª∞C TH·ªÇ G·ªêC t·ª´ danh s√°ch ƒë·∫ßu v√†o, KH√îNG ƒë∆∞·ª£c th√™m b·∫•t k·ª≥ th√¥ng tin n√†o kh√°c t·ª´ ph·∫ßn \"·∫¢nh h∆∞·ªüng\" ho·∫∑c gi·∫£i th√≠ch.\n",
    "                                                                  \n",
    "(B·∫ÆT ƒê·∫¶U V√ç D·ª§)\n",
    "Danh s√°ch th·ª±c th·ªÉ ngu·ªìn:\n",
    "\n",
    "Th·ª±c th·ªÉ g·ªëc: B·ªô X√¢y d·ª±ng Vi·ªát Nam\n",
    "\n",
    "·∫¢nh h∆∞·ªüng: NEGATIVE, √Åp l·ª±c qu·∫£n l√Ω 28 d·ª± √°n v·ªõi t·ªïng chi·ªÅu d√†i 1188 km, nh·∫±m hi·ªán th·ª±c h√≥a m·ª•c ti√™u ƒë·∫°t 3000 km cao t·ªëc v√†o nƒÉm 2025. S·ªë l∆∞·ª£ng d·ª± √°n tƒÉng g·∫•p nhi·ªÅu l·∫ßn so v·ªõi giai ƒëo·∫°n tr∆∞·ªõc, ƒë√≤i h·ªèi ƒëi·ªÅu ph·ªëi ngu·ªìn l·ª±c v√† ki·ªÉm so√°t ti·∫øn ƒë·ªô ch·∫∑t ch·∫Ω h∆°n.\n",
    "\n",
    "---\n",
    "\n",
    "Danh s√°ch th·ª±c th·ªÉ s·∫Ω b·ªã ·∫£nh h∆∞·ªüng b·ªüi hi·ªáu ·ª©ng d√¢y chuy·ªÅn:\n",
    "\n",
    "[[SOURCE: B·ªô X√¢y d·ª±ng Vi·ªát Nam]]\n",
    "[[IMPACT: NEGATIVE]]\n",
    "\n",
    "[[POSITIVE]]\n",
    "Doanh nghi·ªáp x√¢y d·ª±ng Vi·ªát Nam: C√≥ c∆° h·ªôi m·ªü r·ªông h·ª£p ƒë·ªìng thi c√¥ng, tƒÉng doanh thu nh·ªù s·ªë l∆∞·ª£ng d·ª± √°n cao t·ªëc l·ªõn ƒëang tri·ªÉn khai ƒë·ªìng lo·∫°t.\n",
    "Ng∆∞·ªùi lao ƒë·ªông Vi·ªát Nam: C√≥ th√™m nhi·ªÅu c∆° h·ªôi vi·ªác l√†m t·ª´ c√°c d·ª± √°n thi c√¥ng tr·∫£i d√†i kh·∫Øp c·∫£ n∆∞·ªõc.\n",
    "\n",
    "[[NEGATIVE]]\n",
    "B·ªô Giao th√¥ng V·∫≠n t·∫£i Vi·ªát Nam: Ch·ªãu √°p l·ª±c ph·ªëi h·ª£p v√† gi√°m s√°t hi·ªáu qu·∫£ gi·ªØa c√°c b√™n li√™n quan, c√≥ nguy c∆° b·ªã ch·ªâ tr√≠ch n·∫øu d·ª± √°n ch·∫≠m ti·∫øn ƒë·ªô.\n",
    "Doanh nghi·ªáp x√¢y d·ª±ng Vi·ªát Nam: C√≥ th·ªÉ ch·ªãu √°p l·ª±c tƒÉng gi√° nguy√™n v·∫≠t li·ªáu v√† thi·∫øu h·ª•t ngu·ªìn cung do nhu c·∫ßu tƒÉng ƒë·ªôt bi·∫øn.\n",
    "\n",
    "(K·∫æT TH√öC V√ç D·ª§)\n",
    "\n",
    "Danh s√°ch th·ª±c th·ªÉ ngu·ªìn:\n",
    "\n",
    "{input_entities}\n",
    "\n",
    "Danh s√°ch th·ª±c th·ªÉ s·∫Ω b·ªã ·∫£nh h∆∞·ªüng b·ªüi hi·ªáu ·ª©ng d√¢y chuy·ªÅn:\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úì ƒê√£ ƒë·ªãnh nghƒ©a 3 prompt templates: entity_extraction, relation_extraction, batch_relation_extraction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0bcc5e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì ƒê√£ ƒë·ªãnh nghƒ©a portfolio, constants v√† thread locks\n"
     ]
    }
   ],
   "source": [
    "PORTFOLIO_STOCKS = [\"FPT\", \"SSI\", \"VCB\", \"VHM\", \"HPG\", \"GAS\", \"MSN\", \"MWG\", \"GVR\", \"VIC\"]\n",
    "PORTFOLIO_SECTOR = [\"C√¥ng ngh·ªá\", \"Ch·ª©ng kho√°n\", \"Ng√¢n h√†ng\", \"B·∫•t ƒë·ªông s·∫£n\", \"V·∫≠t li·ªáu c∆° b·∫£n\", \n",
    "                     \"D·ªãch v·ª• H·∫° t·∫ßng\", \"Ti√™u d√πng c∆° b·∫£n\", \"B√°n l·∫ª\", \"Ch·∫ø bi·∫øn\", \"B·∫•t ƒë·ªông s·∫£n\"]\n",
    "BASE_DELAY = 30\n",
    "MAX_RETRIES = 3\n",
    "MAX_WORKERS = 5  # S·ªë l∆∞·ª£ng threads x·ª≠ l√Ω ƒë·ªìng th·ªùi\n",
    "\n",
    "# Global locks for thread safety\n",
    "graph_lock = Lock()\n",
    "canonical_lock = Lock()\n",
    "results_lock = Lock()\n",
    "\n",
    "def create_chains(api_manager):\n",
    "    \"\"\"\n",
    "    T·∫°o chains v·ªõi models t·ª´ APIKeyManager\n",
    "    \"\"\"\n",
    "    model, model_more_temp, model_pro = api_manager.get_models()\n",
    "    \n",
    "    # T·∫°o 2 chains\n",
    "    chain_entity = entity_extraction_template | model\n",
    "    chain_batch_relation = batch_relation_extraction_template | model\n",
    "    \n",
    "    return chain_entity, chain_batch_relation\n",
    "\n",
    "print(\"‚úì ƒê√£ ƒë·ªãnh nghƒ©a portfolio, constants v√† thread locks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c320c01e",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Utility Functions\n",
    "\n",
    "C√°c h√†m h·ªó tr·ª£:\n",
    "1. `invoke_chain_with_retry()` - G·ªçi API v·ªõi retry t·ª± ƒë·ªông\n",
    "2. `parse_entity_response()` - Parse response t·ª´ LLM\n",
    "3. `merge_entity()` - Chu·∫©n h√≥a t√™n entity\n",
    "4. `graph_entities_to_str()` - Convert graph entities th√†nh string cho prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65b81888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì ƒê√£ ƒë·ªãnh nghƒ©a c√°c h√†m ti·ªán √≠ch\n"
     ]
    }
   ],
   "source": [
    "def invoke_chain_with_retry(chain, prompt, api_manager, base_delay=BASE_DELAY):\n",
    "    \"\"\"\n",
    "    G·ªçi chain v·ªõi c∆° ch·∫ø retry t·ª± ƒë·ªông v√† t√≠ch h·ª£p APIKeyManager\n",
    "    \"\"\"\n",
    "    total_attempts = 0\n",
    "    max_total_attempts = len(api_manager.keys) * api_manager.MAX_RETRIES_PER_KEY\n",
    "    \n",
    "    while total_attempts < max_total_attempts:\n",
    "        try:\n",
    "            # Th·ª≠ g·ªçi API\n",
    "            response = chain.invoke(prompt)\n",
    "            \n",
    "            # Th√†nh c√¥ng -> reset error count\n",
    "            api_manager.reset_errors()\n",
    "            return response\n",
    "            \n",
    "        except Exception as e:\n",
    "            total_attempts += 1\n",
    "            error_msg = str(e)\n",
    "            \n",
    "            # B√°o l·ªói cho API manager\n",
    "            switched = api_manager.on_error()\n",
    "            \n",
    "            if total_attempts >= max_total_attempts:\n",
    "                print(f\"‚ùå ƒê√£ th·ª≠ t·∫•t c·∫£ {len(api_manager.keys)} API keys ({total_attempts} l·∫ßn) nh∆∞ng v·∫´n l·ªói\")\n",
    "                print(f\"   L·ªói cu·ªëi: {error_msg}\")\n",
    "                return None\n",
    "            \n",
    "            # Ch·ªù tr∆∞·ªõc khi retry\n",
    "            if switched:\n",
    "                delay = base_delay\n",
    "                print(f\"‚è≥ ƒê·ª£i {delay}s tr∆∞·ªõc khi th·ª≠ key m·ªõi...\")\n",
    "            else:\n",
    "                key_name = api_manager.keys[api_manager.current_index][0]\n",
    "                retry_num = api_manager.error_counts.get(key_name, 0)\n",
    "                delay = base_delay * (1.5 ** (retry_num - 1))\n",
    "                print(f\"‚è≥ ƒê·ª£i {delay:.0f}s tr∆∞·ªõc khi retry ({retry_num}/{api_manager.MAX_RETRIES_PER_KEY})...\")\n",
    "            \n",
    "            time.sleep(delay)\n",
    "\n",
    "def parse_entity_response(response):\n",
    "    \"\"\"\n",
    "    Ph√¢n t√≠ch response t·ª´ entity extraction prompt\n",
    "    \n",
    "    Returns:\n",
    "        dict: {\"POSITIVE\": [(entity, explanation), ...], \"NEGATIVE\": [(entity, explanation), ...]}\n",
    "    \"\"\"\n",
    "    if response is None:\n",
    "        print(\"Response is None\")\n",
    "        return {\"POSITIVE\": [], \"NEGATIVE\": []}\n",
    "        \n",
    "    sections = {\"POSITIVE\": [], \"NEGATIVE\": []}\n",
    "    current_section = None\n",
    "    str_resp = response.content\n",
    "    \n",
    "    for line in str(str_resp).splitlines():\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        if \"[[POSITIVE]]\" in line.upper():\n",
    "            current_section = \"POSITIVE\"\n",
    "            continue\n",
    "        if \"[[NEGATIVE]]\" in line.upper():\n",
    "            current_section = \"NEGATIVE\"\n",
    "            continue\n",
    "        if current_section and ':' in line:\n",
    "            entity = line.split(\":\", 1)[0].strip()\n",
    "            # Skip invalid entities\n",
    "            if not entity or \"kh√¥ng c√≥ th·ª±c th·ªÉ n√†o\" in entity.lower():\n",
    "                continue\n",
    "            # content = all line except entity\n",
    "            content = line.split(entity, 1)[-1].strip(':').strip()\n",
    "            sections[current_section].append((entity, content))\n",
    "\n",
    "    return sections\n",
    "\n",
    "def parse_batch_entity_response(response):\n",
    "    \"\"\"\n",
    "    Ph√¢n t√≠ch response t·ª´ batch relation extraction prompt\n",
    "    \n",
    "    Returns:\n",
    "        list: [(source_entity, impact, target_entity, content), ...]\n",
    "    \"\"\"\n",
    "    if response is None:\n",
    "        print(\"Response is None\")\n",
    "        return []\n",
    "        \n",
    "    results = []\n",
    "    current_source = None\n",
    "    current_impact = None\n",
    "    current_section = None\n",
    "    \n",
    "    str_resp = str(response.content)\n",
    "    lines = str_resp.splitlines()\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "            \n",
    "        # Check for source entity marker\n",
    "        if line.startswith(\"[[SOURCE:\") or \"[[SOURCE:\" in line:\n",
    "            source_text = line.replace(\"[[SOURCE:\", \"\").replace(\"]]\", \"\").strip()\n",
    "            if source_text and \"kh√¥ng c√≥ th·ª±c th·ªÉ n√†o\" not in source_text.lower():\n",
    "                current_source = source_text\n",
    "            else:\n",
    "                current_source = None\n",
    "            continue\n",
    "            \n",
    "        # Check for impact marker\n",
    "        if line.startswith(\"[[IMPACT:\") or \"[[IMPACT:\" in line:\n",
    "            impact_str = line.replace(\"[[IMPACT:\", \"\").replace(\"]]\", \"\").strip()\n",
    "            current_impact = impact_str.upper()\n",
    "            continue\n",
    "            \n",
    "        # Check for positive/negative section markers\n",
    "        if \"[[POSITIVE]]\" in line.upper():\n",
    "            current_section = \"POSITIVE\"\n",
    "            continue\n",
    "            \n",
    "        if \"[[NEGATIVE]]\" in line.upper():\n",
    "            current_section = \"NEGATIVE\"\n",
    "            continue\n",
    "            \n",
    "        # Process entity and explanation\n",
    "        if current_source and current_section and ':' in line:\n",
    "            try:\n",
    "                entity, *content_parts = line.split(\":\", 1)\n",
    "                entity = entity.strip().strip('[]')\n",
    "                \n",
    "                if not entity or \"kh√¥ng c√≥ th·ª±c th·ªÉ n√†o\" in entity.lower():\n",
    "                    continue\n",
    "                    \n",
    "                if entity and content_parts:\n",
    "                    content = content_parts[0].strip()\n",
    "                    actual_impact = current_impact if current_impact else current_section\n",
    "                    results.append((current_source, actual_impact, entity, content))\n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing line: {line}. Error: {e}\")\n",
    "                continue\n",
    "    \n",
    "    if not results:\n",
    "        print(\"Warning: No relationships were parsed from the response\")\n",
    "        \n",
    "    return results\n",
    "\n",
    "def merge_entity(entity, canonical_set):\n",
    "    \"\"\"\n",
    "    Tr·∫£ v·ªÅ phi√™n b·∫£n canonical c·ªßa entity n·∫øu ƒë√£ t·ªìn t·∫°i (case-insensitive),\n",
    "    n·∫øu kh√¥ng th√¨ th√™m v√† tr·∫£ v·ªÅ entity m·ªõi.\n",
    "    \n",
    "    FIX: ƒê·∫£m b·∫£o t√™n entity ƒë∆∞·ª£c l∆∞u v√† tr·∫£ v·ªÅ nh·∫•t qu√°n (gi·ªØ nguy√™n vi·∫øt hoa/th∆∞·ªùng c·ªßa l·∫ßn ƒë·∫ßu)\n",
    "    \"\"\"\n",
    "    # Chu·∫©n h√≥a: b·ªè d·∫•u ngo·∫∑c vu√¥ng, kho·∫£ng tr·∫Øng th·ª´a\n",
    "    cleaned_entity = str(entity).strip('[').strip(']').strip()\n",
    "    normalized_lower = cleaned_entity.lower()\n",
    "    \n",
    "    # T√¨m entity ƒë√£ t·ªìn t·∫°i (case-insensitive)\n",
    "    for existing_entity in canonical_set:\n",
    "        if existing_entity.lower() == normalized_lower:\n",
    "            # Tr·∫£ v·ªÅ entity ƒë√£ t·ªìn t·∫°i (gi·ªØ nguy√™n vi·∫øt hoa/th∆∞·ªùng c·ªßa l·∫ßn ƒë·∫ßu)\n",
    "            return existing_entity\n",
    "    \n",
    "    # Ch∆∞a t·ªìn t·∫°i ‚Üí th√™m entity m·ªõi (GI·ªÆ NGUY√äN vi·∫øt hoa/th∆∞·ªùng g·ªëc)\n",
    "    canonical_set.add(cleaned_entity)\n",
    "    return cleaned_entity\n",
    "\n",
    "def add_edge(G, source, target, impact, timestamp):\n",
    "    \"\"\"\n",
    "    Th√™m edge v√†o graph n·∫øu ch∆∞a t·ªìn t·∫°i\n",
    "    \"\"\"\n",
    "    if not G.has_edge(source, target):\n",
    "        G.add_edge(source, target, impact=impact, timestamp=timestamp)\n",
    "\n",
    "def graph_entities_to_str(G, max_entities=50):\n",
    "    \"\"\"\n",
    "    Chuy·ªÉn ƒë·ªïi c√°c entities trong graph th√†nh chu·ªói ƒë·ªÉ ƒë∆∞a v√†o prompt\n",
    "    \"\"\"\n",
    "    entities = [node for node in G.nodes() if not node.startswith(\"Article_\")]\n",
    "    # Gi·ªõi h·∫°n s·ªë l∆∞·ª£ng ƒë·ªÉ kh√¥ng l√†m prompt qu√° d√†i\n",
    "    entities = entities[:max_entities]\n",
    "    return \", \".join(entities) if entities else \"Ch∆∞a c√≥ th·ª±c th·ªÉ n√†o\"\n",
    "\n",
    "def graph_to_tuples(G):\n",
    "    \"\"\"\n",
    "    Chuy·ªÉn ƒë·ªïi graph th√†nh list c√°c tuples (date, source, impact, target)\n",
    "    \"\"\"\n",
    "    tuples = []\n",
    "    for u, v, data in G.edges(data=True):\n",
    "        # Skip article nodes\n",
    "        if u.startswith(\"Article_\"):\n",
    "            continue\n",
    "            \n",
    "        timestamp = data.get(\"timestamp\")\n",
    "        if timestamp is None:\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            # Handle different timestamp formats\n",
    "            if isinstance(timestamp, pd.Timestamp):\n",
    "                date_str = timestamp.date().isoformat()\n",
    "            elif hasattr(timestamp, \"date\"):\n",
    "                date_str = timestamp.date().isoformat()\n",
    "            elif isinstance(timestamp, (int, float)):\n",
    "                date_str = pd.Timestamp(timestamp, unit='s').date().isoformat()\n",
    "            else:\n",
    "                parsed_date = pd.to_datetime(timestamp)\n",
    "                date_str = parsed_date.date().isoformat()\n",
    "                \n",
    "            # Skip invalid entities\n",
    "            if \"kh√¥ng c√≥ th·ª±c th·ªÉ n√†o\" in str(u).lower() or \"kh√¥ng c√≥ th·ª±c th·ªÉ n√†o\" in str(v).lower():\n",
    "                continue\n",
    "                \n",
    "            tuples.append(f\"({date_str}, {u}, {data.get('impact')} TO, {v})\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing edge ({u}, {v}): {e}\")\n",
    "            continue\n",
    "\n",
    "    return \"\\n\".join(sorted(tuples))\n",
    "\n",
    "print(\"‚úì ƒê√£ ƒë·ªãnh nghƒ©a c√°c h√†m ti·ªán √≠ch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf93f87",
   "metadata": {},
   "source": [
    "## üéØ Main Function: Extract Entities from News\n",
    "\n",
    "H√†m ch√≠nh tr√≠ch xu·∫•t entities t·ª´ CSV tin t·ª©c ƒë√£ t√≥m t·∫Øt.\n",
    "\n",
    "**Features:**\n",
    "- ‚úÖ Validation dataset tr∆∞·ªõc khi ch·∫°y\n",
    "- ‚úÖ Error handling v·ªõi th√¥ng b√°o chi ti·∫øt\n",
    "- ‚úÖ Export to tuples CSV cho ph√¢n t√≠ch\n",
    "- ‚úÖ Progress tracking v√† logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "855534aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì ƒê√£ ƒë·ªãnh nghƒ©a h√†m x·ª≠ l√Ω multithreading\n"
     ]
    }
   ],
   "source": [
    "def process_article_thread_safe(article_idx, row, G, canonical_entities, portfolio_str_full, \n",
    "                                 chain_entity, chain_batch_relation, api_manager, \n",
    "                                 max_iterations=2, batch_size=5):\n",
    "    \"\"\"\n",
    "    X·ª≠ l√Ω m·ªôt b√†i b√°o trong thread ri√™ng - Thread-safe version\n",
    "    \n",
    "    Returns:\n",
    "        list: Danh s√°ch entities ƒë∆∞·ª£c tr√≠ch xu·∫•t t·ª´ b√†i b√°o n√†y\n",
    "    \"\"\"\n",
    "    article_results = []\n",
    "    \n",
    "    article_node = f\"Article_{article_idx}: {row['title']}\"\n",
    "    article_timestamp = row['parsed_date']\n",
    "    \n",
    "    # Thread-safe: Add article node\n",
    "    with graph_lock:\n",
    "        if not G.has_node(article_node):\n",
    "            G.add_node(article_node, type=\"article\", timestamp=article_timestamp)\n",
    "    \n",
    "    print(f\"\\n[{article_idx}] üì∞ {row['title'][:60]}...\")\n",
    "    \n",
    "    # Get stock codes\n",
    "    stock_codes = row.get('stockCodes', '')\n",
    "    if not stock_codes or pd.isna(stock_codes):\n",
    "        stock_codes = \"(kh√¥ng c√≥)\"\n",
    "    \n",
    "    # ====================================\n",
    "    # Phase 1: Extract initial entities\n",
    "    # ====================================\n",
    "    print(f\"   üìå Phase 1: Tr√≠ch xu·∫•t entities ban ƒë·∫ßu...\")\n",
    "    max_entity_retries = MAX_RETRIES\n",
    "    entity_retry_count = 0\n",
    "    entities_dict = {\"POSITIVE\": [], \"NEGATIVE\": []}\n",
    "    \n",
    "    while entity_retry_count < max_entity_retries:\n",
    "        with graph_lock:\n",
    "            existing_entities = graph_entities_to_str(G)\n",
    "        \n",
    "        prompt_text = {\n",
    "            \"portfolio\": portfolio_str_full,\n",
    "            \"date\": row['date'],\n",
    "            \"stockCodes\": stock_codes,\n",
    "            \"title\": row['title'],\n",
    "            \"description\": row['description'],\n",
    "            \"existing_entities\": existing_entities\n",
    "        }\n",
    "        \n",
    "        response_text = invoke_chain_with_retry(chain_entity, prompt_text, api_manager)\n",
    "        time.sleep(0.5)  # Rate limiting\n",
    "        \n",
    "        if response_text is None:\n",
    "            print(f\"   ‚ùå B·ªè qua tin {article_idx} do l·ªói API\")\n",
    "            return article_results\n",
    "        \n",
    "        entities_dict = parse_entity_response(response_text)\n",
    "        \n",
    "        total_entities = len(entities_dict.get(\"POSITIVE\", [])) + len(entities_dict.get(\"NEGATIVE\", []))\n",
    "        if total_entities > 0:\n",
    "            print(f\"   ‚úì Tr√≠ch xu·∫•t ƒë∆∞·ª£c {total_entities} th·ª±c th·ªÉ ban ƒë·∫ßu\")\n",
    "            break\n",
    "            \n",
    "        entity_retry_count += 1\n",
    "        print(f\"   ‚ö† Kh√¥ng c√≥ th·ª±c th·ªÉ. Th·ª≠ l·∫°i {entity_retry_count}/{max_entity_retries}\")\n",
    "        time.sleep(BASE_DELAY / 2)\n",
    "    \n",
    "    if entity_retry_count == max_entity_retries and total_entities == 0:\n",
    "        print(f\"   ‚ùå Th·∫•t b·∫°i sau {max_entity_retries} l·∫ßn th·ª≠\")\n",
    "        return article_results\n",
    "    \n",
    "    # Process initial entities and build frontier\n",
    "    initial_frontier = []\n",
    "    \n",
    "    for impact in [\"POSITIVE\", \"NEGATIVE\"]:\n",
    "        for ent, content in entities_dict.get(impact, []):\n",
    "            if not ent or \"kh√¥ng c√≥ th·ª±c th·ªÉ n√†o\" in ent.lower():\n",
    "                continue\n",
    "            \n",
    "            # Thread-safe: Canonicalize entity\n",
    "            with canonical_lock:\n",
    "                canon_ent = merge_entity(ent, canonical_entities)\n",
    "            \n",
    "            node_type = \"stock\" if any(str(canon_ent).lower().find(stock.lower()) != -1 for stock in PORTFOLIO_STOCKS) else \"entity\"\n",
    "            \n",
    "            # Thread-safe: Add node and edge\n",
    "            with graph_lock:\n",
    "                if not G.has_node(canon_ent):\n",
    "                    G.add_node(canon_ent, type=node_type, timestamp=article_timestamp)\n",
    "                \n",
    "                if not G.has_edge(article_node, canon_ent):\n",
    "                    G.add_edge(article_node, canon_ent, impact=impact, timestamp=article_timestamp)\n",
    "            \n",
    "            # Save to results\n",
    "            article_results.append({\n",
    "                \"article_id\": article_idx,\n",
    "                \"article_title\": row['title'],\n",
    "                \"date\": row['date'],\n",
    "                \"entity\": canon_ent,\n",
    "                \"entity_type\": node_type,\n",
    "                \"impact\": impact,\n",
    "                \"explanation\": content,\n",
    "                \"iteration\": 0\n",
    "            })\n",
    "            \n",
    "            # Add to frontier if entity type\n",
    "            if node_type == \"entity\":\n",
    "                initial_frontier.append((canon_ent, impact, content))\n",
    "    \n",
    "    # ====================================\n",
    "    # Phase 2: Relation extraction\n",
    "    # ====================================\n",
    "    print(f\"   üîó Phase 2: Tr√≠ch xu·∫•t m·ªëi quan h·ªá (max {max_iterations} iterations)...\")\n",
    "    \n",
    "    current_frontier = initial_frontier\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        if not current_frontier:\n",
    "            print(f\"   ‚úì Iteration {iteration + 1}: Frontier r·ªóng, d·ª´ng s·ªõm\")\n",
    "            break\n",
    "        \n",
    "        print(f\"   üîÑ Iteration {iteration + 1}: X·ª≠ l√Ω {len(current_frontier)} entities...\")\n",
    "        \n",
    "        # Process in batches\n",
    "        next_frontier = []\n",
    "        \n",
    "        for i in range(0, len(current_frontier), batch_size):\n",
    "            batch = current_frontier[i:i + batch_size]\n",
    "            \n",
    "            # Process batch with thread-safe operations\n",
    "            new_entities = batch_process_entity_relationships_thread_safe(\n",
    "                batch, G, canonical_entities, PORTFOLIO_STOCKS, \n",
    "                portfolio_str_full, article_timestamp, chain_batch_relation, api_manager\n",
    "            )\n",
    "            \n",
    "            # Save new entities to results\n",
    "            for new_ent, new_impact, new_content in new_entities:\n",
    "                article_results.append({\n",
    "                    \"article_id\": article_idx,\n",
    "                    \"article_title\": row['title'],\n",
    "                    \"date\": row['date'],\n",
    "                    \"entity\": new_ent,\n",
    "                    \"entity_type\": \"entity\",\n",
    "                    \"impact\": new_impact,\n",
    "                    \"explanation\": new_content,\n",
    "                    \"iteration\": iteration + 1\n",
    "                })\n",
    "            \n",
    "            next_frontier.extend(new_entities)\n",
    "        \n",
    "        print(f\"   ‚úì Iteration {iteration + 1}: Ph√°t hi·ªán {len(next_frontier)} entities m·ªõi\")\n",
    "        current_frontier = next_frontier\n",
    "    \n",
    "    print(f\"   ‚úÖ Ho√†n th√†nh x·ª≠ l√Ω tin {article_idx}\")\n",
    "    return article_results\n",
    "\n",
    "\n",
    "def batch_process_entity_relationships_thread_safe(entity_batch, G, canonical_entities, portfolio, \n",
    "                                                     portfolio_str_full, article_timestamp, \n",
    "                                                     chain_batch_relation, api_manager):\n",
    "    \"\"\"\n",
    "    X·ª≠ l√Ω nhi·ªÅu entities trong m·ªôt l·∫ßn g·ªçi API - Thread-safe version\n",
    "    Returns: list of new entities to process\n",
    "    \"\"\"\n",
    "    if not entity_batch:\n",
    "        return []\n",
    "    \n",
    "    max_batch_retries = 2\n",
    "    batch_retry_count = 0\n",
    "    relationships = []\n",
    "    \n",
    "    while batch_retry_count < max_batch_retries:\n",
    "        # Format input entities\n",
    "        input_entities_text = \"\"\n",
    "        for entity, impact, content in entity_batch:\n",
    "            input_entities_text += f\"Th·ª±c th·ªÉ g·ªëc: {entity}\\n\\n·∫¢nh h∆∞·ªüng: {impact}, {content}\\n\\n---\\n\\n\"\n",
    "        \n",
    "        # Thread-safe: Get existing entities\n",
    "        with graph_lock:\n",
    "            existing_entities = graph_entities_to_str(G)\n",
    "        \n",
    "        # Create prompt\n",
    "        prompt_batch = {\n",
    "            \"input_entities\": input_entities_text,\n",
    "            \"portfolio\": portfolio_str_full,\n",
    "            \"existing_entities\": existing_entities\n",
    "        }\n",
    "        \n",
    "        # Get relationships\n",
    "        response = invoke_chain_with_retry(chain_batch_relation, prompt_batch, api_manager)\n",
    "        time.sleep(0.5)\n",
    "        \n",
    "        if response is None:\n",
    "            return []\n",
    "        \n",
    "        # Parse response\n",
    "        relationships = parse_batch_entity_response(response)\n",
    "        \n",
    "        if len(relationships) > 0:\n",
    "            break\n",
    "            \n",
    "        batch_retry_count += 1\n",
    "        print(f\"   ‚ö† Batch processing returned 0 relationships. Retry {batch_retry_count}/{max_batch_retries}\")\n",
    "        time.sleep(BASE_DELAY / 2)\n",
    "    \n",
    "    # Process relationships to update graph\n",
    "    next_entities = []\n",
    "    \n",
    "    for source, impact, target, content in relationships:\n",
    "        if \"kh√¥ng c√≥ th·ª±c th·ªÉ n√†o\" in source.lower() or \"kh√¥ng c√≥ th·ª±c th·ªÉ n√†o\" in target.lower():\n",
    "            continue\n",
    "        \n",
    "        # Thread-safe: Canonicalize entities\n",
    "        with canonical_lock:\n",
    "            canon_source = merge_entity(source, canonical_entities)\n",
    "            canon_target = merge_entity(target, canonical_entities)\n",
    "        \n",
    "        # Determine node type for target\n",
    "        node_type = \"stock\" if any(str(canon_target).lower().find(stock.lower()) != -1 for stock in portfolio) else \"entity\"\n",
    "        \n",
    "        # Thread-safe: Update graph\n",
    "        with graph_lock:\n",
    "            # Add target node if not exists\n",
    "            if not G.has_node(canon_target):\n",
    "                G.add_node(canon_target, type=node_type, timestamp=article_timestamp)\n",
    "            \n",
    "            # Ensure source node exists\n",
    "            if not G.has_node(canon_source):\n",
    "                source_type = \"stock\" if any(str(canon_source).lower().find(stock.lower()) != -1 for stock in portfolio) else \"entity\"\n",
    "                G.add_node(canon_source, type=source_type, timestamp=article_timestamp)\n",
    "            \n",
    "            # Add edge\n",
    "            add_edge(G, canon_source, canon_target, impact, article_timestamp)\n",
    "        \n",
    "        # Add to frontier if entity type\n",
    "        if node_type == \"entity\":\n",
    "            next_entities.append((canon_target, impact, content))\n",
    "    \n",
    "    return next_entities\n",
    "\n",
    "print(\"‚úì ƒê√£ ƒë·ªãnh nghƒ©a h√†m x·ª≠ l√Ω multithreading\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3ddc4a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì ƒê√£ ƒë·ªãnh nghƒ©a h√†m extract_entities_from_news_multithreaded()\n"
     ]
    }
   ],
   "source": [
    "def extract_entities_from_news_multithreaded(\n",
    "    csv_path=\"summarized_news_with_stocks_merged.csv\",\n",
    "    output_csv=\"entities_extracted.csv\",\n",
    "    output_graph_pkl=\"knowledge_graph.pkl\",\n",
    "    output_canonical_pkl=\"canonical_entities.pkl\",\n",
    "    output_tuples_csv=\"graph_tuples_step1.csv\",\n",
    "    start_date=None,\n",
    "    end_date=None,\n",
    "    max_articles=None,\n",
    "    max_iterations=2,\n",
    "    batch_size=5,\n",
    "    max_workers=MAX_WORKERS\n",
    "):\n",
    "    \"\"\"\n",
    "    Tr√≠ch xu·∫•t th·ª±c th·ªÉ t·ª´ tin t·ª©c v·ªõi MULTITHREADING ƒë·ªÉ t·∫≠n d·ª•ng t·ªëi ƒëa API quota\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    max_workers : int\n",
    "        S·ªë l∆∞·ª£ng threads x·ª≠ l√Ω ƒë·ªìng th·ªùi (default: 5)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple: (entities_df, G, canonical_entities)\n",
    "    \"\"\"\n",
    "    print(f\"üìñ ƒêang ƒë·ªçc d·ªØ li·ªáu t·ª´ {csv_path}...\")\n",
    "    \n",
    "    # ƒê·ªçc d·ªØ li·ªáu\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        print(f\"‚úì ƒê√£ ƒë·ªçc {len(df)} tin t·ª©c\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå Kh√¥ng t√¨m th·∫•y file: {csv_path}\")\n",
    "        raise\n",
    "    \n",
    "    # Validate columns\n",
    "    required_cols = ['postID', 'stockCodes', 'title', 'description', 'date']\n",
    "    missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        print(f\"‚ùå Dataset thi·∫øu c√°c c·ªôt: {missing_cols}\")\n",
    "        print(f\"üìã C√°c c·ªôt hi·ªán c√≥: {list(df.columns)}\")\n",
    "        raise ValueError(f\"Dataset kh√¥ng h·ª£p l·ªá: thi·∫øu c·ªôt {missing_cols}\")\n",
    "    \n",
    "    print(f\"‚úì Dataset h·ª£p l·ªá v·ªõi c√°c c·ªôt: {list(df.columns)}\")\n",
    "    \n",
    "    # Chuy·ªÉn ƒë·ªïi date\n",
    "    df['parsed_date'] = pd.to_datetime(df['date'])\n",
    "    df['only_date'] = df['parsed_date'].dt.date\n",
    "    \n",
    "    # Filter by date range\n",
    "    if start_date:\n",
    "        start_dt = pd.to_datetime(start_date).date()\n",
    "        df = df[df['only_date'] >= start_dt]\n",
    "        print(f\"‚úì L·ªçc t·ª´ ng√†y {start_date}: c√≤n {len(df)} tin\")\n",
    "    \n",
    "    if end_date:\n",
    "        end_dt = pd.to_datetime(end_date).date()\n",
    "        df = df[df['only_date'] <= end_dt]\n",
    "        print(f\"‚úì L·ªçc ƒë·∫øn ng√†y {end_date}: c√≤n {len(df)} tin\")\n",
    "    \n",
    "    # Limit articles\n",
    "    if max_articles:\n",
    "        df = df.head(max_articles)\n",
    "        print(f\"‚úì Gi·ªõi h·∫°n xu·ªëng {len(df)} tin ƒë·ªÉ x·ª≠ l√Ω\")\n",
    "    \n",
    "    # Sort by date\n",
    "    df = df.sort_values('date')\n",
    "    \n",
    "    # Initialize graph and canonical entities\n",
    "    G = nx.DiGraph()\n",
    "    canonical_entities = set()\n",
    "    \n",
    "    # Build portfolio string\n",
    "    portfolio_str_full = \", \".join([f\"{stock}-{sector}\" for stock, sector in zip(PORTFOLIO_STOCKS, PORTFOLIO_SECTOR)])\n",
    "    \n",
    "    # Create chains\n",
    "    chain_entity, chain_batch_relation = create_chains(api_manager)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üöÄ B·∫ÆT ƒê·∫¶U TR√çCH XU·∫§T V·ªöI MULTITHREADING ({max_workers} threads)\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Prepare articles for processing\n",
    "    articles_to_process = []\n",
    "    for idx, row in df.iterrows():\n",
    "        articles_to_process.append((idx + 1, row))\n",
    "    \n",
    "    # Process articles with multithreading\n",
    "    all_entities = []\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit all tasks\n",
    "        future_to_article = {\n",
    "            executor.submit(\n",
    "                process_article_thread_safe,\n",
    "                article_idx, row, G, canonical_entities, portfolio_str_full,\n",
    "                chain_entity, chain_batch_relation, api_manager,\n",
    "                max_iterations, batch_size\n",
    "            ): article_idx\n",
    "            for article_idx, row in articles_to_process\n",
    "        }\n",
    "        \n",
    "        # Collect results as they complete\n",
    "        completed = 0\n",
    "        total = len(future_to_article)\n",
    "        \n",
    "        for future in as_completed(future_to_article):\n",
    "            article_idx = future_to_article[future]\n",
    "            try:\n",
    "                article_results = future.result()\n",
    "                \n",
    "                # Thread-safe: Add to all_entities\n",
    "                with results_lock:\n",
    "                    all_entities.extend(article_results)\n",
    "                \n",
    "                completed += 1\n",
    "                print(f\"\\n‚úÖ Ho√†n th√†nh {completed}/{total} b√†i b√°o\")\n",
    "                \n",
    "            except Exception as exc:\n",
    "                print(f\"\\n‚ùå B√†i b√°o {article_idx} g·∫∑p l·ªói: {exc}\")\n",
    "    \n",
    "    # Create results DataFrame\n",
    "    entities_df = pd.DataFrame(all_entities)\n",
    "    \n",
    "    # Save entities CSV\n",
    "    entities_df.to_csv(output_csv, index=False, encoding='utf-8-sig')\n",
    "    print(f\"\\nüíæ ƒê√£ l∆∞u entities v√†o: {output_csv}\")\n",
    "    \n",
    "    # Save graph pickle\n",
    "    import pickle\n",
    "    with open(output_graph_pkl, 'wb') as f:\n",
    "        pickle.dump(G, f)\n",
    "    print(f\"üíæ ƒê√£ l∆∞u graph v√†o: {output_graph_pkl}\")\n",
    "    \n",
    "    # Save canonical entities pickle\n",
    "    with open(output_canonical_pkl, 'wb') as f:\n",
    "        pickle.dump(canonical_entities, f)\n",
    "    print(f\"üíæ ƒê√£ l∆∞u canonical entities v√†o: {output_canonical_pkl}\")\n",
    "    \n",
    "    # Export graph tuples CSV\n",
    "    print(f\"\\nüì¶ ƒêang export graph tuples...\")\n",
    "    tuples_data = []\n",
    "    for u, v, data in G.edges(data=True):\n",
    "        # Skip article nodes\n",
    "        if u.startswith(\"Article_\"):\n",
    "            continue\n",
    "        \n",
    "        timestamp = data.get('timestamp')\n",
    "        if timestamp:\n",
    "            try:\n",
    "                if isinstance(timestamp, pd.Timestamp):\n",
    "                    date_str = timestamp.date().isoformat()\n",
    "                else:\n",
    "                    date_str = pd.to_datetime(timestamp).date().isoformat()\n",
    "                \n",
    "                tuples_data.append({\n",
    "                    'date': date_str,\n",
    "                    'source': u,\n",
    "                    'impact': data.get('impact', ''),\n",
    "                    'target': v\n",
    "                })\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    if tuples_data:\n",
    "        tuples_df = pd.DataFrame(tuples_data)\n",
    "        tuples_df = tuples_df.sort_values('date')\n",
    "        tuples_df.to_csv(output_tuples_csv, index=False, encoding='utf-8-sig')\n",
    "        print(f\"‚úì ƒê√£ export {len(tuples_df)} tuples v√†o: {output_tuples_csv}\")\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"‚úÖ HO√ÄN TH√ÄNH V·ªöI MULTITHREADING!\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"üìä T·ªïng s·ªë entities: {len(entities_df)}\")\n",
    "    print(f\"üîπ Unique entities: {len(canonical_entities)}\")\n",
    "    print(f\"üìà Graph nodes: {len(G.nodes())}\")\n",
    "    print(f\"üîó Graph edges: {len(G.edges())}\")\n",
    "    print(f\"‚ö° Threads s·ª≠ d·ª•ng: {max_workers}\")\n",
    "    print(f\"üìÅ Files ƒë√£ t·∫°o:\")\n",
    "    print(f\"   - {output_csv}\")\n",
    "    print(f\"   - {output_graph_pkl}\")\n",
    "    print(f\"   - {output_canonical_pkl}\")\n",
    "    print(f\"   - {output_tuples_csv}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    return entities_df, G, canonical_entities\n",
    "\n",
    "print(\"‚úì ƒê√£ ƒë·ªãnh nghƒ©a h√†m extract_entities_from_news_multithreaded()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94a13ac",
   "metadata": {},
   "source": [
    "## üöÄ Ch·∫°y v·ªõi MULTITHREADING (T·∫≠n d·ª•ng t·ªëi ƒëa API quota)\n",
    "\n",
    "### ‚ö° TƒÉng t·ªëc ƒë·ªô x·ª≠ l√Ω:\n",
    "\n",
    "**So s√°nh:**\n",
    "- **Single-threaded:** 1 b√†i b√°o m·∫•t ~30-60s ‚Üí 100 b√†i = 50-100 ph√∫t\n",
    "- **5 threads:** 5 b√†i ƒë·ªìng th·ªùi ‚Üí 100 b√†i = ~15-25 ph√∫t\n",
    "- **10 threads:** 10 b√†i ƒë·ªìng th·ªùi ‚Üí 100 b√†i = ~10-15 ph√∫t\n",
    "\n",
    "### ‚öôÔ∏è C·∫•u h√¨nh:\n",
    "\n",
    "- `max_workers=5` - S·ªë threads (khuy·∫øn ngh·ªã 5-10)\n",
    "- TƒÉng l√™n n·∫øu API quota cao\n",
    "- Gi·∫£m xu·ªëng n·∫øu g·∫∑p rate limit errors\n",
    "\n",
    "### üîí Thread Safety:\n",
    "\n",
    "- ‚úÖ Graph operations v·ªõi `graph_lock`\n",
    "- ‚úÖ Canonical entities v·ªõi `canonical_lock`\n",
    "- ‚úÖ Results collection v·ªõi `results_lock`\n",
    "\n",
    "### üìä ∆Øu ƒëi·ªÉm:\n",
    "\n",
    "1. **T·∫≠n d·ª•ng t·ªëi ƒëa API quota** - G·ª≠i nhi·ªÅu requests ƒë·ªìng th·ªùi\n",
    "2. **Gi·∫£m th·ªùi gian ch·ªù** - Kh√¥ng ph·∫£i ƒë·ª£i t·ª´ng b√†i xong m·ªõi x·ª≠ l√Ω ti·∫øp\n",
    "3. **Thread-safe** - Kh√¥ng b·ªã conflict khi nhi·ªÅu threads c√πng update graph\n",
    "4. **Auto-retry** - M·ªói thread c√≥ retry logic ri√™ng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874853a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìñ ƒêang ƒë·ªçc d·ªØ li·ªáu t·ª´ summarized_news_with_stocks_merged.csv...\n",
      "‚úì ƒê√£ ƒë·ªçc 24571 tin t·ª©c\n",
      "‚úì Dataset h·ª£p l·ªá v·ªõi c√°c c·ªôt: ['postID', 'stockCodes', 'title', 'description', 'date']\n",
      "‚úì L·ªçc t·ª´ ng√†y 2025-01-01: c√≤n 24571 tin\n",
      "‚úì L·ªçc ƒë·∫øn ng√†y 2025-06-01: c√≤n 12382 tin\n",
      "‚úì Gi·ªõi h·∫°n xu·ªëng 12382 tin ƒë·ªÉ x·ª≠ l√Ω\n",
      "\n",
      "============================================================\n",
      "üöÄ B·∫ÆT ƒê·∫¶U TR√çCH XU·∫§T V·ªöI MULTITHREADING (5 threads)\n",
      "============================================================\n",
      "\n",
      "‚úì ƒê√£ ƒë·ªçc 24571 tin t·ª©c\n",
      "‚úì Dataset h·ª£p l·ªá v·ªõi c√°c c·ªôt: ['postID', 'stockCodes', 'title', 'description', 'date']\n",
      "‚úì L·ªçc t·ª´ ng√†y 2025-01-01: c√≤n 24571 tin\n",
      "‚úì L·ªçc ƒë·∫øn ng√†y 2025-06-01: c√≤n 12382 tin\n",
      "‚úì Gi·ªõi h·∫°n xu·ªëng 12382 tin ƒë·ªÉ x·ª≠ l√Ω\n",
      "\n",
      "============================================================\n",
      "üöÄ B·∫ÆT ƒê·∫¶U TR√çCH XU·∫§T V·ªöI MULTITHREADING (5 threads)\n",
      "============================================================\n",
      "\n",
      "\n",
      "[1] üì∞ S·ªë l∆∞·ª£ng h√≥a ƒë∆°n kh·ªüi t·∫°o t·ª´ m√°y t√≠nh ti·ªÅn tƒÉng g·∫•p 13 l·∫ßn n...\n",
      "   üìå Phase 1: Tr√≠ch xu·∫•t entities ban ƒë·∫ßu...\n",
      "\n",
      "[56] üì∞ B·ªï sung h∆°n 6.434 t·ª∑ ƒë·ªìng t·ª´ ng√¢n s√°ch trung ∆∞∆°ng nƒÉm 2024 c...\n",
      "   üìå Phase 1: Tr√≠ch xu·∫•t entities ban ƒë·∫ßu...\n",
      "\n",
      "[55] üì∞ M√°y bay ch·ªü kh√°ch C919 c·ªßa Trung Qu·ªëc th·ª±c hi·ªán chuy·∫øn bay t...\n",
      "   üìå Phase 1: Tr√≠ch xu·∫•t entities ban ƒë·∫ßu...\n",
      "\n",
      "[54] üì∞ Ch·ª©ng kho√°n Trung Qu·ªëc d∆∞ÃÅt chu√¥ÃÉi ba nƒÉm suy giaÃâm li√™n ti√™...\n",
      "   üìå Phase 1: Tr√≠ch xu·∫•t entities ban ƒë·∫ßu...\n",
      "\n",
      "[53] üì∞ Gi√° kh√≠ ƒë·ªët ·ªü ch√¢u √Çu tƒÉng l√™n m·ª©c cao nh·∫•t trong m·ªôt nƒÉm do...\n",
      "   üìå Phase 1: Tr√≠ch xu·∫•t entities ban ƒë·∫ßu...\n",
      "\n",
      "[1] üì∞ S·ªë l∆∞·ª£ng h√≥a ƒë∆°n kh·ªüi t·∫°o t·ª´ m√°y t√≠nh ti·ªÅn tƒÉng g·∫•p 13 l·∫ßn n...\n",
      "   üìå Phase 1: Tr√≠ch xu·∫•t entities ban ƒë·∫ßu...\n",
      "\n",
      "[56] üì∞ B·ªï sung h∆°n 6.434 t·ª∑ ƒë·ªìng t·ª´ ng√¢n s√°ch trung ∆∞∆°ng nƒÉm 2024 c...\n",
      "   üìå Phase 1: Tr√≠ch xu·∫•t entities ban ƒë·∫ßu...\n",
      "\n",
      "[55] üì∞ M√°y bay ch·ªü kh√°ch C919 c·ªßa Trung Qu·ªëc th·ª±c hi·ªán chuy·∫øn bay t...\n",
      "   üìå Phase 1: Tr√≠ch xu·∫•t entities ban ƒë·∫ßu...\n",
      "\n",
      "[54] üì∞ Ch·ª©ng kho√°n Trung Qu·ªëc d∆∞ÃÅt chu√¥ÃÉi ba nƒÉm suy giaÃâm li√™n ti√™...\n",
      "   üìå Phase 1: Tr√≠ch xu·∫•t entities ban ƒë·∫ßu...\n",
      "\n",
      "[53] üì∞ Gi√° kh√≠ ƒë·ªët ·ªü ch√¢u √Çu tƒÉng l√™n m·ª©c cao nh·∫•t trong m·ªôt nƒÉm do...\n",
      "   üìå Phase 1: Tr√≠ch xu·∫•t entities ban ƒë·∫ßu...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised PermissionDenied: 403 The caller does not have permission.\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised PermissionDenied: 403 Permission denied: Consumer 'api_key:AIzaSyB0LPWR1ENupLFY400BMFdOUXVWke5AyOM' has been suspended. [reason: \"CONSUMER_SUSPENDED\"\n",
      "domain: \"googleapis.com\"\n",
      "metadata {\n",
      "  key: \"service\"\n",
      "  value: \"generativelanguage.googleapis.com\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"containerInfo\"\n",
      "  value: \"api_key:AIzaSyB0LPWR1ENupLFY400BMFdOUXVWke5AyOM\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"consumer\"\n",
      "  value: \"projects/137916974205\"\n",
      "}\n",
      ", locale: \"en-US\"\n",
      "message: \"Permission denied: Consumer \\'api_key:AIzaSyB0LPWR1ENupLFY400BMFdOUXVWke5AyOM\\' has been suspended.\"\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised PermissionDenied: 403 Permission denied: Consumer 'api_key:AIzaSyB0LPWR1ENupLFY400BMFdOUXVWke5AyOM' has been suspended. [reason: \"CONSUMER_SUSPENDED\"\n",
      "domain: \"googleapis.com\"\n",
      "metadata {\n",
      "  key: \"service\"\n",
      "  value: \"generativelanguage.googleapis.com\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"containerInfo\"\n",
      "  value: \"api_key:AIzaSyB0LPWR1ENupLFY400BMFdOUXVWke5AyOM\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"consumer\"\n",
      "  value: \"projects/137916974205\"\n",
      "}\n",
      ", locale: \"en-US\"\n",
      "message: \"Permission denied: Consumer \\'api_key:AIzaSyB0LPWR1ENupLFY400BMFdOUXVWke5AyOM\\' has been suspended.\"\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised PermissionDenied: 403 Permission denied: Consumer 'api_key:AIzaSyB0LPWR1ENupLFY400BMFdOUXVWke5AyOM' has been suspended. [reason: \"CONSUMER_SUSPENDED\"\n",
      "domain: \"googleapis.com\"\n",
      "metadata {\n",
      "  key: \"service\"\n",
      "  value: \"generativelanguage.googleapis.com\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"containerInfo\"\n",
      "  value: \"api_key:AIzaSyB0LPWR1ENupLFY400BMFdOUXVWke5AyOM\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"consumer\"\n",
      "  value: \"projects/137916974205\"\n",
      "}\n",
      ", locale: \"en-US\"\n",
      "message: \"Permission denied: Consumer \\'api_key:AIzaSyB0LPWR1ENupLFY400BMFdOUXVWke5AyOM\\' has been suspended.\"\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised PermissionDenied: 403 Permission denied: Consumer 'api_key:AIzaSyB0LPWR1ENupLFY400BMFdOUXVWke5AyOM' has been suspended. [reason: \"CONSUMER_SUSPENDED\"\n",
      "domain: \"googleapis.com\"\n",
      "metadata {\n",
      "  key: \"service\"\n",
      "  value: \"generativelanguage.googleapis.com\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"containerInfo\"\n",
      "  value: \"api_key:AIzaSyB0LPWR1ENupLFY400BMFdOUXVWke5AyOM\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"consumer\"\n",
      "  value: \"projects/137916974205\"\n",
      "}\n",
      ", locale: \"en-US\"\n",
      "message: \"Permission denied: Consumer \\'api_key:AIzaSyB0LPWR1ENupLFY400BMFdOUXVWke5AyOM\\' has been suspended.\"\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised PermissionDenied: 403 Permission denied: Consumer 'api_key:AIzaSyB0LPWR1ENupLFY400BMFdOUXVWke5AyOM' has been suspended. [reason: \"CONSUMER_SUSPENDED\"\n",
      "domain: \"googleapis.com\"\n",
      "metadata {\n",
      "  key: \"service\"\n",
      "  value: \"generativelanguage.googleapis.com\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"containerInfo\"\n",
      "  value: \"api_key:AIzaSyB0LPWR1ENupLFY400BMFdOUXVWke5AyOM\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"consumer\"\n",
      "  value: \"projects/137916974205\"\n",
      "}\n",
      ", locale: \"en-US\"\n",
      "message: \"Permission denied: Consumer \\'api_key:AIzaSyB0LPWR1ENupLFY400BMFdOUXVWke5AyOM\\' has been suspended.\"\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised PermissionDenied: 403 Permission denied: Consumer 'api_key:AIzaSyB0LPWR1ENupLFY400BMFdOUXVWke5AyOM' has been suspended. [reason: \"CONSUMER_SUSPENDED\"\n",
      "domain: \"googleapis.com\"\n",
      "metadata {\n",
      "  key: \"service\"\n",
      "  value: \"generativelanguage.googleapis.com\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"containerInfo\"\n",
      "  value: \"api_key:AIzaSyB0LPWR1ENupLFY400BMFdOUXVWke5AyOM\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"consumer\"\n",
      "  value: \"projects/137916974205\"\n",
      "}\n",
      ", locale: \"en-US\"\n",
      "message: \"Permission denied: Consumer \\'api_key:AIzaSyB0LPWR1ENupLFY400BMFdOUXVWke5AyOM\\' has been suspended.\"\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised PermissionDenied: 403 The caller does not have permission.\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised PermissionDenied: 403 The caller does not have permission.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö† L·ªói l·∫ßn 1 v·ªõi GOOGLE_API_KEY\n",
      "‚è≥ ƒê·ª£i 30s tr∆∞·ªõc khi th·ª≠ key m·ªõi...\n",
      "‚ö† L·ªói l·∫ßn 2 v·ªõi GOOGLE_API_KEY\n",
      "‚õî GOOGLE_API_KEY ƒë√£ l·ªói 2/2 l·∫ßn\n",
      "üîÑ Chuy·ªÉn sang key ti·∫øp theo...\n",
      "üîë ƒêang s·ª≠ d·ª•ng: GOOGLE_API_KEY_2 (Key 2/4)\n",
      "‚è≥ ƒê·ª£i 30s tr∆∞·ªõc khi th·ª≠ key m·ªõi...\n",
      "‚ö† L·ªói l·∫ßn 1 v·ªõi GOOGLE_API_KEY_2\n",
      "‚è≥ ƒê·ª£i 30s tr∆∞·ªõc khi th·ª≠ key m·ªõi...\n",
      "‚ö† L·ªói l·∫ßn 2 v·ªõi GOOGLE_API_KEY_2\n",
      "‚õî GOOGLE_API_KEY_2 ƒë√£ l·ªói 2/2 l·∫ßn\n",
      "üîÑ Chuy·ªÉn sang key ti·∫øp theo...\n",
      "üîë ƒêang s·ª≠ d·ª•ng: GOOGLE_API_KEY_3 (Key 3/4)\n",
      "‚è≥ ƒê·ª£i 30s tr∆∞·ªõc khi th·ª≠ key m·ªõi...\n",
      "‚ö† L·ªói l·∫ßn 1 v·ªõi GOOGLE_API_KEY_3\n",
      "‚è≥ ƒê·ª£i 30s tr∆∞·ªõc khi th·ª≠ key m·ªõi...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised PermissionDenied: 403 Permission denied: Consumer 'api_key:AIzaSyB0LPWR1ENupLFY400BMFdOUXVWke5AyOM' has been suspended. [reason: \"CONSUMER_SUSPENDED\"\n",
      "domain: \"googleapis.com\"\n",
      "metadata {\n",
      "  key: \"service\"\n",
      "  value: \"generativelanguage.googleapis.com\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"containerInfo\"\n",
      "  value: \"api_key:AIzaSyB0LPWR1ENupLFY400BMFdOUXVWke5AyOM\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"consumer\"\n",
      "  value: \"projects/137916974205\"\n",
      "}\n",
      ", locale: \"en-US\"\n",
      "message: \"Permission denied: Consumer \\'api_key:AIzaSyB0LPWR1ENupLFY400BMFdOUXVWke5AyOM\\' has been suspended.\"\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised PermissionDenied: 403 Permission denied: Consumer 'api_key:AIzaSyB0LPWR1ENupLFY400BMFdOUXVWke5AyOM' has been suspended. [reason: \"CONSUMER_SUSPENDED\"\n",
      "domain: \"googleapis.com\"\n",
      "metadata {\n",
      "  key: \"service\"\n",
      "  value: \"generativelanguage.googleapis.com\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"containerInfo\"\n",
      "  value: \"api_key:AIzaSyB0LPWR1ENupLFY400BMFdOUXVWke5AyOM\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"consumer\"\n",
      "  value: \"projects/137916974205\"\n",
      "}\n",
      ", locale: \"en-US\"\n",
      "message: \"Permission denied: Consumer \\'api_key:AIzaSyB0LPWR1ENupLFY400BMFdOUXVWke5AyOM\\' has been suspended.\"\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised PermissionDenied: 403 Permission denied: Consumer 'api_key:AIzaSyB0LPWR1ENupLFY400BMFdOUXVWke5AyOM' has been suspended. [reason: \"CONSUMER_SUSPENDED\"\n",
      "domain: \"googleapis.com\"\n",
      "metadata {\n",
      "  key: \"service\"\n",
      "  value: \"generativelanguage.googleapis.com\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"containerInfo\"\n",
      "  value: \"api_key:AIzaSyB0LPWR1ENupLFY400BMFdOUXVWke5AyOM\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"consumer\"\n",
      "  value: \"projects/137916974205\"\n",
      "}\n",
      ", locale: \"en-US\"\n",
      "message: \"Permission denied: Consumer \\'api_key:AIzaSyB0LPWR1ENupLFY400BMFdOUXVWke5AyOM\\' has been suspended.\"\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised PermissionDenied: 403 Permission denied: Consumer 'api_key:AIzaSyB0LPWR1ENupLFY400BMFdOUXVWke5AyOM' has been suspended. [reason: \"CONSUMER_SUSPENDED\"\n",
      "domain: \"googleapis.com\"\n",
      "metadata {\n",
      "  key: \"service\"\n",
      "  value: \"generativelanguage.googleapis.com\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"containerInfo\"\n",
      "  value: \"api_key:AIzaSyB0LPWR1ENupLFY400BMFdOUXVWke5AyOM\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"consumer\"\n",
      "  value: \"projects/137916974205\"\n",
      "}\n",
      ", locale: \"en-US\"\n",
      "message: \"Permission denied: Consumer \\'api_key:AIzaSyB0LPWR1ENupLFY400BMFdOUXVWke5AyOM\\' has been suspended.\"\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised PermissionDenied: 403 Permission denied: Consumer 'api_key:AIzaSyB0LPWR1ENupLFY400BMFdOUXVWke5AyOM' has been suspended. [reason: \"CONSUMER_SUSPENDED\"\n",
      "domain: \"googleapis.com\"\n",
      "metadata {\n",
      "  key: \"service\"\n",
      "  value: \"generativelanguage.googleapis.com\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"containerInfo\"\n",
      "  value: \"api_key:AIzaSyB0LPWR1ENupLFY400BMFdOUXVWke5AyOM\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"consumer\"\n",
      "  value: \"projects/137916974205\"\n",
      "}\n",
      ", locale: \"en-US\"\n",
      "message: \"Permission denied: Consumer \\'api_key:AIzaSyB0LPWR1ENupLFY400BMFdOUXVWke5AyOM\\' has been suspended.\"\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised PermissionDenied: 403 Permission denied: Consumer 'api_key:AIzaSyB0LPWR1ENupLFY400BMFdOUXVWke5AyOM' has been suspended. [reason: \"CONSUMER_SUSPENDED\"\n",
      "domain: \"googleapis.com\"\n",
      "metadata {\n",
      "  key: \"service\"\n",
      "  value: \"generativelanguage.googleapis.com\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"containerInfo\"\n",
      "  value: \"api_key:AIzaSyB0LPWR1ENupLFY400BMFdOUXVWke5AyOM\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"consumer\"\n",
      "  value: \"projects/137916974205\"\n",
      "}\n",
      ", locale: \"en-US\"\n",
      "message: \"Permission denied: Consumer \\'api_key:AIzaSyB0LPWR1ENupLFY400BMFdOUXVWke5AyOM\\' has been suspended.\"\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised PermissionDenied: 403 Permission denied: Consumer 'api_key:AIzaSyB0LPWR1ENupLFY400BMFdOUXVWke5AyOM' has been suspended. [reason: \"CONSUMER_SUSPENDED\"\n",
      "domain: \"googleapis.com\"\n",
      "metadata {\n",
      "  key: \"service\"\n",
      "  value: \"generativelanguage.googleapis.com\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"containerInfo\"\n",
      "  value: \"api_key:AIzaSyB0LPWR1ENupLFY400BMFdOUXVWke5AyOM\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"consumer\"\n",
      "  value: \"projects/137916974205\"\n",
      "}\n",
      ", locale: \"en-US\"\n",
      "message: \"Permission denied: Consumer \\'api_key:AIzaSyB0LPWR1ENupLFY400BMFdOUXVWke5AyOM\\' has been suspended.\"\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised PermissionDenied: 403 The caller does not have permission.\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised PermissionDenied: 403 The caller does not have permission.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö† L·ªói l·∫ßn 2 v·ªõi GOOGLE_API_KEY_3\n",
      "‚õî GOOGLE_API_KEY_3 ƒë√£ l·ªói 2/2 l·∫ßn\n",
      "üîÑ Chuy·ªÉn sang key ti·∫øp theo...\n",
      "üîë ƒêang s·ª≠ d·ª•ng: GOOGLE_API_KEY_4 (Key 4/4)\n",
      "‚è≥ ƒê·ª£i 30s tr∆∞·ªõc khi th·ª≠ key m·ªõi...\n",
      "‚ö† L·ªói l·∫ßn 1 v·ªõi GOOGLE_API_KEY_4\n",
      "‚è≥ ƒê·ª£i 30s tr∆∞·ªõc khi th·ª≠ key m·ªõi...\n",
      "‚ö† L·ªói l·∫ßn 2 v·ªõi GOOGLE_API_KEY_4\n",
      "‚õî GOOGLE_API_KEY_4 ƒë√£ l·ªói 2/2 l·∫ßn\n",
      "‚ùå ƒê√£ h·∫øt t·∫•t c·∫£ 4 keys!\n",
      "‚è≥ ƒê·ª£i 45s tr∆∞·ªõc khi retry (2/2)...\n",
      "‚ö† L·ªói l·∫ßn 3 v·ªõi GOOGLE_API_KEY_4\n",
      "‚õî GOOGLE_API_KEY_4 ƒë√£ l·ªói 3/2 l·∫ßn\n",
      "‚ùå ƒê√£ h·∫øt t·∫•t c·∫£ 4 keys!\n",
      "‚è≥ ƒê·ª£i 68s tr∆∞·ªõc khi retry (3/2)...\n",
      "‚ö† L·ªói l·∫ßn 4 v·ªõi GOOGLE_API_KEY_4\n",
      "‚õî GOOGLE_API_KEY_4 ƒë√£ l·ªói 4/2 l·∫ßn\n",
      "‚ùå ƒê√£ h·∫øt t·∫•t c·∫£ 4 keys!\n",
      "‚è≥ ƒê·ª£i 101s tr∆∞·ªõc khi retry (4/2)...\n",
      "‚ö† L·ªói l·∫ßn 2 v·ªõi GOOGLE_API_KEY_4\n",
      "‚õî GOOGLE_API_KEY_4 ƒë√£ l·ªói 2/2 l·∫ßn\n",
      "‚ùå ƒê√£ h·∫øt t·∫•t c·∫£ 4 keys!\n",
      "‚è≥ ƒê·ª£i 45s tr∆∞·ªõc khi retry (2/2)...\n",
      "‚ö† L·ªói l·∫ßn 3 v·ªõi GOOGLE_API_KEY_4\n",
      "‚õî GOOGLE_API_KEY_4 ƒë√£ l·ªói 3/2 l·∫ßn\n",
      "‚ùå ƒê√£ h·∫øt t·∫•t c·∫£ 4 keys!\n",
      "‚è≥ ƒê·ª£i 68s tr∆∞·ªõc khi retry (3/2)...\n",
      "‚ö† L·ªói l·∫ßn 4 v·ªõi GOOGLE_API_KEY_4\n",
      "‚õî GOOGLE_API_KEY_4 ƒë√£ l·ªói 4/2 l·∫ßn\n",
      "‚ùå ƒê√£ h·∫øt t·∫•t c·∫£ 4 keys!\n",
      "‚è≥ ƒê·ª£i 101s tr∆∞·ªõc khi retry (4/2)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised PermissionDenied: 403 Permission denied: Consumer 'api_key:AIzaSyB0LPWR1ENupLFY400BMFdOUXVWke5AyOM' has been suspended. [reason: \"CONSUMER_SUSPENDED\"\n",
      "domain: \"googleapis.com\"\n",
      "metadata {\n",
      "  key: \"service\"\n",
      "  value: \"generativelanguage.googleapis.com\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"containerInfo\"\n",
      "  value: \"api_key:AIzaSyB0LPWR1ENupLFY400BMFdOUXVWke5AyOM\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"consumer\"\n",
      "  value: \"projects/137916974205\"\n",
      "}\n",
      ", locale: \"en-US\"\n",
      "message: \"Permission denied: Consumer \\'api_key:AIzaSyB0LPWR1ENupLFY400BMFdOUXVWke5AyOM\\' has been suspended.\"\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised PermissionDenied: 403 Permission denied: Consumer 'api_key:AIzaSyB0LPWR1ENupLFY400BMFdOUXVWke5AyOM' has been suspended. [reason: \"CONSUMER_SUSPENDED\"\n",
      "domain: \"googleapis.com\"\n",
      "metadata {\n",
      "  key: \"service\"\n",
      "  value: \"generativelanguage.googleapis.com\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"containerInfo\"\n",
      "  value: \"api_key:AIzaSyB0LPWR1ENupLFY400BMFdOUXVWke5AyOM\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"consumer\"\n",
      "  value: \"projects/137916974205\"\n",
      "}\n",
      ", locale: \"en-US\"\n",
      "message: \"Permission denied: Consumer \\'api_key:AIzaSyB0LPWR1ENupLFY400BMFdOUXVWke5AyOM\\' has been suspended.\"\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised PermissionDenied: 403 Permission denied: Consumer 'api_key:AIzaSyB0LPWR1ENupLFY400BMFdOUXVWke5AyOM' has been suspended. [reason: \"CONSUMER_SUSPENDED\"\n",
      "domain: \"googleapis.com\"\n",
      "metadata {\n",
      "  key: \"service\"\n",
      "  value: \"generativelanguage.googleapis.com\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"containerInfo\"\n",
      "  value: \"api_key:AIzaSyB0LPWR1ENupLFY400BMFdOUXVWke5AyOM\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"consumer\"\n",
      "  value: \"projects/137916974205\"\n",
      "}\n",
      ", locale: \"en-US\"\n",
      "message: \"Permission denied: Consumer \\'api_key:AIzaSyB0LPWR1ENupLFY400BMFdOUXVWke5AyOM\\' has been suspended.\"\n",
      "].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö† L·ªói l·∫ßn 5 v·ªõi GOOGLE_API_KEY_4\n",
      "‚õî GOOGLE_API_KEY_4 ƒë√£ l·ªói 5/2 l·∫ßn\n",
      "‚ùå ƒê√£ h·∫øt t·∫•t c·∫£ 4 keys!\n",
      "‚è≥ ƒê·ª£i 152s tr∆∞·ªõc khi retry (5/2)...\n",
      "‚ö† L·ªói l·∫ßn 6 v·ªõi GOOGLE_API_KEY_4\n",
      "‚õî GOOGLE_API_KEY_4 ƒë√£ l·ªói 6/2 l·∫ßn\n",
      "‚ùå ƒê√£ h·∫øt t·∫•t c·∫£ 4 keys!\n",
      "‚è≥ ƒê·ª£i 228s tr∆∞·ªõc khi retry (6/2)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised PermissionDenied: 403 Permission denied: Consumer 'api_key:AIzaSyB0LPWR1ENupLFY400BMFdOUXVWke5AyOM' has been suspended. [reason: \"CONSUMER_SUSPENDED\"\n",
      "domain: \"googleapis.com\"\n",
      "metadata {\n",
      "  key: \"service\"\n",
      "  value: \"generativelanguage.googleapis.com\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"containerInfo\"\n",
      "  value: \"api_key:AIzaSyB0LPWR1ENupLFY400BMFdOUXVWke5AyOM\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"consumer\"\n",
      "  value: \"projects/137916974205\"\n",
      "}\n",
      ", locale: \"en-US\"\n",
      "message: \"Permission denied: Consumer \\'api_key:AIzaSyB0LPWR1ENupLFY400BMFdOUXVWke5AyOM\\' has been suspended.\"\n",
      "].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö† L·ªói l·∫ßn 7 v·ªõi GOOGLE_API_KEY_4\n",
      "‚õî GOOGLE_API_KEY_4 ƒë√£ l·ªói 7/2 l·∫ßn\n",
      "‚ùå ƒê√£ h·∫øt t·∫•t c·∫£ 4 keys!\n",
      "‚è≥ ƒê·ª£i 342s tr∆∞·ªõc khi retry (7/2)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised PermissionDenied: 403 Permission denied: Consumer 'api_key:AIzaSyB0LPWR1ENupLFY400BMFdOUXVWke5AyOM' has been suspended. [reason: \"CONSUMER_SUSPENDED\"\n",
      "domain: \"googleapis.com\"\n",
      "metadata {\n",
      "  key: \"service\"\n",
      "  value: \"generativelanguage.googleapis.com\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"containerInfo\"\n",
      "  value: \"api_key:AIzaSyB0LPWR1ENupLFY400BMFdOUXVWke5AyOM\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"consumer\"\n",
      "  value: \"projects/137916974205\"\n",
      "}\n",
      ", locale: \"en-US\"\n",
      "message: \"Permission denied: Consumer \\'api_key:AIzaSyB0LPWR1ENupLFY400BMFdOUXVWke5AyOM\\' has been suspended.\"\n",
      "].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö† L·ªói l·∫ßn 8 v·ªõi GOOGLE_API_KEY_4\n",
      "‚õî GOOGLE_API_KEY_4 ƒë√£ l·ªói 8/2 l·∫ßn\n",
      "‚ùå ƒê√£ h·∫øt t·∫•t c·∫£ 4 keys!\n",
      "‚è≥ ƒê·ª£i 513s tr∆∞·ªõc khi retry (8/2)...\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CH·∫†Y V·ªöI MULTITHREADING - T·∫¨N D·ª§NG T·ªêI ƒêA API QUOTA\n",
    "# ============================================================\n",
    "\n",
    "# Ch·∫°y v·ªõi 5 threads ƒë·ªìng th·ªùi\n",
    "entities_df_mt, G_mt, canonical_entities_mt = extract_entities_from_news_multithreaded(\n",
    "    csv_path=\"summarized_news_with_stocks_merged.csv\",\n",
    "    output_csv=\"entities_extracted_mt.csv\",\n",
    "    output_graph_pkl=\"knowledge_graph_mt_100000.pkl\",\n",
    "    output_canonical_pkl=\"canonical_entities_mt.pkl\",\n",
    "    output_tuples_csv=\"graph_tuples_step1_mt.csv\",\n",
    "    start_date=\"2025-01-01\",\n",
    "    end_date=\"2025-06-01\",\n",
    "    max_articles=100000,\n",
    "    max_iterations=2,\n",
    "    batch_size=500,\n",
    "    max_workers=5  # üöÄ 5 threads ƒë·ªìng th·ªùi\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eae71c3",
   "metadata": {},
   "source": [
    "## üìä So s√°nh Performance: Single-thread vs Multi-thread\n",
    "\n",
    "### üí° Tips t·ªëi ∆∞u:\n",
    "\n",
    "1. **TƒÉng max_workers khi:**\n",
    "   - C√≥ nhi·ªÅu API keys\n",
    "   - API quota cao\n",
    "   - X·ª≠ l√Ω dataset l·ªõn (1000+ b√†i)\n",
    "\n",
    "2. **Gi·∫£m max_workers khi:**\n",
    "   - G·∫∑p rate limit errors (429)\n",
    "   - API quota th·∫•p\n",
    "   - Mu·ªën ·ªïn ƒë·ªãnh h∆°n\n",
    "\n",
    "3. **Khuy·∫øn ngh·ªã:**\n",
    "   - `max_workers=5`: An to√†n, t·ªëc ƒë·ªô t·ªët\n",
    "   - `max_workers=10`: Nhanh h∆°n, c·∫ßn API quota cao\n",
    "   - `max_workers=3`: ·ªîn ƒë·ªãnh nh·∫•t, cho API free tier\n",
    "\n",
    "### ‚öôÔ∏è ƒêi·ªÅu ch·ªânh trong code:\n",
    "\n",
    "```python\n",
    "# Thay ƒë·ªïi s·ªë threads\n",
    "max_workers=10  # TƒÉng l√™n 10 threads\n",
    "\n",
    "# Gi·∫£m delay gi·ªØa c√°c API calls\n",
    "time.sleep(0.5)  # T·ª´ 1s xu·ªëng 0.5s\n",
    "\n",
    "# TƒÉng batch size\n",
    "batch_size=10  # X·ª≠ l√Ω nhi·ªÅu entities h∆°n trong 1 call\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a42dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# HELPER: So s√°nh k·∫øt qu·∫£ Single-thread vs Multi-thread\n",
    "# ============================================================\n",
    "\n",
    "def compare_results(single_thread_csv, multi_thread_csv):\n",
    "    \"\"\"\n",
    "    So s√°nh k·∫øt qu·∫£ gi·ªØa single-thread v√† multi-thread\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df_st = pd.read_csv(single_thread_csv)\n",
    "        df_mt = pd.read_csv(multi_thread_csv)\n",
    "        \n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"üìä SO S√ÅNH K·∫æT QU·∫¢\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "        \n",
    "        print(f\"Single-thread:\")\n",
    "        print(f\"  - Entities: {len(df_st)}\")\n",
    "        print(f\"  - Unique entities: {df_st['entity'].nunique()}\")\n",
    "        print(f\"  - Articles: {df_st['article_id'].nunique()}\")\n",
    "        \n",
    "        print(f\"\\nMulti-thread:\")\n",
    "        print(f\"  - Entities: {len(df_mt)}\")\n",
    "        print(f\"  - Unique entities: {df_mt['entity'].nunique()}\")\n",
    "        print(f\"  - Articles: {df_mt['article_id'].nunique()}\")\n",
    "        \n",
    "        print(f\"\\nüí° ƒê·ªô ch√≠nh x√°c:\")\n",
    "        common_entities = set(df_st['entity'].unique()) & set(df_mt['entity'].unique())\n",
    "        print(f\"  - Entities chung: {len(common_entities)}\")\n",
    "        print(f\"  - T·ª∑ l·ªá tr√πng kh·ªõp: {len(common_entities)/max(len(df_st['entity'].unique()), len(df_mt['entity'].unique()))*100:.1f}%\")\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"‚ùå Ch∆∞a c√≥ file ƒë·ªÉ so s√°nh: {e}\")\n",
    "\n",
    "# Uncomment ƒë·ªÉ so s√°nh n·∫øu ƒë√£ ch·∫°y c·∫£ 2 versions\n",
    "# compare_results(\"entities_extracted.csv\", \"entities_extracted_mt.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec082aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì ƒê√£ ƒë·ªãnh nghƒ©a h√†m extract_entities_from_news() v·ªõi workflow ƒë·∫ßy ƒë·ªß nh∆∞ TRR\n"
     ]
    }
   ],
   "source": [
    "def batch_process_entity_relationships(entity_batch, G, canonical_entities, portfolio, portfolio_str_full, article_timestamp, chain_batch_relation):\n",
    "    \"\"\"\n",
    "    X·ª≠ l√Ω nhi·ªÅu entities trong m·ªôt l·∫ßn g·ªçi API (batch processing)\n",
    "    Returns: list of new entities to process\n",
    "    \"\"\"\n",
    "    if not entity_batch:\n",
    "        return []\n",
    "    \n",
    "    max_batch_retries = 2\n",
    "    batch_retry_count = 0\n",
    "    relationships = []\n",
    "    \n",
    "    while batch_retry_count < max_batch_retries:\n",
    "        # Format input entities\n",
    "        input_entities_text = \"\"\n",
    "        for entity, impact, content in entity_batch:\n",
    "            input_entities_text += f\"Th·ª±c th·ªÉ g·ªëc: {entity}\\n\\n·∫¢nh h∆∞·ªüng: {impact}, {content}\\n\\n---\\n\\n\"\n",
    "        \n",
    "        # Create prompt\n",
    "        prompt_batch = {\n",
    "            \"input_entities\": input_entities_text,\n",
    "            \"portfolio\": portfolio_str_full,\n",
    "            \"existing_entities\": graph_entities_to_str(G)\n",
    "        }\n",
    "        \n",
    "        # Get relationships\n",
    "        response = invoke_chain_with_retry(chain_batch_relation, prompt_batch, api_manager)\n",
    "        time.sleep(1)\n",
    "        \n",
    "        if response is None:\n",
    "            return []\n",
    "        \n",
    "        # Parse response\n",
    "        relationships = parse_batch_entity_response(response)\n",
    "        \n",
    "        if len(relationships) > 0:\n",
    "            break\n",
    "            \n",
    "        batch_retry_count += 1\n",
    "        print(f\"   ‚ö† Batch processing returned 0 relationships. Retry {batch_retry_count}/{max_batch_retries}\")\n",
    "        time.sleep(BASE_DELAY)\n",
    "    \n",
    "    # Process relationships to update graph\n",
    "    next_entities = []\n",
    "    \n",
    "    for source, impact, target, content in relationships:\n",
    "        if \"kh√¥ng c√≥ th·ª±c th·ªÉ n√†o\" in source.lower() or \"kh√¥ng c√≥ th·ª±c th·ªÉ n√†o\" in target.lower():\n",
    "            continue\n",
    "        \n",
    "        # FIX: Chu·∫©n h√≥a C·∫¢ source V√Ä target qua merge_entity\n",
    "        canon_source = merge_entity(source, canonical_entities)\n",
    "        canon_target = merge_entity(target, canonical_entities)\n",
    "        \n",
    "        # Determine node type for target\n",
    "        node_type = \"stock\" if any(str(canon_target).lower().find(stock.lower()) != -1 for stock in portfolio) else \"entity\"\n",
    "        \n",
    "        # Add target node if not exists\n",
    "        if not G.has_node(canon_target):\n",
    "            G.add_node(canon_target, type=node_type, timestamp=article_timestamp)\n",
    "        \n",
    "        # Ensure source node exists in graph (should be from previous iteration)\n",
    "        if not G.has_node(canon_source):\n",
    "            # Determine source node type\n",
    "            source_type = \"stock\" if any(str(canon_source).lower().find(stock.lower()) != -1 for stock in portfolio) else \"entity\"\n",
    "            G.add_node(canon_source, type=source_type, timestamp=article_timestamp)\n",
    "            \n",
    "        # Add edge\n",
    "        add_edge(G, canon_source, canon_target, impact, article_timestamp)\n",
    "        \n",
    "        # Add to frontier if entity type\n",
    "        if node_type == \"entity\":\n",
    "            next_entities.append((canon_target, impact, content))\n",
    "    \n",
    "    return next_entities\n",
    "\n",
    "def extract_entities_from_news(\n",
    "    csv_path=\"summarized_news_with_stocks_merged.csv\",\n",
    "    output_csv=\"entities_extracted.csv\",\n",
    "    output_graph_pkl=\"knowledge_graph.pkl\",\n",
    "    output_canonical_pkl=\"canonical_entities.pkl\",\n",
    "    output_tuples_csv=\"graph_tuples_step1.csv\",\n",
    "    start_date=None,\n",
    "    end_date=None,\n",
    "    max_articles=None,\n",
    "    max_iterations=2,\n",
    "    batch_size=5\n",
    "):\n",
    "    \"\"\"\n",
    "    Tr√≠ch xu·∫•t th·ª±c th·ªÉ t·ª´ tin t·ª©c ƒë√£ t√≥m t·∫Øt v·ªõi workflow ƒë·∫ßy ƒë·ªß nh∆∞ TRR_model\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    csv_path : str\n",
    "        ƒê∆∞·ªùng d·∫´n ƒë·∫øn file CSV ch·ª©a tin t·ª©c\n",
    "    output_csv : str\n",
    "        File CSV k·∫øt qu·∫£ ch·ª©a entities\n",
    "    output_graph_pkl : str\n",
    "        File pickle ch·ª©a NetworkX graph\n",
    "    output_canonical_pkl : str\n",
    "        File pickle ch·ª©a canonical entities set\n",
    "    output_tuples_csv : str\n",
    "        File CSV ch·ª©a graph tuples (date, source, impact, target)\n",
    "    start_date : str\n",
    "        Ng√†y b·∫Øt ƒë·∫ßu (YYYY-MM-DD)\n",
    "    end_date : str\n",
    "        Ng√†y k·∫øt th√∫c (YYYY-MM-DD)\n",
    "    max_articles : int\n",
    "        S·ªë l∆∞·ª£ng b√†i b√°o t·ªëi ƒëa\n",
    "    max_iterations : int\n",
    "        S·ªë v√≤ng l·∫∑p t·ªëi ƒëa cho relation extraction\n",
    "    batch_size : int\n",
    "        S·ªë entities x·ª≠ l√Ω trong m·ªôt batch\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple: (entities_df, G, canonical_entities)\n",
    "    \"\"\"\n",
    "    print(f\"üìñ ƒêang ƒë·ªçc d·ªØ li·ªáu t·ª´ {csv_path}...\")\n",
    "    \n",
    "    # ƒê·ªçc d·ªØ li·ªáu\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        print(f\"‚úì ƒê√£ ƒë·ªçc {len(df)} tin t·ª©c\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå Kh√¥ng t√¨m th·∫•y file: {csv_path}\")\n",
    "        raise\n",
    "    \n",
    "    # Validate columns\n",
    "    required_cols = ['postID', 'stockCodes', 'title', 'description', 'date']\n",
    "    missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        print(f\"‚ùå Dataset thi·∫øu c√°c c·ªôt: {missing_cols}\")\n",
    "        print(f\"üìã C√°c c·ªôt hi·ªán c√≥: {list(df.columns)}\")\n",
    "        raise ValueError(f\"Dataset kh√¥ng h·ª£p l·ªá: thi·∫øu c·ªôt {missing_cols}\")\n",
    "    \n",
    "    print(f\"‚úì Dataset h·ª£p l·ªá v·ªõi c√°c c·ªôt: {list(df.columns)}\")\n",
    "    \n",
    "    # Chuy·ªÉn ƒë·ªïi date\n",
    "    df['parsed_date'] = pd.to_datetime(df['date'])\n",
    "    df['only_date'] = df['parsed_date'].dt.date\n",
    "    \n",
    "    # Filter by date range\n",
    "    if start_date:\n",
    "        start_dt = pd.to_datetime(start_date).date()\n",
    "        df = df[df['only_date'] >= start_dt]\n",
    "        print(f\"‚úì L·ªçc t·ª´ ng√†y {start_date}: c√≤n {len(df)} tin\")\n",
    "    \n",
    "    if end_date:\n",
    "        end_dt = pd.to_datetime(end_date).date()\n",
    "        df = df[df['only_date'] <= end_dt]\n",
    "        print(f\"‚úì L·ªçc ƒë·∫øn ng√†y {end_date}: c√≤n {len(df)} tin\")\n",
    "    \n",
    "    # Limit articles\n",
    "    if max_articles:\n",
    "        df = df.head(max_articles)\n",
    "        print(f\"‚úì Gi·ªõi h·∫°n xu·ªëng {len(df)} tin ƒë·ªÉ x·ª≠ l√Ω\")\n",
    "    \n",
    "    # Sort by date\n",
    "    df = df.sort_values('date')\n",
    "    \n",
    "    # Initialize graph and canonical entities\n",
    "    G = nx.DiGraph()\n",
    "    canonical_entities = set()\n",
    "    \n",
    "    # Build portfolio string\n",
    "    portfolio_str_full = \", \".join([f\"{stock}-{sector}\" for stock, sector in zip(PORTFOLIO_STOCKS, PORTFOLIO_SECTOR)])\n",
    "    \n",
    "    # Create chains\n",
    "    chain_entity, chain_batch_relation = create_chains(api_manager)\n",
    "    \n",
    "    # Results\n",
    "    all_entities = []\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üîç B·∫ÆT ƒê·∫¶U TR√çCH XU·∫§T TH·ª∞C TH·ªÇ V√Ä M·ªêI QUAN H·ªÜ\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Process each article\n",
    "    for idx, row in df.iterrows():\n",
    "        article_idx = idx + 1\n",
    "        article_node = f\"Article_{article_idx}: {row['title']}\"\n",
    "        article_timestamp = row['parsed_date']\n",
    "        \n",
    "        # Add article node\n",
    "        if not G.has_node(article_node):\n",
    "            G.add_node(article_node, type=\"article\", timestamp=article_timestamp)\n",
    "        \n",
    "        print(f\"\\n[{article_idx}/{len(df)}] üì∞ {row['title'][:60]}...\")\n",
    "        \n",
    "        # Get stock codes\n",
    "        stock_codes = row.get('stockCodes', '')\n",
    "        if not stock_codes or pd.isna(stock_codes):\n",
    "            stock_codes = \"(kh√¥ng c√≥)\"\n",
    "        \n",
    "        # Phase 1: Extract initial entities\n",
    "        print(f\"   üìå Phase 1: Tr√≠ch xu·∫•t entities ban ƒë·∫ßu...\")\n",
    "        max_entity_retries = MAX_RETRIES\n",
    "        entity_retry_count = 0\n",
    "        entities_dict = {\"POSITIVE\": [], \"NEGATIVE\": []}\n",
    "        \n",
    "        while entity_retry_count < max_entity_retries:\n",
    "            prompt_text = {\n",
    "                \"portfolio\": portfolio_str_full,\n",
    "                \"date\": row['date'],\n",
    "                \"stockCodes\": stock_codes,\n",
    "                \"title\": row['title'],\n",
    "                \"description\": row['description'],\n",
    "                \"existing_entities\": graph_entities_to_str(G)\n",
    "            }\n",
    "            \n",
    "            response_text = invoke_chain_with_retry(chain_entity, prompt_text, api_manager)\n",
    "            time.sleep(1)\n",
    "            \n",
    "            if response_text is None:\n",
    "                print(f\"   ‚ùå B·ªè qua tin {article_idx} do l·ªói API\")\n",
    "                break\n",
    "            \n",
    "            entities_dict = parse_entity_response(response_text)\n",
    "            \n",
    "            total_entities = len(entities_dict.get(\"POSITIVE\", [])) + len(entities_dict.get(\"NEGATIVE\", []))\n",
    "            if total_entities > 0:\n",
    "                print(f\"   ‚úì Tr√≠ch xu·∫•t ƒë∆∞·ª£c {total_entities} th·ª±c th·ªÉ ban ƒë·∫ßu\")\n",
    "                break\n",
    "                \n",
    "            entity_retry_count += 1\n",
    "            print(f\"   ‚ö† Kh√¥ng c√≥ th·ª±c th·ªÉ. Th·ª≠ l·∫°i {entity_retry_count}/{max_entity_retries}\")\n",
    "            time.sleep(BASE_DELAY)\n",
    "        \n",
    "        if entity_retry_count == max_entity_retries and total_entities == 0:\n",
    "            print(f\"   ‚ùå Th·∫•t b·∫°i sau {max_entity_retries} l·∫ßn th·ª≠\")\n",
    "            continue\n",
    "        \n",
    "        # Process initial entities and build frontier\n",
    "        initial_frontier = []\n",
    "        \n",
    "        for impact in [\"POSITIVE\", \"NEGATIVE\"]:\n",
    "            for ent, content in entities_dict.get(impact, []):\n",
    "                if not ent or \"kh√¥ng c√≥ th·ª±c th·ªÉ n√†o\" in ent.lower():\n",
    "                    continue\n",
    "                \n",
    "                canon_ent = merge_entity(ent, canonical_entities)\n",
    "                \n",
    "                node_type = \"stock\" if any(str(canon_ent).lower().find(stock.lower()) != -1 for stock in PORTFOLIO_STOCKS) else \"entity\"\n",
    "                \n",
    "                if not G.has_node(canon_ent):\n",
    "                    G.add_node(canon_ent, type=node_type, timestamp=article_timestamp)\n",
    "                \n",
    "                if not G.has_edge(article_node, canon_ent):\n",
    "                    G.add_edge(article_node, canon_ent, impact=impact, timestamp=article_timestamp)\n",
    "                \n",
    "                # Save to results\n",
    "                all_entities.append({\n",
    "                    \"article_id\": article_idx,\n",
    "                    \"article_title\": row['title'],\n",
    "                    \"date\": row['date'],\n",
    "                    \"entity\": canon_ent,\n",
    "                    \"entity_type\": node_type,\n",
    "                    \"impact\": impact,\n",
    "                    \"explanation\": content,\n",
    "                    \"iteration\": 0\n",
    "                })\n",
    "                \n",
    "                # Add to frontier if entity type\n",
    "                if node_type == \"entity\":\n",
    "                    initial_frontier.append((canon_ent, impact, content))\n",
    "        \n",
    "        # Phase 2: Relation extraction with iterations\n",
    "        print(f\"   üîó Phase 2: Tr√≠ch xu·∫•t m·ªëi quan h·ªá (max {max_iterations} iterations)...\")\n",
    "        \n",
    "        current_frontier = initial_frontier\n",
    "        \n",
    "        for iteration in range(max_iterations):\n",
    "            if not current_frontier:\n",
    "                print(f\"   ‚úì Iteration {iteration + 1}: Frontier r·ªóng, d·ª´ng s·ªõm\")\n",
    "                break\n",
    "            \n",
    "            print(f\"   üîÑ Iteration {iteration + 1}: X·ª≠ l√Ω {len(current_frontier)} entities...\")\n",
    "            \n",
    "            # Process in batches\n",
    "            next_frontier = []\n",
    "            \n",
    "            for i in range(0, len(current_frontier), batch_size):\n",
    "                batch = current_frontier[i:i + batch_size]\n",
    "                print(f\"      Batch {i//batch_size + 1}: {len(batch)} entities\")\n",
    "                \n",
    "                new_entities = batch_process_entity_relationships(\n",
    "                    batch, G, canonical_entities, PORTFOLIO_STOCKS, \n",
    "                    portfolio_str_full, article_timestamp, chain_batch_relation\n",
    "                )\n",
    "                \n",
    "                # Save new entities to results\n",
    "                for new_ent, new_impact, new_content in new_entities:\n",
    "                    all_entities.append({\n",
    "                        \"article_id\": article_idx,\n",
    "                        \"article_title\": row['title'],\n",
    "                        \"date\": row['date'],\n",
    "                        \"entity\": new_ent,\n",
    "                        \"entity_type\": \"entity\",\n",
    "                        \"impact\": new_impact,\n",
    "                        \"explanation\": new_content,\n",
    "                        \"iteration\": iteration + 1\n",
    "                    })\n",
    "                \n",
    "                next_frontier.extend(new_entities)\n",
    "            \n",
    "            print(f\"   ‚úì Iteration {iteration + 1}: Ph√°t hi·ªán {len(next_frontier)} entities m·ªõi\")\n",
    "            current_frontier = next_frontier\n",
    "        \n",
    "        print(f\"   ‚úÖ Ho√†n th√†nh x·ª≠ l√Ω tin {article_idx}\")\n",
    "    \n",
    "    # Create results DataFrame\n",
    "    entities_df = pd.DataFrame(all_entities)\n",
    "    \n",
    "    # Save entities CSV\n",
    "    entities_df.to_csv(output_csv, index=False, encoding='utf-8-sig')\n",
    "    print(f\"\\nüíæ ƒê√£ l∆∞u entities v√†o: {output_csv}\")\n",
    "    \n",
    "    # Save graph pickle\n",
    "    import pickle\n",
    "    with open(output_graph_pkl, 'wb') as f:\n",
    "        pickle.dump(G, f)\n",
    "    print(f\"üíæ ƒê√£ l∆∞u graph v√†o: {output_graph_pkl}\")\n",
    "    \n",
    "    # Save canonical entities pickle\n",
    "    with open(output_canonical_pkl, 'wb') as f:\n",
    "        pickle.dump(canonical_entities, f)\n",
    "    print(f\"üíæ ƒê√£ l∆∞u canonical entities v√†o: {output_canonical_pkl}\")\n",
    "    \n",
    "    # Export graph tuples CSV\n",
    "    print(f\"\\nüì¶ ƒêang export graph tuples...\")\n",
    "    tuples_data = []\n",
    "    for u, v, data in G.edges(data=True):\n",
    "        # Skip article nodes\n",
    "        if u.startswith(\"Article_\"):\n",
    "            continue\n",
    "        \n",
    "        timestamp = data.get('timestamp')\n",
    "        if timestamp:\n",
    "            try:\n",
    "                if isinstance(timestamp, pd.Timestamp):\n",
    "                    date_str = timestamp.date().isoformat()\n",
    "                else:\n",
    "                    date_str = pd.to_datetime(timestamp).date().isoformat()\n",
    "                \n",
    "                tuples_data.append({\n",
    "                    'date': date_str,\n",
    "                    'source': u,\n",
    "                    'impact': data.get('impact', ''),\n",
    "                    'target': v\n",
    "                })\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    if tuples_data:\n",
    "        tuples_df = pd.DataFrame(tuples_data)\n",
    "        tuples_df = tuples_df.sort_values('date')\n",
    "        tuples_df.to_csv(output_tuples_csv, index=False, encoding='utf-8-sig')\n",
    "        print(f\"‚úì ƒê√£ export {len(tuples_df)} tuples v√†o: {output_tuples_csv}\")\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"‚úÖ HO√ÄN TH√ÄNH!\")\n",
    "    print(f\"üìä T·ªïng s·ªë entities: {len(entities_df)}\")\n",
    "    print(f\"üîπ Unique entities: {len(canonical_entities)}\")\n",
    "    print(f\"üìà Graph nodes: {len(G.nodes())}\")\n",
    "    print(f\"üîó Graph edges: {len(G.edges())}\")\n",
    "    print(f\"üìÅ Files ƒë√£ t·∫°o:\")\n",
    "    print(f\"   - {output_csv}\")\n",
    "    print(f\"   - {output_graph_pkl}\")\n",
    "    print(f\"   - {output_canonical_pkl}\")\n",
    "    print(f\"   - {output_tuples_csv}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    return entities_df, G, canonical_entities\n",
    "\n",
    "print(\"‚úì ƒê√£ ƒë·ªãnh nghƒ©a h√†m extract_entities_from_news() v·ªõi workflow ƒë·∫ßy ƒë·ªß nh∆∞ TRR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea251fe5",
   "metadata": {},
   "source": [
    "## üöÄ Ch·∫°y tr√≠ch xu·∫•t th·ª±c th·ªÉ v√† m·ªëi quan h·ªá (Workflow ƒë·∫ßy ƒë·ªß)\n",
    "\n",
    "### Parameters Quan Tr·ªçng:\n",
    "\n",
    "- **csv_path**: File CSV input (`summarized_news_with_stocks_merged.csv`)\n",
    "- **start_date**: `\"2025-01-01\"` - Ch·ªâ x·ª≠ l√Ω tin t·ª´ ng√†y n√†y tr·ªü ƒëi\n",
    "- **max_articles**: `100` - S·ªë tin t·ªëi ƒëa (test), b·ªè ƒë·ªÉ ch·∫°y h·∫øt\n",
    "- **max_iterations**: `2` - S·ªë v√≤ng l·∫∑p relation extraction (gi·ªëng TRR)\n",
    "- **batch_size**: `5` - S·ªë entities x·ª≠ l√Ω trong m·ªôt batch API call\n",
    "\n",
    "### Workflow:\n",
    "1. **Load & Filter Data** - ƒê·ªçc CSV, l·ªçc theo date range\n",
    "2. **For each article:**\n",
    "   - **Phase 1:** Extract initial entities ‚Üí Add to graph\n",
    "   - **Phase 2:** Relation extraction v·ªõi 2 iterations:\n",
    "     - Iteration 1: Extract relations t·ª´ initial entities\n",
    "     - Iteration 2: Extract relations t·ª´ entities m·ªõi t√¨m ƒë∆∞·ª£c\n",
    "   - X·ª≠ l√Ω entities trong batches (5 entities/call)\n",
    "3. **Save Outputs:**\n",
    "   - `entities_extracted.csv` v·ªõi iteration info\n",
    "   - `knowledge_graph.pkl` - NetworkX graph\n",
    "   - `canonical_entities.pkl` - Entity set\n",
    "   - `graph_tuples_step1.csv` - **Tuples CSV gi·ªëng TRR**\n",
    "\n",
    "### Outputs:\n",
    "- ‚úÖ DataFrame v·ªõi iteration tracking\n",
    "- ‚úÖ NetworkX DiGraph v·ªõi article, entity, stock nodes\n",
    "- ‚úÖ CSV tuples format: `(date, source, impact, target)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a79ca8e",
   "metadata": {},
   "source": [
    "## üìä Ph√¢n t√≠ch k·∫øt qu·∫£\n",
    "\n",
    "Xem c√°c th·ª±c th·ªÉ ƒë√£ tr√≠ch xu·∫•t:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505062d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä PH√ÇN T√çCH KNOWLEDGE GRAPH\n",
      "\n",
      "============================================================\n",
      "\n",
      "T·ªïng s·ªë nodes: 21\n",
      "T·ªïng s·ªë edges: 129\n",
      "\n",
      "üìå Ph√¢n lo·∫°i nodes:\n",
      "   article: 5\n",
      "   entity: 16\n",
      "\n",
      "üî• TOP 10 ENTITIES ƒê∆Ø·ª¢C NH·∫ÆC ƒê·∫æN/·∫¢NH H∆Ø·ªûNG NHI·ªÄU NH·∫§T:\n",
      "\n",
      " 1. [entity] Ng∆∞·ªùi ti√™u d√πng Vi·ªát Nam: 14 connections\n",
      " 2. [entity] Doanh nghi·ªáp b√°n l·∫ª Vi·ªát Nam: 13 connections\n",
      " 3. [entity] Doanh nghi·ªáp c√¥ng ngh·ªá Vi·ªát Nam: 13 connections\n",
      " 4. [entity] Ng∆∞·ªùi lao ƒë·ªông Vi·ªát Nam: 13 connections\n",
      " 5. [entity] C∆° quan qu·∫£n l√Ω nh√† n∆∞·ªõc Vi·ªát Nam: 13 connections\n",
      " 6. [entity] Ng√†nh c√¥ng ngh·ªá Vi·ªát Nam: 12 connections\n",
      " 7. [entity] Ng√†nh b√°n l·∫ª Vi·ªát Nam: 11 connections\n",
      " 8. [entity] C∆° quan thu·∫ø Vi·ªát Nam: 11 connections\n",
      " 9. [entity] Doanh nghi·ªáp xu·∫•t kh·∫©u ƒê·ªìng Th√°p: 9 connections\n",
      "10. [entity] Ng√†nh n√¥ng nghi·ªáp Vi·ªát Nam: 8 connections\n",
      "\n",
      "üåä TOP 10 ENTITIES ·∫¢NH H∆Ø·ªûNG NHI·ªÄU NH·∫§T ƒê·∫æN ENTITIES KH√ÅC:\n",
      "\n",
      " 1. [entity] C∆° quan thu·∫ø Vi·ªát Nam: 12 connections\n",
      " 2. [entity] Ng√†nh c√¥ng ngh·ªá Vi·ªát Nam: 11 connections\n",
      " 3. [entity] Doanh nghi·ªáp b√°n l·∫ª Vi·ªát Nam: 11 connections\n",
      " 4. [entity] C∆° quan qu·∫£n l√Ω nh√† n∆∞·ªõc Vi·ªát Nam: 11 connections\n",
      " 5. [entity] Ng∆∞·ªùi ti√™u d√πng Vi·ªát Nam: 10 connections\n",
      " 6. [entity] Ng∆∞·ªùi lao ƒë·ªông Vi·ªát Nam: 10 connections\n",
      " 7. [entity] Ng√†nh b√°n l·∫ª Vi·ªát Nam: 9 connections\n",
      " 8. [entity] Doanh nghi·ªáp c√¥ng ngh·ªá Vi·ªát Nam: 8 connections\n",
      " 9. [entity] ƒê·ªìng Th√°p: 8 connections\n",
      "10. [entity] Ng√†nh n√¥ng nghi·ªáp Vi·ªát Nam: 8 connections\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# PH√ÇN T√çCH GRAPH - NETWORK STRUCTURE\n",
    "# ============================================================\n",
    "\n",
    "def analyze_graph(G):\n",
    "    \"\"\"\n",
    "    Ph√¢n t√≠ch knowledge graph ƒë√£ x√¢y d·ª±ng (gi·ªëng TRR_model)\n",
    "    \"\"\"\n",
    "    print(f\"üìä PH√ÇN T√çCH KNOWLEDGE GRAPH\\n\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Th·ªëng k√™ c∆° b·∫£n\n",
    "    print(f\"T·ªïng s·ªë nodes: {len(G.nodes())}\")\n",
    "    print(f\"T·ªïng s·ªë edges: {len(G.edges())}\")\n",
    "    \n",
    "    # Ph√¢n lo·∫°i nodes\n",
    "    node_types = {}\n",
    "    for node, data in G.nodes(data=True):\n",
    "        node_type = data.get('type', 'unknown')\n",
    "        node_types[node_type] = node_types.get(node_type, 0) + 1\n",
    "    \n",
    "    print(f\"\\nüìå Ph√¢n lo·∫°i nodes:\")\n",
    "    for ntype, count in node_types.items():\n",
    "        print(f\"   {ntype}: {count}\")\n",
    "    \n",
    "    # T√¨m nodes c√≥ nhi·ªÅu k·∫øt n·ªëi nh·∫•t\n",
    "    entity_nodes = [n for n, d in G.nodes(data=True) if d.get('type') in ['entity', 'stock']]\n",
    "    \n",
    "    if entity_nodes:\n",
    "        # In-degree: s·ªë l∆∞·ª£ng b√†i b√°o/entity li√™n k·∫øt ƒë·∫øn\n",
    "        in_degrees = [(node, G.in_degree(node)) for node in entity_nodes]\n",
    "        in_degrees.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        print(f\"\\nüî• TOP 10 ENTITIES ƒê∆Ø·ª¢C NH·∫ÆC ƒê·∫æN/·∫¢NH H∆Ø·ªûNG NHI·ªÄU NH·∫§T:\\n\")\n",
    "        for i, (node, degree) in enumerate(in_degrees[:10], 1):\n",
    "            node_type = G.nodes[node].get('type', 'unknown')\n",
    "            print(f\"{i:2d}. [{node_type}] {node}: {degree} connections\")\n",
    "        \n",
    "        # Out-degree: s·ªë l∆∞·ª£ng entity m√† node n√†y ·∫£nh h∆∞·ªüng ƒë·∫øn\n",
    "        out_degrees = [(node, G.out_degree(node)) for node in entity_nodes]\n",
    "        out_degrees.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        print(f\"\\nüåä TOP 10 ENTITIES ·∫¢NH H∆Ø·ªûNG NHI·ªÄU NH·∫§T ƒê·∫æN ENTITIES KH√ÅC:\\n\")\n",
    "        for i, (node, degree) in enumerate(out_degrees[:10], 1):\n",
    "            node_type = G.nodes[node].get('type', 'unknown')\n",
    "            print(f\"{i:2d}. [{node_type}] {node}: {degree} connections\")\n",
    "    \n",
    "    # Ph√¢n t√≠ch stock nodes ƒë·∫∑c bi·ªát\n",
    "    stock_nodes = [n for n, d in G.nodes(data=True) if d.get('type') == 'stock']\n",
    "    if stock_nodes:\n",
    "        print(f\"\\nüìà PH√ÇN T√çCH C√ÅC C·ªî PHI·∫æU TRONG PORTFOLIO:\\n\")\n",
    "        for stock in stock_nodes:\n",
    "            in_deg = G.in_degree(stock)\n",
    "            out_deg = G.out_degree(stock)\n",
    "            \n",
    "            # Count positive vs negative impacts\n",
    "            pos_count = len([e for e in G.in_edges(stock, data=True) if e[2].get('impact') == 'POSITIVE'])\n",
    "            neg_count = len([e for e in G.in_edges(stock, data=True) if e[2].get('impact') == 'NEGATIVE'])\n",
    "            \n",
    "            print(f\"   {stock}:\")\n",
    "            print(f\"      In: {in_deg} (‚úÖ {pos_count} | ‚ùå {neg_count})\")\n",
    "            print(f\"      Out: {out_deg}\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "\n",
    "# Ch·∫°y ph√¢n t√≠ch\n",
    "analyze_graph(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b70144",
   "metadata": {},
   "source": [
    "## üíæ L∆∞u Graph ƒë·ªÉ s·ª≠ d·ª•ng sau\n",
    "\n",
    "L∆∞u graph v√†o file pickle ƒë·ªÉ s·ª≠ d·ª•ng cho c√°c b∆∞·ªõc ti·∫øp theo (relation extraction, attention mechanism...):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcc08b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ C√ÅC FILES ƒê√É ƒê∆Ø·ª¢C L∆ØU T·ª∞ ƒê·ªòNG:\n",
      "\n",
      "1. entities_extracted.csv - DataFrame ch·ª©a t·∫•t c·∫£ entities v·ªõi iteration info\n",
      "2. knowledge_graph.pkl - NetworkX DiGraph (c·∫•u tr√∫c y h·ªát TRR)\n",
      "3. canonical_entities.pkl - Set c√°c entity canonical\n",
      "4. graph_tuples_step1.csv - CSV tuples (date, source, impact, target)\n",
      "\n",
      "============================================================\n",
      "üìñ C√ÅCH LOAD L·∫†I C√ÅC FILES:\n",
      "\n",
      "\n",
      "# Load entities DataFrame\n",
      "import pandas as pd\n",
      "entities_df = pd.read_csv('entities_extracted.csv')\n",
      "\n",
      "# Load NetworkX graph\n",
      "import pickle\n",
      "with open('knowledge_graph.pkl', 'rb') as f:\n",
      "    G = pickle.load(f)\n",
      "\n",
      "# Load canonical entities\n",
      "with open('canonical_entities.pkl', 'rb') as f:\n",
      "    canonical_entities = pickle.load(f)\n",
      "\n",
      "# Load graph tuples\n",
      "tuples_df = pd.read_csv('graph_tuples_step1.csv')\n",
      "\n",
      "============================================================\n",
      "üîó C·∫§U TR√öC GRAPH TUPLES CSV:\n",
      "\n",
      "Columns: date, source, impact, target\n",
      "Format: YYYY-MM-DD, Entity_Name, POSITIVE/NEGATIVE, Target_Entity\n",
      "\n",
      "V√≠ d·ª•:\n",
      "2025-01-01, FPT-C√¥ng ngh·ªá, POSITIVE TO, Ng√†nh c√¥ng ngh·ªá Vi·ªát Nam\n",
      "2025-01-01, Ng√†nh c√¥ng ngh·ªá Vi·ªát Nam, NEGATIVE TO, Ng√†nh s·∫£n xu·∫•t Vi·ªát Nam\n",
      "\n",
      "============================================================\n",
      "‚ú® GRAPH STRUCTURE HO√ÄN CH·ªàNH GI·ªêNG TRR_MODEL!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ƒê√É L∆ØU C√ÅC FILES - H∆Ø·ªöNG D·∫™N S·ª¨ D·ª§NG\n",
    "# ============================================================\n",
    "\n",
    "print(\"‚úÖ C√ÅC FILES ƒê√É ƒê∆Ø·ª¢C L∆ØU T·ª∞ ƒê·ªòNG:\\n\")\n",
    "print(\"1. entities_extracted.csv - DataFrame ch·ª©a t·∫•t c·∫£ entities v·ªõi iteration info\")\n",
    "print(\"2. knowledge_graph.pkl - NetworkX DiGraph (c·∫•u tr√∫c y h·ªát TRR)\")\n",
    "print(\"3. canonical_entities.pkl - Set c√°c entity canonical\")\n",
    "print(\"4. graph_tuples_step1.csv - CSV tuples (date, source, impact, target)\\n\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üìñ C√ÅCH LOAD L·∫†I C√ÅC FILES:\\n\")\n",
    "\n",
    "print(\"\"\"\n",
    "# Load entities DataFrame\n",
    "import pandas as pd\n",
    "entities_df = pd.read_csv('entities_extracted.csv')\n",
    "\n",
    "# Load NetworkX graph\n",
    "import pickle\n",
    "with open('knowledge_graph.pkl', 'rb') as f:\n",
    "    G = pickle.load(f)\n",
    "\n",
    "# Load canonical entities\n",
    "with open('canonical_entities.pkl', 'rb') as f:\n",
    "    canonical_entities = pickle.load(f)\n",
    "\n",
    "# Load graph tuples\n",
    "tuples_df = pd.read_csv('graph_tuples_step1.csv')\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üîó C·∫§U TR√öC GRAPH TUPLES CSV:\\n\")\n",
    "print(\"Columns: date, source, impact, target\")\n",
    "print(\"Format: YYYY-MM-DD, Entity_Name, POSITIVE/NEGATIVE, Target_Entity\")\n",
    "print(\"\\nV√≠ d·ª•:\")\n",
    "print(\"2025-01-01, FPT-C√¥ng ngh·ªá, POSITIVE TO, Ng√†nh c√¥ng ngh·ªá Vi·ªát Nam\")\n",
    "print(\"2025-01-01, Ng√†nh c√¥ng ngh·ªá Vi·ªát Nam, NEGATIVE TO, Ng√†nh s·∫£n xu·∫•t Vi·ªát Nam\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚ú® GRAPH STRUCTURE HO√ÄN CH·ªàNH GI·ªêNG TRR_MODEL!\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
