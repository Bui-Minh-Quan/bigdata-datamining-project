{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "734e43e5",
   "metadata": {},
   "source": [
    "# ü§ñ Entity Extractor - TRR Step 1: Tr√≠ch xu·∫•t th·ª±c th·ªÉ & M·ªëi quan h·ªá\n",
    "\n",
    "## ‚ú® PHI√äN B·∫¢N M·ªöI - HO√ÄN CH·ªàNH GI·ªêNG TRR_MODEL\n",
    "\n",
    "Notebook n√†y ƒë√£ ƒë∆∞·ª£c c·∫£i ti·∫øn ƒë·ªÉ c√≥ c·∫•u tr√∫c graph y h·ªát TRR_model v·ªõi:\n",
    "\n",
    "### üéØ 3 Prompt Templates:\n",
    "1. **Entity Extraction** - Tr√≠ch xu·∫•t entities ban ƒë·∫ßu\n",
    "2. **Relation Extraction** - Tr√≠ch xu·∫•t m·ªëi quan h·ªá cho 1 entity\n",
    "3. **Batch Relation Extraction** - X·ª≠ l√Ω nhi·ªÅu entities c√πng l√∫c\n",
    "\n",
    "### üîÑ Workflow ƒê·∫ßy ƒê·ªß:\n",
    "- **Phase 1:** Entity Extraction t·ª´ b√†i b√°o\n",
    "- **Phase 2:** Relation Extraction v·ªõi iterations (max 2 rounds)\n",
    "- **Batch Processing:** X·ª≠ l√Ω nhi·ªÅu entities trong m·ªôt API call\n",
    "\n",
    "### üì¶ Output Files:\n",
    "1. `entities_extracted.csv` - DataFrame ch·ª©a entities v·ªõi iteration info\n",
    "2. `knowledge_graph.pkl` - NetworkX DiGraph\n",
    "3. `canonical_entities.pkl` - Set c√°c entity canonical\n",
    "4. `graph_tuples_step1.csv` - **CSV tuples (date, source, impact, target)**\n",
    "\n",
    "## Input:\n",
    "- File CSV: `summarized_news_with_stocks_merged.csv`\n",
    "- Columns: `postID`, `stockCodes`, `title`, `description`, `date`\n",
    "- Start date: `2025-01-01`\n",
    "\n",
    "## Thay ƒê·ªïi So V·ªõi Phi√™n B·∫£n C≈©:\n",
    "- ‚úÖ Th√™m 2 prompt templates m·ªõi (relation extraction, batch relation)\n",
    "- ‚úÖ Th√™m workflow iterations gi·ªëng TRR\n",
    "- ‚úÖ Th√™m batch processing ƒë·ªÉ t·ªëi ∆∞u API calls\n",
    "- ‚úÖ Export CSV tuples v·ªõi format chu·∫©n `(date, source, impact, target)`\n",
    "- ‚úÖ S·ª≠ d·ª•ng `stockCodes` t·ª´ dataset thay v√¨ `group`\n",
    "- ‚úÖ L∆∞u iteration info cho m·ªói entity\n",
    "- ‚úÖ C·∫•u tr√∫c graph ho√†n ch·ªânh v·ªõi article nodes, entity nodes, stock nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3c679e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì ƒê√£ import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "import google.generativeai as genai\n",
    "import networkx as nx\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "print(\"‚úì ƒê√£ import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "68c32e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì ƒê√£ ƒë·ªãnh nghƒ©a APIKeyManager\n",
      "‚úì Ph√°t hi·ªán 4 API keys\n",
      "üîë ƒêang s·ª≠ d·ª•ng: GOOGLE_API_KEY (Key 1/4)\n"
     ]
    }
   ],
   "source": [
    "class APIKeyManager:\n",
    "    \"\"\"\n",
    "    Qu·∫£n l√Ω nhi·ªÅu Google API keys v√† t·ª± ƒë·ªông chuy·ªÉn ƒë·ªïi khi g·∫∑p l·ªói.\n",
    "    \n",
    "    Quy t·∫Øc:\n",
    "    - M·ªói key ƒë∆∞·ª£c th·ª≠ t·ªëi ƒëa 2 l·∫ßn\n",
    "    - Sau 2 l·∫ßn l·ªói ‚Üí t·ª± ƒë·ªông chuy·ªÉn key ti·∫øp theo\n",
    "    - H·∫øt key ‚Üí b√°o l·ªói\n",
    "    \"\"\"\n",
    "    \n",
    "    MAX_RETRIES_PER_KEY = 2  # S·ªë l·∫ßn th·ª≠ t·ªëi ƒëa cho m·ªói key\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Kh·ªüi t·∫°o v√† load t·∫•t c·∫£ API keys t·ª´ .env\"\"\"\n",
    "        # Load c√°c API keys t·ª´ environment\n",
    "        self.keys = [\n",
    "            (\"GOOGLE_API_KEY\", os.getenv(\"GOOGLE_API_KEY\")),\n",
    "            (\"GOOGLE_API_KEY_2\", os.getenv(\"GOOGLE_API_KEY_2\")),\n",
    "            (\"GOOGLE_API_KEY_3\", os.getenv(\"GOOGLE_API_KEY_3\")),\n",
    "            (\"GOOGLE_API_KEY_4\", os.getenv(\"GOOGLE_API_KEY_4\")),\n",
    "        ]\n",
    "        \n",
    "        # Ch·ªâ gi·ªØ l·∫°i c√°c key h·ª£p l·ªá (kh√¥ng None)\n",
    "        self.keys = [(name, key) for name, key in self.keys if key]\n",
    "        \n",
    "        if not self.keys:\n",
    "            raise ValueError(\"‚ùå Kh√¥ng t√¨m th·∫•y API key! Ki·ªÉm tra file .env\")\n",
    "        \n",
    "        # Kh·ªüi t·∫°o tr·∫°ng th√°i\n",
    "        self.current_index = 0\n",
    "        self.error_counts = {name: 0 for name, _ in self.keys}  # ƒê·∫øm l·ªói m·ªói key\n",
    "        \n",
    "        print(f\"‚úì Ph√°t hi·ªán {len(self.keys)} API keys\")\n",
    "        self._activate_key(0)\n",
    "    \n",
    "    def _activate_key(self, index):\n",
    "        \"\"\"K√≠ch ho·∫°t API key t·∫°i v·ªã tr√≠ index\"\"\"\n",
    "        if index >= len(self.keys):\n",
    "            raise Exception(\"‚ùå ƒê√£ h·∫øt t·∫•t c·∫£ API keys!\")\n",
    "        \n",
    "        self.current_index = index\n",
    "        key_name, key_value = self.keys[index]\n",
    "        \n",
    "        # C·∫•u h√¨nh Google AI v·ªõi key m·ªõi\n",
    "        genai.configure(api_key=key_value)\n",
    "        \n",
    "        print(f\"üîë ƒêang s·ª≠ d·ª•ng: {key_name} (Key {index + 1}/{len(self.keys)})\")\n",
    "    \n",
    "    def get_current_key(self):\n",
    "        \"\"\"L·∫•y API key hi·ªán t·∫°i\"\"\"\n",
    "        return self.keys[self.current_index][1]\n",
    "    \n",
    "    def get_models(self):\n",
    "        \"\"\"\n",
    "        T·∫°o c√°c model AI v·ªõi API key hi·ªán t·∫°i.\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (model, model_more_temp, model_pro)\n",
    "        \"\"\"\n",
    "        current_key = self.get_current_key()\n",
    "        \n",
    "        model = ChatGoogleGenerativeAI(\n",
    "            model=\"gemini-2.5-flash-lite\", \n",
    "            temperature=0.02,\n",
    "            google_api_key=current_key\n",
    "        )\n",
    "        \n",
    "        model_more_temp = ChatGoogleGenerativeAI(\n",
    "            model=\"gemini-2.5-flash-lite\", \n",
    "            temperature=0.1,\n",
    "            google_api_key=current_key\n",
    "        )\n",
    "        \n",
    "        model_pro = ChatGoogleGenerativeAI(\n",
    "            model=\"gemini-2.5-pro\", \n",
    "            temperature=0.1,\n",
    "            google_api_key=current_key\n",
    "        )\n",
    "        \n",
    "        return model, model_more_temp, model_pro\n",
    "    \n",
    "    def on_error(self):\n",
    "        \"\"\"\n",
    "        X·ª≠ l√Ω khi g·∫∑p l·ªói API.\n",
    "        \n",
    "        Returns:\n",
    "            bool: True n·∫øu c√≤n key kh√°c ƒë·ªÉ th·ª≠, False n·∫øu h·∫øt key\n",
    "        \"\"\"\n",
    "        key_name = self.keys[self.current_index][0]\n",
    "        self.error_counts[key_name] += 1\n",
    "        \n",
    "        error_count = self.error_counts[key_name]\n",
    "        print(f\"‚ö† L·ªói l·∫ßn {error_count} v·ªõi {key_name}\")\n",
    "        \n",
    "        # N·∫øu ƒë√£ ƒë·∫°t gi·ªõi h·∫°n retry cho key n√†y\n",
    "        if error_count >= self.MAX_RETRIES_PER_KEY:\n",
    "            print(f\"‚õî {key_name} ƒë√£ l·ªói {error_count}/{self.MAX_RETRIES_PER_KEY} l·∫ßn\")\n",
    "            \n",
    "            # Th·ª≠ chuy·ªÉn sang key ti·∫øp theo\n",
    "            next_index = self.current_index + 1\n",
    "            if next_index < len(self.keys):\n",
    "                print(f\"üîÑ Chuy·ªÉn sang key ti·∫øp theo...\")\n",
    "                self._activate_key(next_index)\n",
    "                return True\n",
    "            else:\n",
    "                print(f\"‚ùå ƒê√£ h·∫øt t·∫•t c·∫£ {len(self.keys)} keys!\")\n",
    "                return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def reset_errors(self):\n",
    "        \"\"\"Reset error counters cho t·∫•t c·∫£ keys\"\"\"\n",
    "        self.error_counts = {name: 0 for name, _ in self.keys}\n",
    "        print(\"‚ôª ƒê√£ reset error counters\")\n",
    "\n",
    "print(\"‚úì ƒê√£ ƒë·ªãnh nghƒ©a APIKeyManager\")\n",
    "\n",
    "# Kh·ªüi t·∫°o API Manager\n",
    "api_manager = APIKeyManager()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851c0369",
   "metadata": {},
   "source": [
    "## üìù Prompt Templates (Ho√†n ch·ªânh nh∆∞ TRR Model)\n",
    "\n",
    "Notebook n√†y ƒë√£ ƒë∆∞·ª£c c·∫£i ti·∫øn ƒë·ªÉ c√≥ c·∫•u tr√∫c graph y h·ªát TRR_model v·ªõi 3 prompt templates:\n",
    "\n",
    "### 1. Entity Extraction Prompt\n",
    "- **M·ª•c ƒë√≠ch:** Tr√≠ch xu·∫•t c√°c th·ª±c th·ªÉ ban ƒë·∫ßu t·ª´ b√†i b√°o\n",
    "- **Input:** `date`, `stockCodes`, `title`, `description`, `portfolio`, `existing_entities`\n",
    "- **Output:** `[[POSITIVE]]` v√† `[[NEGATIVE]]` entities v·ªõi gi·∫£i th√≠ch\n",
    "- **ƒê·∫∑c ƒëi·ªÉm:** \n",
    "  - S·ª≠ d·ª•ng `stockCodes` t·ª´ dataset thay v√¨ `group`\n",
    "  - Focus v√†o 10 m√£ c·ªï phi·∫øu trong portfolio\n",
    "  - B·∫Øt bu·ªôc tr√≠ch d·∫´n s·ªë li·ªáu c·ª• th·ªÉ\n",
    "\n",
    "### 2. Relation Extraction Prompt\n",
    "- **M·ª•c ƒë√≠ch:** Tr√≠ch xu·∫•t m·ªëi quan h·ªá hi·ªáu ·ª©ng d√¢y chuy·ªÅn t·ª´ m·ªôt th·ª±c th·ªÉ\n",
    "- **Input:** `entities` (th·ª±c th·ªÉ g·ªëc), `description`, `portfolio`, `existing_entities`\n",
    "- **Output:** C√°c th·ª±c th·ªÉ b·ªã ·∫£nh h∆∞·ªüng b·ªüi th·ª±c th·ªÉ g·ªëc\n",
    "- **Gi·ªõi h·∫°n:** T·ªëi ƒëa 2 th·ª±c th·ªÉ m·ªõi, li√™n k·∫øt t·ªõi 4 th·ª±c th·ªÉ kh√°c\n",
    "\n",
    "### 3. Batch Relation Extraction Prompt\n",
    "- **M·ª•c ƒë√≠ch:** X·ª≠ l√Ω nhi·ªÅu th·ª±c th·ªÉ c√πng l√∫c (hi·ªáu qu·∫£ h∆°n)\n",
    "- **Input:** `input_entities` (nhi·ªÅu th·ª±c th·ªÉ g·ªëc), `portfolio`, `existing_entities`\n",
    "- **Output:** M·ªëi quan h·ªá cho t·ª´ng th·ª±c th·ªÉ g·ªëc theo ƒë·ªãnh d·∫°ng `[[SOURCE:...]]`\n",
    "- **Gi·ªõi h·∫°n:** 2 th·ª±c th·ªÉ m·ªõi/th·ª±c th·ªÉ g·ªëc, li√™n k·∫øt t·ªõi 3 th·ª±c th·ªÉ/th·ª±c th·ªÉ g·ªëc\n",
    "\n",
    "## üîÑ Workflow (Gi·ªëng TRR Model)\n",
    "\n",
    "1. **Phase 1: Entity Extraction** - Tr√≠ch xu·∫•t entities ban ƒë·∫ßu t·ª´ b√†i b√°o\n",
    "2. **Phase 2: Relation Extraction** - L·∫∑p qua `max_iterations` ƒë·ªÉ x√¢y d·ª±ng m·ªëi quan h·ªá\n",
    "   - M·ªói iteration x·ª≠ l√Ω entities trong batches\n",
    "   - Entities m·ªõi ƒë∆∞·ª£c th√™m v√†o frontier cho iteration ti·∫øp theo\n",
    "3. **Output:** \n",
    "   - `entities_extracted.csv` - DataFrame ch·ª©a t·∫•t c·∫£ entities\n",
    "   - `knowledge_graph.pkl` - NetworkX DiGraph\n",
    "   - `canonical_entities.pkl` - Set c√°c entity canonical\n",
    "   - `graph_tuples_step1.csv` - CSV tuples `(date, source, impact, target)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "af71d02d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì ƒê√£ ƒë·ªãnh nghƒ©a 3 prompt templates: entity_extraction, relation_extraction, batch_relation_extraction\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# PROMPT TEMPLATES - Tr√≠ch xu·∫•t th·ª±c th·ªÉ v√† m·ªëi quan h·ªá\n",
    "# ============================================================\n",
    "\n",
    "# Template tr√≠ch xu·∫•t th·ª±c th·ªÉ t·ª´ tin t·ª©c\n",
    "entity_extraction_template = PromptTemplate.from_template(\"\"\"B·∫°n ƒë∆∞·ª£c cho m·ªôt ho·∫∑c nhi·ªÅu b√†i b√°o, bao g·ªìm t·ª±a ƒë·ªÅ v√† m√¥ t·∫£ ng·∫Øn g·ªçn v·ªÅ b√†i b√°o ƒë√≥, ngo√†i ra b·∫°n c√≥\n",
    "th√¥ng tin v·ªÅ ng√†y xu·∫•t b·∫£n c·ªßa b√†i b√°o, v√† lo·∫°i ch·ªß ƒë·ªÅ m√† b√†i b√°o ƒëang ƒë·ªÅ c·∫≠p t·ªõi.\n",
    "\n",
    "[QUAN TR·ªåNG] H·∫°n ch·∫ø t·∫°o m·ªõi m·ªôt th·ª±c th·ªÉ, ch·ªâ t·∫°o li√™n k·∫øt t·ªõi 5 th·ª±c th·ªÉ. Lu√¥n ∆∞u ti√™n li√™n k·∫øt v·ªõi c√°c th·ª±c th·ªÉ ƒë√£ c√≥: {existing_entities}\n",
    "\n",
    "B·∫°n c·∫ßn ph√¢n t√≠ch b√†i b√°o, ƒë∆∞a ra t√™n c·ªßa nh·ªØng th·ª±c th·ªÉ (v√≠ d·ª• nh∆∞ c·ªï phi·∫øu, ng√†nh ngh·ªÅ, c√¥ng ty, qu·ªëc gia, t·ªânh th√†nh...)\n",
    "s·∫Ω b·ªã ·∫£nh h∆∞·ªüng tr·ª±c ti·∫øp b·ªüi th√¥ng tin c·ªßa b√†i b√°o, theo h∆∞·ªõng t√≠ch c·ª±c ho·∫∑c ti√™u c·ª±c.\n",
    "\n",
    "V·ªõi m·ªói th·ª±c th·ªÉ, ·ªü ph·∫ßn T√™n th·ª±c th·ªÉ, h·∫°n ch·∫ø d√πng d·∫•u ch·∫•m, g·∫°ch ngang, d·∫•u v√† &, d·∫•u ch·∫•m ph·∫©y ;. V√† c·∫ßn ghi th√™m qu·ªëc gia, ƒë·ªãa ph∆∞∆°ng c·ª• th·ªÉ v√† ng√†nh ngh·ªÅ c·ªßa n√≥ (n·∫øu c√≥).\n",
    "T√™n ch·ªâ n√≥i t·ªõi m·ªôt th·ª±c th·ªÉ duy nh·∫•t. Ph·∫ßn T√™n kh√¥ng ƒë∆∞·ª£c qu√° ph·ª©c t·∫°p, ƒë∆°n gi·∫£n nh·∫•t c√≥ th·ªÉ.\n",
    "N·∫øu th·ª±c th·ªÉ n√†o thu·ªôc danh m·ª•c c·ªï phi·∫øu sau: {portfolio}, h√£y ghi r√µ t√™n c·ªï phi·∫øu.\n",
    "V√≠ d·ª•: SSI-Ch·ª©ng kho√°n; Ng√†nh c√¥ng nghi·ªáp Vi·ªát Nam; Ng∆∞·ªùi d√πng M·ªπ; Ng√†nh th√©p Ch√¢u √Å; Ng√†nh du l·ªãch H·∫° Long, ...\n",
    "\n",
    "Ghi nh·ªõ, H·∫°n ch·∫ø t·∫°o m·ªõi m·ªôt th·ª±c th·ªÉ, ch·ªâ t·∫°o li√™n k·∫øt t·ªõi 5 th·ª±c th·ªÉ. Lu√¥n c·ªë li√™n k·∫øt v·ªõi c√°c th·ª±c th·ªÉ ƒë√£ c√≥.\n",
    "\n",
    "Ph·∫ßn gi·∫£i th√≠ch m·ªói th·ª±c th·ªÉ, b·∫Øt bu·ªôc ƒë√°nh gi√° s·ªë li·ªáu ƒë∆∞·ª£c ghi, nhi·ªÅu ho·∫∑c √≠t, tƒÉng ho·∫∑c gi·∫£m, g·∫•p bao nhi√™u l·∫ßn, ...\n",
    "C·∫ßn c·ªë g·∫Øng li√™n k·∫øt v·ªõi nhi·ªÅu th·ª±c th·ªÉ kh√°c. Tuy nhi√™n kh√¥ng suy ngo√†i ph·∫°m vi b√†i b√°o. Kh√¥ng t·ª± ch√®n s·ªë li·ªáu ngo√†i b√†i b√°o.\n",
    "Kh√¥ng d√πng d·∫•u hai ch·∫•m trong ph·∫ßn gi·∫£i th√≠ch, ch·ªâ d√πng hai ch·∫•m : ƒë·ªÉ t√°ch gi·ªØa T√™n th·ª±c th·ªÉ v√† ph·∫ßn gi·∫£i th√≠ch.\n",
    "                                                          \n",
    "ƒê∆∞a ra theo ƒë·ªãnh d·∫°ng sau:\n",
    "[[POSITIVE]]\n",
    "[Entity 1]: [Explanation]\n",
    "...\n",
    "[Entity N]: [Explanation]\n",
    "\n",
    "[[NEGATIVE]]\n",
    "[Entity A]: [Explanation]\n",
    "..\n",
    "[Entity Z]: [Explanation]\n",
    "                                                          \n",
    "M·ªôt v√≠ d·ª• cho b√†i b√°o:\n",
    "\n",
    "(B·∫ÆT ƒê·∫¶U V√ç D·ª§)\n",
    "\n",
    "Ng√†y ƒëƒÉng: 2025-01-01T00:00:00+07:00\n",
    "M√£ c·ªï phi·∫øu li√™n quan: (kh√¥ng c√≥)\n",
    "T·ª±a ƒë·ªÅ: S·ªë l∆∞·ª£ng h√≥a ƒë∆°n kh·ªüi t·∫°o t·ª´ m√°y t√≠nh ti·ªÅn tƒÉng g·∫•p 13 l·∫ßn nƒÉm 2023\n",
    "\n",
    "M√¥ t·∫£: T·ª∑ l·ªá c∆° s·ªü kinh doanh s·ª≠ d·ª•ng h√≥a ƒë∆°n ƒëi·ªán t·ª≠ tƒÉng m·∫°nh, v·ªõi s·ªë l∆∞·ª£ng h√≥a ƒë∆°n t·ª´ m√°y t√≠nh ti·ªÅn tƒÉng g·∫•p 13 l·∫ßn so v·ªõi nƒÉm tr∆∞·ªõc. Ng√†nh b√°n l·∫ª v√† d·ªãch v·ª• h∆∞·ªüng l·ª£i l·ªõn t·ª´ chuy·ªÉn ƒë·ªïi s·ªë n√†y.\n",
    "\n",
    "Danh s√°ch th·ª±c th·ªÉ s·∫Ω b·ªã ·∫£nh h∆∞·ªüng:\n",
    "\n",
    "[[POSITIVE]]\n",
    "Ng√†nh b√°n l·∫ª Vi·ªát Nam: S·ªë l∆∞·ª£ng h√≥a ƒë∆°n ƒëi·ªán t·ª≠ t·ª´ m√°y t√≠nh ti·ªÅn tƒÉng g·∫•p 13 l·∫ßn trong nƒÉm 2023, gi√∫p tƒÉng hi·ªáu qu·∫£ qu·∫£n l√Ω v√† gi·∫£m chi ph√≠ v·∫≠n h√†nh\n",
    "MWG-B√°n l·∫ª: L√† chu·ªói b√°n l·∫ª l·ªõn, h∆∞·ªüng l·ª£i tr·ª±c ti·∫øp t·ª´ vi·ªác s·ªë h√≥a h√≥a ƒë∆°n tƒÉng 13 l·∫ßn, c·∫£i thi·ªán kh·∫£ nƒÉng qu·∫£n l√Ω t·ªìn kho v√† d√≤ng ti·ªÅn\n",
    "Ng√†nh c√¥ng ngh·ªá Vi·ªát Nam: Cung c·∫•p gi·∫£i ph√°p h√≥a ƒë∆°n ƒëi·ªán t·ª≠ v√† m√°y t√≠nh ti·ªÅn cho h√†ng ngh√¨n c∆° s·ªü kinh doanh, doanh thu d·ª± ki·∫øn tƒÉng m·∫°nh\n",
    "FPT-C√¥ng ngh·ªá: L√† nh√† cung c·∫•p gi·∫£i ph√°p chuy·ªÉn ƒë·ªïi s·ªë h√†ng ƒë·∫ßu, h∆∞·ªüng l·ª£i t·ª´ nhu c·∫ßu tri·ªÉn khai h√≥a ƒë∆°n ƒëi·ªán t·ª≠ tƒÉng ƒë·ªôt bi·∫øn\n",
    "\n",
    "[[NEGATIVE]]\n",
    "(Kh√¥ng c√≥ th·ª±c th·ªÉ b·ªã ·∫£nh h∆∞·ªüng ti√™u c·ª±c r√µ r√†ng t·ª´ b√†i b√°o n√†y)\n",
    "\n",
    "(K·∫æT TH√öC V√ç D·ª§)\n",
    "\n",
    "Ng√†y ƒëƒÉng: {date}\n",
    "M√£ c·ªï phi·∫øu li√™n quan: {stockCodes}\n",
    "T·ª±a ƒë·ªÅ: {title}\n",
    "\n",
    "M√¥ t·∫£: {description}\n",
    "\n",
    "Danh s√°ch th·ª±c th·ªÉ s·∫Ω b·ªã ·∫£nh h∆∞·ªüng:\n",
    "\"\"\")\n",
    "\n",
    "# Template tr√≠ch xu·∫•t m·ªëi quan h·ªá (relation extraction)\n",
    "relation_extraction_template = PromptTemplate.from_template(\"\"\"B·∫°n ƒëang l√†m vi·ªác d∆∞·ªõi b·ªëi c·∫£nh ph√¢n t√≠ch kinh t·∫ø.                                                            \n",
    "H·∫°n ch·∫ø t·∫°o m·ªõi m·ªôt th·ª±c th·ªÉ, ch·ªâ ƒë∆∞·ª£c t·∫°o m·ªõi t·ªëi ƒëa 2 th·ª±c th·ªÉ m·ªõi. Ch·ªâ ƒë∆∞·ª£c li√™n k·∫øt t·ªõi 4 th·ª±c th·ªÉ kh√°c. Lu√¥n ∆∞u ti√™n li√™n k·∫øt v·ªõi c√°c th·ª±c th·ªÉ ƒë√£ c√≥: {existing_entities}\n",
    "\n",
    "D·ª±a tr√™n t√°c ƒë·ªông ƒë·∫øn m·ªôt th·ª±c th·ªÉ, h√£y li·ªát k√™ c√°c th·ª±c th·ªÉ s·∫Ω b·ªã ·∫£nh h∆∞·ªüng ti√™u c·ª±c v√† ·∫£nh h∆∞·ªüng t√≠ch c·ª±c do hi·ªáu ·ª©ng d√¢y chuy·ªÅn.\n",
    "H√£y suy lu·∫≠n xem th·ª±c th·ªÉ hi·ªán t·∫°i n√†y c√≥ th·ªÉ ·∫£nh h∆∞·ªüng ti·∫øp ƒë·∫øn nh·ªØng th·ª±c th·ªÉ kh√°c n√†o, theo h∆∞·ªõng t√≠ch c·ª±c ho·∫∑c ti√™u c·ª±c.\n",
    "                                                            \n",
    "V·ªõi m·ªói th·ª±c th·ªÉ, ·ªü ph·∫ßn T√™n th·ª±c th·ªÉ, h·∫°n ch·∫ø d√πng d·∫•u ch·∫•m, g·∫°ch ngang, d·∫•u v√† &, d·∫•u ch·∫•m ph·∫©y ;. C·∫ßn ghi th√™m qu·ªëc gia, ƒë·ªãa ph∆∞∆°ng c·ª• th·ªÉ v√† ng√†nh ngh·ªÅ c·ªßa n√≥ (n·∫øu c√≥). \n",
    "T√™n ch·ªâ n√≥i t·ªõi m·ªôt th·ª±c th·ªÉ duy nh·∫•t. Ph·∫ßn T√™n kh√¥ng ƒë∆∞·ª£c qu√° ph·ª©c t·∫°p, ƒë∆°n gi·∫£n nh·∫•t c√≥ th·ªÉ.\n",
    "N·∫øu th·ª±c th·ªÉ n√†o thu·ªôc danh m·ª•c c·ªï phi·∫øu sau: {portfolio}, h√£y ghi r√µ t√™n c·ªï phi·∫øu.\n",
    "V√≠ d·ª•: SSI-Ch·ª©ng kho√°n; Ng√†nh c√¥ng nghi·ªáp Vi·ªát Nam; Ng∆∞·ªùi d√πng M·ªπ; Ng√†nh th√©p Ch√¢u √Å; Ng√†nh du l·ªãch H·∫° Long, ...\n",
    "\n",
    "Ghi nh·ªõ, H·∫°n ch·∫ø t·∫°o m·ªõi th·ª±c th·ªÉ, ch·ªâ ƒë∆∞·ª£c t·∫°o m·ªõi t·ªëi ƒëa 2 th·ª±c th·ªÉ m·ªõi. Ch·ªâ ƒë∆∞·ª£c li√™n k·∫øt t·ªõi 4 th·ª±c th·ªÉ kh√°c. Lu√¥n c·ªë li√™n k·∫øt v·ªõi c√°c th·ª±c th·ªÉ ƒë√£ c√≥.\n",
    "\n",
    "Ph·∫ßn gi·∫£i th√≠ch m·ªói th·ª±c th·ªÉ, b·∫Øt bu·ªôc ƒë√°nh gi√° s·ªë li·ªáu ƒë∆∞·ª£c ghi, nhi·ªÅu ho·∫∑c √≠t, tƒÉng ho·∫∑c gi·∫£m, g·∫•p bao nhi√™u l·∫ßn, ...\n",
    "C·∫ßn c·ªë g·∫Øng li√™n k·∫øt v·ªõi nhi·ªÅu th·ª±c th·ªÉ kh√°c. Tuy nhi√™n kh√¥ng suy ngo√†i ph·∫°m vi b√†i b√°o. Kh√¥ng t·ª± ch√®n s·ªë li·ªáu ngo√†i b√†i b√°o.\n",
    "Kh√¥ng d√πng d·∫•u hai ch·∫•m trong ph·∫ßn gi·∫£i th√≠ch, ch·ªâ d√πng hai ch·∫•m : ƒë·ªÉ t√°ch gi·ªØa T√™n th·ª±c th·ªÉ v√† ph·∫ßn gi·∫£i th√≠ch.\n",
    "\n",
    "ƒê∆∞a ra theo ƒë·ªãnh d·∫°ng sau:\n",
    "[[POSITIVE]]\n",
    "[Entity 1]: [Explanation]\n",
    "...\n",
    "[Entity N]: [Explanation]\n",
    "\n",
    "[[NEGATIVE]]\n",
    "[Entity A]: [Explanation]\n",
    "..\n",
    "[Entity Z]: [Explanation]\n",
    "\n",
    "(B·∫ÆT ƒê·∫¶U V√ç D·ª§)\n",
    "\n",
    "Th·ª±c th·ªÉ g·ªëc: B·ªô X√¢y d·ª±ng Vi·ªát Nam\n",
    "\n",
    "·∫¢nh h∆∞·ªüng: √Åp l·ª±c qu·∫£n l√Ω 28 d·ª± √°n v·ªõi t·ªïng chi·ªÅu d√†i 1188 km, nh·∫±m hi·ªán th·ª±c h√≥a m·ª•c ti√™u ƒë·∫°t 3000 km cao t·ªëc v√†o nƒÉm 2025. S·ªë l∆∞·ª£ng d·ª± √°n tƒÉng g·∫•p nhi·ªÅu l·∫ßn so v·ªõi giai ƒëo·∫°n tr∆∞·ªõc, ƒë√≤i h·ªèi ƒëi·ªÅu ph·ªëi ngu·ªìn l·ª±c v√† ki·ªÉm so√°t ti·∫øn ƒë·ªô ch·∫∑t ch·∫Ω h∆°n.\n",
    "\n",
    "Danh s√°ch th·ª±c th·ªÉ s·∫Ω b·ªã ·∫£nh h∆∞·ªüng b·ªüi hi·ªáu ·ª©ng d√¢y chuy·ªÅn:\n",
    "\n",
    "[[POSITIVE]]\n",
    "Doanh nghi·ªáp x√¢y d·ª±ng Vi·ªát Nam: C√≥ c∆° h·ªôi m·ªü r·ªông h·ª£p ƒë·ªìng thi c√¥ng, tƒÉng doanh thu nh·ªù s·ªë l∆∞·ª£ng d·ª± √°n cao t·ªëc l·ªõn ƒëang tri·ªÉn khai ƒë·ªìng lo·∫°t.\n",
    "Ng∆∞·ªùi lao ƒë·ªông Vi·ªát Nam: C√≥ th√™m nhi·ªÅu c∆° h·ªôi vi·ªác l√†m t·ª´ c√°c d·ª± √°n thi c√¥ng tr·∫£i d√†i kh·∫Øp c·∫£ n∆∞·ªõc.\n",
    "\n",
    "[[NEGATIVE]]\n",
    "B·ªô Giao th√¥ng V·∫≠n t·∫£i Vi·ªát Nam: Ch·ªãu √°p l·ª±c ph·ªëi h·ª£p v√† gi√°m s√°t hi·ªáu qu·∫£ gi·ªØa c√°c b√™n li√™n quan, c√≥ nguy c∆° b·ªã ch·ªâ tr√≠ch n·∫øu d·ª± √°n ch·∫≠m ti·∫øn ƒë·ªô.\n",
    "Doanh nghi·ªáp x√¢y d·ª±ng Vi·ªát Nam: C√≥ th·ªÉ ch·ªãu √°p l·ª±c tƒÉng gi√° nguy√™n v·∫≠t li·ªáu v√† thi·∫øu h·ª•t ngu·ªìn cung do nhu c·∫ßu tƒÉng ƒë·ªôt bi·∫øn.\n",
    "\n",
    "(K·∫æT TH√öC V√ç D·ª§)\n",
    "\n",
    "Th·ª±c th·ªÉ g·ªëc: {entities}\n",
    "\n",
    "·∫¢nh h∆∞·ªüng: {description}\n",
    "\n",
    "Danh s√°ch th·ª±c th·ªÉ s·∫Ω b·ªã ·∫£nh h∆∞·ªüng b·ªüi hi·ªáu ·ª©ng d√¢y chuy·ªÅn:\n",
    "\"\"\")\n",
    "\n",
    "# Template batch relation extraction (x·ª≠ l√Ω nhi·ªÅu th·ª±c th·ªÉ c√πng l√∫c)\n",
    "batch_relation_extraction_template = PromptTemplate.from_template(\"\"\"B·∫°n ƒëang l√†m vi·ªác d∆∞·ªõi b·ªëi c·∫£nh ph√¢n t√≠ch kinh t·∫ø.\n",
    "H·∫°n ch·∫ø t·∫°o m·ªõi th·ª±c th·ªÉ, ch·ªâ ƒë∆∞·ª£c t·∫°o m·ªõi t·ªëi ƒëa 2 th·ª±c th·ªÉ m·ªõi cho m·ªói th·ª±c th·ªÉ g·ªëc. Ch·ªâ ƒë∆∞·ª£c li√™n k·∫øt t·ªëi ƒëa 3 th·ª±c th·ªÉ kh√°c cho m·ªói th·ª±c th·ªÉ g·ªëc. Lu√¥n ∆∞u ti√™n li√™n k·∫øt v·ªõi c√°c th·ª±c th·ªÉ ƒë√£ c√≥: {existing_entities}\n",
    "\n",
    "D·ª±a tr√™n t√°c ƒë·ªông ƒë·∫øn c√°c th·ª±c th·ªÉ ƒë·∫ßu v√†o, h√£y ph√¢n t√≠ch hi·ªáu ·ª©ng d√¢y chuy·ªÅn. \n",
    "H√£y suy lu·∫≠n xem m·ªói th·ª±c th·ªÉ hi·ªán t·∫°i c√≥ th·ªÉ ·∫£nh h∆∞·ªüng ti·∫øp ƒë·∫øn nh·ªØng th·ª±c th·ªÉ kh√°c n√†o, theo h∆∞·ªõng t√≠ch c·ª±c ho·∫∑c ti√™u c·ª±c.\n",
    "\n",
    "V·ªõi m·ªói th·ª±c th·ªÉ, ·ªü ph·∫ßn T√™n th·ª±c th·ªÉ, h·∫°n ch·∫ø d√πng d·∫•u ch·∫•m, g·∫°ch ngang, d·∫•u v√† &, d·∫•u ch·∫•m ph·∫©y ;. C·∫ßn ghi th√™m qu·ªëc gia, ƒë·ªãa ph∆∞∆°ng c·ª• th·ªÉ v√† ng√†nh ngh·ªÅ c·ªßa n√≥ (n·∫øu c√≥).\n",
    "T√™n ch·ªâ n√≥i t·ªõi m·ªôt th·ª±c th·ªÉ duy nh·∫•t. Ph·∫ßn T√™n kh√¥ng ƒë∆∞·ª£c qu√° ph·ª©c t·∫°p, ƒë∆°n gi·∫£n nh·∫•t c√≥ th·ªÉ.\n",
    "N·∫øu th·ª±c th·ªÉ n√†o thu·ªôc danh m·ª•c c·ªï phi·∫øu sau: {portfolio}, h√£y ghi r√µ t√™n c·ªï phi·∫øu.\n",
    "V√≠ d·ª•: SSI-Ch·ª©ng kho√°n; Ng√†nh c√¥ng nghi·ªáp Vi·ªát Nam; Ng∆∞·ªùi d√πng M·ªπ; Ng√†nh th√©p Ch√¢u √Å; Ng√†nh du l·ªãch H·∫° Long, ...\n",
    "\n",
    "Ph·∫ßn gi·∫£i th√≠ch m·ªói th·ª±c th·ªÉ, b·∫Øt bu·ªôc ƒë√°nh gi√° s·ªë li·ªáu ƒë∆∞·ª£c ghi, nhi·ªÅu ho·∫∑c √≠t, tƒÉng ho·∫∑c gi·∫£m, g·∫•p bao nhi√™u l·∫ßn...\n",
    "C·∫ßn c·ªë g·∫Øng li√™n k·∫øt v·ªõi nhi·ªÅu th·ª±c th·ªÉ kh√°c. Tuy nhi√™n kh√¥ng suy ngo√†i ph·∫°m vi b√†i b√°o. Kh√¥ng t·ª± ch√®n s·ªë li·ªáu ngo√†i b√†i b√°o.\n",
    "Kh√¥ng d√πng d·∫•u hai ch·∫•m trong ph·∫ßn gi·∫£i th√≠ch, ch·ªâ d√πng hai ch·∫•m : ƒë·ªÉ t√°ch gi·ªØa T√™n th·ª±c th·ªÉ v√† ph·∫ßn gi·∫£i th√≠ch.\n",
    "\n",
    "ƒê∆∞a ra theo ƒë·ªãnh d·∫°ng sau cho m·ªói th·ª±c th·ªÉ ngu·ªìn:\n",
    "\n",
    "[[SOURCE: T√™n th·ª±c th·ªÉ ngu·ªìn]]\n",
    "[[IMPACT: POSITIVE/NEGATIVE]]\n",
    "\n",
    "[[POSITIVE]]\n",
    "[Th·ª±c th·ªÉ ·∫£nh h∆∞·ªüng 1]: [Gi·∫£i th√≠ch]\n",
    "[Th·ª±c th·ªÉ ·∫£nh h∆∞·ªüng 2]: [Gi·∫£i th√≠ch]\n",
    "[Th·ª±c th·ªÉ ·∫£nh h∆∞·ªüng 3]: [Gi·∫£i th√≠ch]\n",
    "\n",
    "[[NEGATIVE]]\n",
    "[Th·ª±c th·ªÉ ·∫£nh h∆∞·ªüng A]: [Gi·∫£i th√≠ch]\n",
    "[Th·ª±c th·ªÉ ·∫£nh h∆∞·ªüng B]: [Gi·∫£i th√≠ch]\n",
    "[Th·ª±c th·ªÉ ·∫£nh h∆∞·ªüng C]: [Gi·∫£i th√≠ch]\n",
    "\n",
    "\n",
    "L∆ØU √ù [R·∫§T QUAN TR·ªåNG]:\n",
    "   - C√≥ th·ªÉ c√≥ R·∫§T NHI·ªÄU th·ª±c th·ªÉ ƒë·∫ßu v√†o, h√£y ph√¢n t√≠ch C·∫®N TH·∫¨N t·ª´ng th·ª±c th·ªÉ ƒë·ªÉ kh√¥ng b·ªè s√≥t. Kh√¥ng ƒë∆∞·ª£c t·∫°o th√™m th·ª±c th·ªÉ g·ªëc. \n",
    "   - B·∫°n s·∫Ω ph√¢n t√≠ch nhi·ªÅu th·ª±c th·ªÉ g·ªëc m·ªôt l√∫c. V·ªõi T·ª™NG th·ª±c th·ªÉ, ch·ªâ ch·ªçn CH√çNH X√ÅC 2-3 th·ª±c th·ªÉ ·∫£nh h∆∞·ªüng t√≠ch c·ª±c nh·∫•t v√† 2-3 th·ª±c th·ªÉ ·∫£nh h∆∞·ªüng ti√™u c·ª±c quan tr·ªçng nh·∫•t.\n",
    "                                                                  \n",
    "(B·∫ÆT ƒê·∫¶U V√ç D·ª§)\n",
    "Danh s√°ch th·ª±c th·ªÉ ngu·ªìn:\n",
    "\n",
    "Th·ª±c th·ªÉ g·ªëc: B·ªô X√¢y d·ª±ng Vi·ªát Nam\n",
    "\n",
    "·∫¢nh h∆∞·ªüng: NEGATIVE, √Åp l·ª±c qu·∫£n l√Ω 28 d·ª± √°n v·ªõi t·ªïng chi·ªÅu d√†i 1188 km, nh·∫±m hi·ªán th·ª±c h√≥a m·ª•c ti√™u ƒë·∫°t 3000 km cao t·ªëc v√†o nƒÉm 2025. S·ªë l∆∞·ª£ng d·ª± √°n tƒÉng g·∫•p nhi·ªÅu l·∫ßn so v·ªõi giai ƒëo·∫°n tr∆∞·ªõc, ƒë√≤i h·ªèi ƒëi·ªÅu ph·ªëi ngu·ªìn l·ª±c v√† ki·ªÉm so√°t ti·∫øn ƒë·ªô ch·∫∑t ch·∫Ω h∆°n.\n",
    "\n",
    "---\n",
    "\n",
    "Danh s√°ch th·ª±c th·ªÉ s·∫Ω b·ªã ·∫£nh h∆∞·ªüng b·ªüi hi·ªáu ·ª©ng d√¢y chuy·ªÅn:\n",
    "\n",
    "[[SOURCE: B·ªô X√¢y d·ª±ng Vi·ªát Nam]]\n",
    "[[IMPACT: NEGATIVE]]\n",
    "\n",
    "[[POSITIVE]]\n",
    "Doanh nghi·ªáp x√¢y d·ª±ng Vi·ªát Nam: C√≥ c∆° h·ªôi m·ªü r·ªông h·ª£p ƒë·ªìng thi c√¥ng, tƒÉng doanh thu nh·ªù s·ªë l∆∞·ª£ng d·ª± √°n cao t·ªëc l·ªõn ƒëang tri·ªÉn khai ƒë·ªìng lo·∫°t.\n",
    "Ng∆∞·ªùi lao ƒë·ªông Vi·ªát Nam: C√≥ th√™m nhi·ªÅu c∆° h·ªôi vi·ªác l√†m t·ª´ c√°c d·ª± √°n thi c√¥ng tr·∫£i d√†i kh·∫Øp c·∫£ n∆∞·ªõc.\n",
    "\n",
    "[[NEGATIVE]]\n",
    "B·ªô Giao th√¥ng V·∫≠n t·∫£i Vi·ªát Nam: Ch·ªãu √°p l·ª±c ph·ªëi h·ª£p v√† gi√°m s√°t hi·ªáu qu·∫£ gi·ªØa c√°c b√™n li√™n quan, c√≥ nguy c∆° b·ªã ch·ªâ tr√≠ch n·∫øu d·ª± √°n ch·∫≠m ti·∫øn ƒë·ªô.\n",
    "Doanh nghi·ªáp x√¢y d·ª±ng Vi·ªát Nam: C√≥ th·ªÉ ch·ªãu √°p l·ª±c tƒÉng gi√° nguy√™n v·∫≠t li·ªáu v√† thi·∫øu h·ª•t ngu·ªìn cung do nhu c·∫ßu tƒÉng ƒë·ªôt bi·∫øn.\n",
    "\n",
    "(K·∫æT TH√öC V√ç D·ª§)\n",
    "\n",
    "Danh s√°ch th·ª±c th·ªÉ ngu·ªìn:\n",
    "\n",
    "{input_entities}\n",
    "\n",
    "Danh s√°ch th·ª±c th·ªÉ s·∫Ω b·ªã ·∫£nh h∆∞·ªüng b·ªüi hi·ªáu ·ª©ng d√¢y chuy·ªÅn:\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úì ƒê√£ ƒë·ªãnh nghƒ©a 3 prompt templates: entity_extraction, relation_extraction, batch_relation_extraction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f0bcc5e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì ƒê√£ ƒë·ªãnh nghƒ©a portfolio v√† constants\n"
     ]
    }
   ],
   "source": [
    "PORTFOLIO_STOCKS = [\"FPT\", \"SSI\", \"VCB\", \"VHM\", \"HPG\", \"GAS\", \"MSN\", \"MWG\", \"GVR\", \"VIC\"]\n",
    "PORTFOLIO_SECTOR = [\"C√¥ng ngh·ªá\", \"Ch·ª©ng kho√°n\", \"Ng√¢n h√†ng\", \"B·∫•t ƒë·ªông s·∫£n\", \"V·∫≠t li·ªáu c∆° b·∫£n\", \n",
    "                     \"D·ªãch v·ª• H·∫° t·∫ßng\", \"Ti√™u d√πng c∆° b·∫£n\", \"B√°n l·∫ª\", \"Ch·∫ø bi·∫øn\", \"B·∫•t ƒë·ªông s·∫£n\"]\n",
    "BASE_DELAY = 30\n",
    "MAX_RETRIES = 3\n",
    "\n",
    "def create_chains(api_manager):\n",
    "    \"\"\"\n",
    "    T·∫°o chains v·ªõi models t·ª´ APIKeyManager\n",
    "    \"\"\"\n",
    "    model, model_more_temp, model_pro = api_manager.get_models()\n",
    "    \n",
    "    # T·∫°o 2 chains\n",
    "    chain_entity = entity_extraction_template | model\n",
    "    chain_batch_relation = batch_relation_extraction_template | model\n",
    "    \n",
    "    return chain_entity, chain_batch_relation\n",
    "\n",
    "print(\"‚úì ƒê√£ ƒë·ªãnh nghƒ©a portfolio v√† constants\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c320c01e",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Utility Functions\n",
    "\n",
    "C√°c h√†m h·ªó tr·ª£:\n",
    "1. `invoke_chain_with_retry()` - G·ªçi API v·ªõi retry t·ª± ƒë·ªông\n",
    "2. `parse_entity_response()` - Parse response t·ª´ LLM\n",
    "3. `merge_entity()` - Chu·∫©n h√≥a t√™n entity\n",
    "4. `graph_entities_to_str()` - Convert graph entities th√†nh string cho prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "65b81888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì ƒê√£ ƒë·ªãnh nghƒ©a c√°c h√†m ti·ªán √≠ch\n"
     ]
    }
   ],
   "source": [
    "def invoke_chain_with_retry(chain, prompt, api_manager, base_delay=BASE_DELAY):\n",
    "    \"\"\"\n",
    "    G·ªçi chain v·ªõi c∆° ch·∫ø retry t·ª± ƒë·ªông v√† t√≠ch h·ª£p APIKeyManager\n",
    "    \"\"\"\n",
    "    total_attempts = 0\n",
    "    max_total_attempts = len(api_manager.keys) * api_manager.MAX_RETRIES_PER_KEY\n",
    "    \n",
    "    while total_attempts < max_total_attempts:\n",
    "        try:\n",
    "            # Th·ª≠ g·ªçi API\n",
    "            response = chain.invoke(prompt)\n",
    "            \n",
    "            # Th√†nh c√¥ng -> reset error count\n",
    "            api_manager.reset_errors()\n",
    "            return response\n",
    "            \n",
    "        except Exception as e:\n",
    "            total_attempts += 1\n",
    "            error_msg = str(e)\n",
    "            \n",
    "            # B√°o l·ªói cho API manager\n",
    "            switched = api_manager.on_error()\n",
    "            \n",
    "            if total_attempts >= max_total_attempts:\n",
    "                print(f\"‚ùå ƒê√£ th·ª≠ t·∫•t c·∫£ {len(api_manager.keys)} API keys ({total_attempts} l·∫ßn) nh∆∞ng v·∫´n l·ªói\")\n",
    "                print(f\"   L·ªói cu·ªëi: {error_msg}\")\n",
    "                return None\n",
    "            \n",
    "            # Ch·ªù tr∆∞·ªõc khi retry\n",
    "            if switched:\n",
    "                delay = base_delay\n",
    "                print(f\"‚è≥ ƒê·ª£i {delay}s tr∆∞·ªõc khi th·ª≠ key m·ªõi...\")\n",
    "            else:\n",
    "                key_name = api_manager.keys[api_manager.current_index][0]\n",
    "                retry_num = api_manager.error_counts.get(key_name, 0)\n",
    "                delay = base_delay * (1.5 ** (retry_num - 1))\n",
    "                print(f\"‚è≥ ƒê·ª£i {delay:.0f}s tr∆∞·ªõc khi retry ({retry_num}/{api_manager.MAX_RETRIES_PER_KEY})...\")\n",
    "            \n",
    "            time.sleep(delay)\n",
    "\n",
    "def parse_entity_response(response):\n",
    "    \"\"\"\n",
    "    Ph√¢n t√≠ch response t·ª´ entity extraction prompt\n",
    "    \n",
    "    Returns:\n",
    "        dict: {\"POSITIVE\": [(entity, explanation), ...], \"NEGATIVE\": [(entity, explanation), ...]}\n",
    "    \"\"\"\n",
    "    if response is None:\n",
    "        print(\"Response is None\")\n",
    "        return {\"POSITIVE\": [], \"NEGATIVE\": []}\n",
    "        \n",
    "    sections = {\"POSITIVE\": [], \"NEGATIVE\": []}\n",
    "    current_section = None\n",
    "    str_resp = response.content\n",
    "    \n",
    "    for line in str(str_resp).splitlines():\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        if \"[[POSITIVE]]\" in line.upper():\n",
    "            current_section = \"POSITIVE\"\n",
    "            continue\n",
    "        if \"[[NEGATIVE]]\" in line.upper():\n",
    "            current_section = \"NEGATIVE\"\n",
    "            continue\n",
    "        if current_section and ':' in line:\n",
    "            entity = line.split(\":\", 1)[0].strip()\n",
    "            # Skip invalid entities\n",
    "            if not entity or \"kh√¥ng c√≥ th·ª±c th·ªÉ n√†o\" in entity.lower():\n",
    "                continue\n",
    "            # content = all line except entity\n",
    "            content = line.split(entity, 1)[-1].strip(':').strip()\n",
    "            sections[current_section].append((entity, content))\n",
    "\n",
    "    return sections\n",
    "\n",
    "def parse_batch_entity_response(response):\n",
    "    \"\"\"\n",
    "    Ph√¢n t√≠ch response t·ª´ batch relation extraction prompt\n",
    "    \n",
    "    Returns:\n",
    "        list: [(source_entity, impact, target_entity, content), ...]\n",
    "    \"\"\"\n",
    "    if response is None:\n",
    "        print(\"Response is None\")\n",
    "        return []\n",
    "        \n",
    "    results = []\n",
    "    current_source = None\n",
    "    current_impact = None\n",
    "    current_section = None\n",
    "    \n",
    "    str_resp = str(response.content)\n",
    "    lines = str_resp.splitlines()\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "            \n",
    "        # Check for source entity marker\n",
    "        if line.startswith(\"[[SOURCE:\") or \"[[SOURCE:\" in line:\n",
    "            source_text = line.replace(\"[[SOURCE:\", \"\").replace(\"]]\", \"\").strip()\n",
    "            if source_text and \"kh√¥ng c√≥ th·ª±c th·ªÉ n√†o\" not in source_text.lower():\n",
    "                current_source = source_text\n",
    "            else:\n",
    "                current_source = None\n",
    "            continue\n",
    "            \n",
    "        # Check for impact marker\n",
    "        if line.startswith(\"[[IMPACT:\") or \"[[IMPACT:\" in line:\n",
    "            impact_str = line.replace(\"[[IMPACT:\", \"\").replace(\"]]\", \"\").strip()\n",
    "            current_impact = impact_str.upper()\n",
    "            continue\n",
    "            \n",
    "        # Check for positive/negative section markers\n",
    "        if \"[[POSITIVE]]\" in line.upper():\n",
    "            current_section = \"POSITIVE\"\n",
    "            continue\n",
    "            \n",
    "        if \"[[NEGATIVE]]\" in line.upper():\n",
    "            current_section = \"NEGATIVE\"\n",
    "            continue\n",
    "            \n",
    "        # Process entity and explanation\n",
    "        if current_source and current_section and ':' in line:\n",
    "            try:\n",
    "                entity, *content_parts = line.split(\":\", 1)\n",
    "                entity = entity.strip().strip('[]')\n",
    "                \n",
    "                if not entity or \"kh√¥ng c√≥ th·ª±c th·ªÉ n√†o\" in entity.lower():\n",
    "                    continue\n",
    "                    \n",
    "                if entity and content_parts:\n",
    "                    content = content_parts[0].strip()\n",
    "                    actual_impact = current_impact if current_impact else current_section\n",
    "                    results.append((current_source, actual_impact, entity, content))\n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing line: {line}. Error: {e}\")\n",
    "                continue\n",
    "    \n",
    "    if not results:\n",
    "        print(\"Warning: No relationships were parsed from the response\")\n",
    "        \n",
    "    return results\n",
    "\n",
    "def merge_entity(entity, canonical_set):\n",
    "    \"\"\"\n",
    "    Tr·∫£ v·ªÅ phi√™n b·∫£n canonical c·ªßa entity n·∫øu ƒë√£ t·ªìn t·∫°i (case-insensitive),\n",
    "    n·∫øu kh√¥ng th√¨ th√™m v√† tr·∫£ v·ªÅ entity m·ªõi.\n",
    "    \n",
    "    FIX: ƒê·∫£m b·∫£o t√™n entity ƒë∆∞·ª£c l∆∞u v√† tr·∫£ v·ªÅ nh·∫•t qu√°n (gi·ªØ nguy√™n vi·∫øt hoa/th∆∞·ªùng c·ªßa l·∫ßn ƒë·∫ßu)\n",
    "    \"\"\"\n",
    "    # Chu·∫©n h√≥a: b·ªè d·∫•u ngo·∫∑c vu√¥ng, kho·∫£ng tr·∫Øng th·ª´a\n",
    "    cleaned_entity = str(entity).strip('[').strip(']').strip()\n",
    "    normalized_lower = cleaned_entity.lower()\n",
    "    \n",
    "    # T√¨m entity ƒë√£ t·ªìn t·∫°i (case-insensitive)\n",
    "    for existing_entity in canonical_set:\n",
    "        if existing_entity.lower() == normalized_lower:\n",
    "            # Tr·∫£ v·ªÅ entity ƒë√£ t·ªìn t·∫°i (gi·ªØ nguy√™n vi·∫øt hoa/th∆∞·ªùng c·ªßa l·∫ßn ƒë·∫ßu)\n",
    "            return existing_entity\n",
    "    \n",
    "    # Ch∆∞a t·ªìn t·∫°i ‚Üí th√™m entity m·ªõi (GI·ªÆ NGUY√äN vi·∫øt hoa/th∆∞·ªùng g·ªëc)\n",
    "    canonical_set.add(cleaned_entity)\n",
    "    return cleaned_entity\n",
    "\n",
    "def add_edge(G, source, target, impact, timestamp):\n",
    "    \"\"\"\n",
    "    Th√™m edge v√†o graph n·∫øu ch∆∞a t·ªìn t·∫°i\n",
    "    \"\"\"\n",
    "    if not G.has_edge(source, target):\n",
    "        G.add_edge(source, target, impact=impact, timestamp=timestamp)\n",
    "\n",
    "def graph_entities_to_str(G, max_entities=50):\n",
    "    \"\"\"\n",
    "    Chuy·ªÉn ƒë·ªïi c√°c entities trong graph th√†nh chu·ªói ƒë·ªÉ ƒë∆∞a v√†o prompt\n",
    "    \"\"\"\n",
    "    entities = [node for node in G.nodes() if not node.startswith(\"Article_\")]\n",
    "    # Gi·ªõi h·∫°n s·ªë l∆∞·ª£ng ƒë·ªÉ kh√¥ng l√†m prompt qu√° d√†i\n",
    "    entities = entities[:max_entities]\n",
    "    return \", \".join(entities) if entities else \"Ch∆∞a c√≥ th·ª±c th·ªÉ n√†o\"\n",
    "\n",
    "def graph_to_tuples(G):\n",
    "    \"\"\"\n",
    "    Chuy·ªÉn ƒë·ªïi graph th√†nh list c√°c tuples (date, source, impact, target)\n",
    "    \"\"\"\n",
    "    tuples = []\n",
    "    for u, v, data in G.edges(data=True):\n",
    "        # Skip article nodes\n",
    "        if u.startswith(\"Article_\"):\n",
    "            continue\n",
    "            \n",
    "        timestamp = data.get(\"timestamp\")\n",
    "        if timestamp is None:\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            # Handle different timestamp formats\n",
    "            if isinstance(timestamp, pd.Timestamp):\n",
    "                date_str = timestamp.date().isoformat()\n",
    "            elif hasattr(timestamp, \"date\"):\n",
    "                date_str = timestamp.date().isoformat()\n",
    "            elif isinstance(timestamp, (int, float)):\n",
    "                date_str = pd.Timestamp(timestamp, unit='s').date().isoformat()\n",
    "            else:\n",
    "                parsed_date = pd.to_datetime(timestamp)\n",
    "                date_str = parsed_date.date().isoformat()\n",
    "                \n",
    "            # Skip invalid entities\n",
    "            if \"kh√¥ng c√≥ th·ª±c th·ªÉ n√†o\" in str(u).lower() or \"kh√¥ng c√≥ th·ª±c th·ªÉ n√†o\" in str(v).lower():\n",
    "                continue\n",
    "                \n",
    "            tuples.append(f\"({date_str}, {u}, {data.get('impact')} TO, {v})\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing edge ({u}, {v}): {e}\")\n",
    "            continue\n",
    "\n",
    "    return \"\\n\".join(sorted(tuples))\n",
    "\n",
    "print(\"‚úì ƒê√£ ƒë·ªãnh nghƒ©a c√°c h√†m ti·ªán √≠ch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf93f87",
   "metadata": {},
   "source": [
    "## üéØ Main Function: Extract Entities from News\n",
    "\n",
    "H√†m ch√≠nh tr√≠ch xu·∫•t entities t·ª´ CSV tin t·ª©c ƒë√£ t√≥m t·∫Øt.\n",
    "\n",
    "**Features:**\n",
    "- ‚úÖ Validation dataset tr∆∞·ªõc khi ch·∫°y\n",
    "- ‚úÖ Error handling v·ªõi th√¥ng b√°o chi ti·∫øt\n",
    "- ‚úÖ Export to tuples CSV cho ph√¢n t√≠ch\n",
    "- ‚úÖ Progress tracking v√† logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bec082aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì ƒê√£ ƒë·ªãnh nghƒ©a h√†m extract_entities_from_news() v·ªõi workflow ƒë·∫ßy ƒë·ªß nh∆∞ TRR\n"
     ]
    }
   ],
   "source": [
    "def batch_process_entity_relationships(entity_batch, G, canonical_entities, portfolio, portfolio_str_full, article_timestamp, chain_batch_relation):\n",
    "    \"\"\"\n",
    "    X·ª≠ l√Ω nhi·ªÅu entities trong m·ªôt l·∫ßn g·ªçi API (batch processing)\n",
    "    Returns: list of new entities to process\n",
    "    \"\"\"\n",
    "    if not entity_batch:\n",
    "        return []\n",
    "    \n",
    "    max_batch_retries = 2\n",
    "    batch_retry_count = 0\n",
    "    relationships = []\n",
    "    \n",
    "    while batch_retry_count < max_batch_retries:\n",
    "        # Format input entities\n",
    "        input_entities_text = \"\"\n",
    "        for entity, impact, content in entity_batch:\n",
    "            input_entities_text += f\"Th·ª±c th·ªÉ g·ªëc: {entity}\\n\\n·∫¢nh h∆∞·ªüng: {impact}, {content}\\n\\n---\\n\\n\"\n",
    "        \n",
    "        # Create prompt\n",
    "        prompt_batch = {\n",
    "            \"input_entities\": input_entities_text,\n",
    "            \"portfolio\": portfolio_str_full,\n",
    "            \"existing_entities\": graph_entities_to_str(G)\n",
    "        }\n",
    "        \n",
    "        # Get relationships\n",
    "        response = invoke_chain_with_retry(chain_batch_relation, prompt_batch, api_manager)\n",
    "        time.sleep(1)\n",
    "        \n",
    "        if response is None:\n",
    "            return []\n",
    "        \n",
    "        # Parse response\n",
    "        relationships = parse_batch_entity_response(response)\n",
    "        \n",
    "        if len(relationships) > 0:\n",
    "            break\n",
    "            \n",
    "        batch_retry_count += 1\n",
    "        print(f\"   ‚ö† Batch processing returned 0 relationships. Retry {batch_retry_count}/{max_batch_retries}\")\n",
    "        time.sleep(BASE_DELAY)\n",
    "    \n",
    "    # Process relationships to update graph\n",
    "    next_entities = []\n",
    "    \n",
    "    for source, impact, target, content in relationships:\n",
    "        if \"kh√¥ng c√≥ th·ª±c th·ªÉ n√†o\" in source.lower() or \"kh√¥ng c√≥ th·ª±c th·ªÉ n√†o\" in target.lower():\n",
    "            continue\n",
    "        \n",
    "        # FIX: Chu·∫©n h√≥a C·∫¢ source V√Ä target qua merge_entity\n",
    "        canon_source = merge_entity(source, canonical_entities)\n",
    "        canon_target = merge_entity(target, canonical_entities)\n",
    "        \n",
    "        # Determine node type for target\n",
    "        node_type = \"stock\" if any(str(canon_target).lower().find(stock.lower()) != -1 for stock in portfolio) else \"entity\"\n",
    "        \n",
    "        # Add target node if not exists\n",
    "        if not G.has_node(canon_target):\n",
    "            G.add_node(canon_target, type=node_type, timestamp=article_timestamp)\n",
    "        \n",
    "        # Ensure source node exists in graph (should be from previous iteration)\n",
    "        if not G.has_node(canon_source):\n",
    "            # Determine source node type\n",
    "            source_type = \"stock\" if any(str(canon_source).lower().find(stock.lower()) != -1 for stock in portfolio) else \"entity\"\n",
    "            G.add_node(canon_source, type=source_type, timestamp=article_timestamp)\n",
    "            \n",
    "        # Add edge\n",
    "        add_edge(G, canon_source, canon_target, impact, article_timestamp)\n",
    "        \n",
    "        # Add to frontier if entity type\n",
    "        if node_type == \"entity\":\n",
    "            next_entities.append((canon_target, impact, content))\n",
    "    \n",
    "    return next_entities\n",
    "\n",
    "def extract_entities_from_news(\n",
    "    csv_path=\"summarized_news_with_stocks_merged.csv\",\n",
    "    output_csv=\"entities_extracted.csv\",\n",
    "    output_graph_pkl=\"knowledge_graph.pkl\",\n",
    "    output_canonical_pkl=\"canonical_entities.pkl\",\n",
    "    output_tuples_csv=\"graph_tuples_step1.csv\",\n",
    "    start_date=None,\n",
    "    end_date=None,\n",
    "    max_articles=None,\n",
    "    max_iterations=2,\n",
    "    batch_size=5\n",
    "):\n",
    "    \"\"\"\n",
    "    Tr√≠ch xu·∫•t th·ª±c th·ªÉ t·ª´ tin t·ª©c ƒë√£ t√≥m t·∫Øt v·ªõi workflow ƒë·∫ßy ƒë·ªß nh∆∞ TRR_model\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    csv_path : str\n",
    "        ƒê∆∞·ªùng d·∫´n ƒë·∫øn file CSV ch·ª©a tin t·ª©c\n",
    "    output_csv : str\n",
    "        File CSV k·∫øt qu·∫£ ch·ª©a entities\n",
    "    output_graph_pkl : str\n",
    "        File pickle ch·ª©a NetworkX graph\n",
    "    output_canonical_pkl : str\n",
    "        File pickle ch·ª©a canonical entities set\n",
    "    output_tuples_csv : str\n",
    "        File CSV ch·ª©a graph tuples (date, source, impact, target)\n",
    "    start_date : str\n",
    "        Ng√†y b·∫Øt ƒë·∫ßu (YYYY-MM-DD)\n",
    "    end_date : str\n",
    "        Ng√†y k·∫øt th√∫c (YYYY-MM-DD)\n",
    "    max_articles : int\n",
    "        S·ªë l∆∞·ª£ng b√†i b√°o t·ªëi ƒëa\n",
    "    max_iterations : int\n",
    "        S·ªë v√≤ng l·∫∑p t·ªëi ƒëa cho relation extraction\n",
    "    batch_size : int\n",
    "        S·ªë entities x·ª≠ l√Ω trong m·ªôt batch\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple: (entities_df, G, canonical_entities)\n",
    "    \"\"\"\n",
    "    print(f\"üìñ ƒêang ƒë·ªçc d·ªØ li·ªáu t·ª´ {csv_path}...\")\n",
    "    \n",
    "    # ƒê·ªçc d·ªØ li·ªáu\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        print(f\"‚úì ƒê√£ ƒë·ªçc {len(df)} tin t·ª©c\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå Kh√¥ng t√¨m th·∫•y file: {csv_path}\")\n",
    "        raise\n",
    "    \n",
    "    # Validate columns\n",
    "    required_cols = ['postID', 'stockCodes', 'title', 'description', 'date']\n",
    "    missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        print(f\"‚ùå Dataset thi·∫øu c√°c c·ªôt: {missing_cols}\")\n",
    "        print(f\"üìã C√°c c·ªôt hi·ªán c√≥: {list(df.columns)}\")\n",
    "        raise ValueError(f\"Dataset kh√¥ng h·ª£p l·ªá: thi·∫øu c·ªôt {missing_cols}\")\n",
    "    \n",
    "    print(f\"‚úì Dataset h·ª£p l·ªá v·ªõi c√°c c·ªôt: {list(df.columns)}\")\n",
    "    \n",
    "    # Chuy·ªÉn ƒë·ªïi date\n",
    "    df['parsed_date'] = pd.to_datetime(df['date'])\n",
    "    df['only_date'] = df['parsed_date'].dt.date\n",
    "    \n",
    "    # Filter by date range\n",
    "    if start_date:\n",
    "        start_dt = pd.to_datetime(start_date).date()\n",
    "        df = df[df['only_date'] >= start_dt]\n",
    "        print(f\"‚úì L·ªçc t·ª´ ng√†y {start_date}: c√≤n {len(df)} tin\")\n",
    "    \n",
    "    if end_date:\n",
    "        end_dt = pd.to_datetime(end_date).date()\n",
    "        df = df[df['only_date'] <= end_dt]\n",
    "        print(f\"‚úì L·ªçc ƒë·∫øn ng√†y {end_date}: c√≤n {len(df)} tin\")\n",
    "    \n",
    "    # Limit articles\n",
    "    if max_articles:\n",
    "        df = df.head(max_articles)\n",
    "        print(f\"‚úì Gi·ªõi h·∫°n xu·ªëng {len(df)} tin ƒë·ªÉ x·ª≠ l√Ω\")\n",
    "    \n",
    "    # Sort by date\n",
    "    df = df.sort_values('date')\n",
    "    \n",
    "    # Initialize graph and canonical entities\n",
    "    G = nx.DiGraph()\n",
    "    canonical_entities = set()\n",
    "    \n",
    "    # Build portfolio string\n",
    "    portfolio_str_full = \", \".join([f\"{stock}-{sector}\" for stock, sector in zip(PORTFOLIO_STOCKS, PORTFOLIO_SECTOR)])\n",
    "    \n",
    "    # Create chains\n",
    "    chain_entity, chain_batch_relation = create_chains(api_manager)\n",
    "    \n",
    "    # Results\n",
    "    all_entities = []\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üîç B·∫ÆT ƒê·∫¶U TR√çCH XU·∫§T TH·ª∞C TH·ªÇ V√Ä M·ªêI QUAN H·ªÜ\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Process each article\n",
    "    for idx, row in df.iterrows():\n",
    "        article_idx = idx + 1\n",
    "        article_node = f\"Article_{article_idx}: {row['title']}\"\n",
    "        article_timestamp = row['parsed_date']\n",
    "        \n",
    "        # Add article node\n",
    "        if not G.has_node(article_node):\n",
    "            G.add_node(article_node, type=\"article\", timestamp=article_timestamp)\n",
    "        \n",
    "        print(f\"\\n[{article_idx}/{len(df)}] üì∞ {row['title'][:60]}...\")\n",
    "        \n",
    "        # Get stock codes\n",
    "        stock_codes = row.get('stockCodes', '')\n",
    "        if not stock_codes or pd.isna(stock_codes):\n",
    "            stock_codes = \"(kh√¥ng c√≥)\"\n",
    "        \n",
    "        # Phase 1: Extract initial entities\n",
    "        print(f\"   üìå Phase 1: Tr√≠ch xu·∫•t entities ban ƒë·∫ßu...\")\n",
    "        max_entity_retries = MAX_RETRIES\n",
    "        entity_retry_count = 0\n",
    "        entities_dict = {\"POSITIVE\": [], \"NEGATIVE\": []}\n",
    "        \n",
    "        while entity_retry_count < max_entity_retries:\n",
    "            prompt_text = {\n",
    "                \"portfolio\": portfolio_str_full,\n",
    "                \"date\": row['date'],\n",
    "                \"stockCodes\": stock_codes,\n",
    "                \"title\": row['title'],\n",
    "                \"description\": row['description'],\n",
    "                \"existing_entities\": graph_entities_to_str(G)\n",
    "            }\n",
    "            \n",
    "            response_text = invoke_chain_with_retry(chain_entity, prompt_text, api_manager)\n",
    "            time.sleep(1)\n",
    "            \n",
    "            if response_text is None:\n",
    "                print(f\"   ‚ùå B·ªè qua tin {article_idx} do l·ªói API\")\n",
    "                break\n",
    "            \n",
    "            entities_dict = parse_entity_response(response_text)\n",
    "            \n",
    "            total_entities = len(entities_dict.get(\"POSITIVE\", [])) + len(entities_dict.get(\"NEGATIVE\", []))\n",
    "            if total_entities > 0:\n",
    "                print(f\"   ‚úì Tr√≠ch xu·∫•t ƒë∆∞·ª£c {total_entities} th·ª±c th·ªÉ ban ƒë·∫ßu\")\n",
    "                break\n",
    "                \n",
    "            entity_retry_count += 1\n",
    "            print(f\"   ‚ö† Kh√¥ng c√≥ th·ª±c th·ªÉ. Th·ª≠ l·∫°i {entity_retry_count}/{max_entity_retries}\")\n",
    "            time.sleep(BASE_DELAY)\n",
    "        \n",
    "        if entity_retry_count == max_entity_retries and total_entities == 0:\n",
    "            print(f\"   ‚ùå Th·∫•t b·∫°i sau {max_entity_retries} l·∫ßn th·ª≠\")\n",
    "            continue\n",
    "        \n",
    "        # Process initial entities and build frontier\n",
    "        initial_frontier = []\n",
    "        \n",
    "        for impact in [\"POSITIVE\", \"NEGATIVE\"]:\n",
    "            for ent, content in entities_dict.get(impact, []):\n",
    "                if not ent or \"kh√¥ng c√≥ th·ª±c th·ªÉ n√†o\" in ent.lower():\n",
    "                    continue\n",
    "                \n",
    "                canon_ent = merge_entity(ent, canonical_entities)\n",
    "                \n",
    "                node_type = \"stock\" if any(str(canon_ent).lower().find(stock.lower()) != -1 for stock in PORTFOLIO_STOCKS) else \"entity\"\n",
    "                \n",
    "                if not G.has_node(canon_ent):\n",
    "                    G.add_node(canon_ent, type=node_type, timestamp=article_timestamp)\n",
    "                \n",
    "                if not G.has_edge(article_node, canon_ent):\n",
    "                    G.add_edge(article_node, canon_ent, impact=impact, timestamp=article_timestamp)\n",
    "                \n",
    "                # Save to results\n",
    "                all_entities.append({\n",
    "                    \"article_id\": article_idx,\n",
    "                    \"article_title\": row['title'],\n",
    "                    \"date\": row['date'],\n",
    "                    \"entity\": canon_ent,\n",
    "                    \"entity_type\": node_type,\n",
    "                    \"impact\": impact,\n",
    "                    \"explanation\": content,\n",
    "                    \"iteration\": 0\n",
    "                })\n",
    "                \n",
    "                # Add to frontier if entity type\n",
    "                if node_type == \"entity\":\n",
    "                    initial_frontier.append((canon_ent, impact, content))\n",
    "        \n",
    "        # Phase 2: Relation extraction with iterations\n",
    "        print(f\"   üîó Phase 2: Tr√≠ch xu·∫•t m·ªëi quan h·ªá (max {max_iterations} iterations)...\")\n",
    "        \n",
    "        current_frontier = initial_frontier\n",
    "        \n",
    "        for iteration in range(max_iterations):\n",
    "            if not current_frontier:\n",
    "                print(f\"   ‚úì Iteration {iteration + 1}: Frontier r·ªóng, d·ª´ng s·ªõm\")\n",
    "                break\n",
    "            \n",
    "            print(f\"   üîÑ Iteration {iteration + 1}: X·ª≠ l√Ω {len(current_frontier)} entities...\")\n",
    "            \n",
    "            # Process in batches\n",
    "            next_frontier = []\n",
    "            \n",
    "            for i in range(0, len(current_frontier), batch_size):\n",
    "                batch = current_frontier[i:i + batch_size]\n",
    "                print(f\"      Batch {i//batch_size + 1}: {len(batch)} entities\")\n",
    "                \n",
    "                new_entities = batch_process_entity_relationships(\n",
    "                    batch, G, canonical_entities, PORTFOLIO_STOCKS, \n",
    "                    portfolio_str_full, article_timestamp, chain_batch_relation\n",
    "                )\n",
    "                \n",
    "                # Save new entities to results\n",
    "                for new_ent, new_impact, new_content in new_entities:\n",
    "                    all_entities.append({\n",
    "                        \"article_id\": article_idx,\n",
    "                        \"article_title\": row['title'],\n",
    "                        \"date\": row['date'],\n",
    "                        \"entity\": new_ent,\n",
    "                        \"entity_type\": \"entity\",\n",
    "                        \"impact\": new_impact,\n",
    "                        \"explanation\": new_content,\n",
    "                        \"iteration\": iteration + 1\n",
    "                    })\n",
    "                \n",
    "                next_frontier.extend(new_entities)\n",
    "            \n",
    "            print(f\"   ‚úì Iteration {iteration + 1}: Ph√°t hi·ªán {len(next_frontier)} entities m·ªõi\")\n",
    "            current_frontier = next_frontier\n",
    "        \n",
    "        print(f\"   ‚úÖ Ho√†n th√†nh x·ª≠ l√Ω tin {article_idx}\")\n",
    "    \n",
    "    # Create results DataFrame\n",
    "    entities_df = pd.DataFrame(all_entities)\n",
    "    \n",
    "    # Save entities CSV\n",
    "    entities_df.to_csv(output_csv, index=False, encoding='utf-8-sig')\n",
    "    print(f\"\\nüíæ ƒê√£ l∆∞u entities v√†o: {output_csv}\")\n",
    "    \n",
    "    # Save graph pickle\n",
    "    import pickle\n",
    "    with open(output_graph_pkl, 'wb') as f:\n",
    "        pickle.dump(G, f)\n",
    "    print(f\"üíæ ƒê√£ l∆∞u graph v√†o: {output_graph_pkl}\")\n",
    "    \n",
    "    # Save canonical entities pickle\n",
    "    with open(output_canonical_pkl, 'wb') as f:\n",
    "        pickle.dump(canonical_entities, f)\n",
    "    print(f\"üíæ ƒê√£ l∆∞u canonical entities v√†o: {output_canonical_pkl}\")\n",
    "    \n",
    "    # Export graph tuples CSV\n",
    "    print(f\"\\nüì¶ ƒêang export graph tuples...\")\n",
    "    tuples_data = []\n",
    "    for u, v, data in G.edges(data=True):\n",
    "        # Skip article nodes\n",
    "        if u.startswith(\"Article_\"):\n",
    "            continue\n",
    "        \n",
    "        timestamp = data.get('timestamp')\n",
    "        if timestamp:\n",
    "            try:\n",
    "                if isinstance(timestamp, pd.Timestamp):\n",
    "                    date_str = timestamp.date().isoformat()\n",
    "                else:\n",
    "                    date_str = pd.to_datetime(timestamp).date().isoformat()\n",
    "                \n",
    "                tuples_data.append({\n",
    "                    'date': date_str,\n",
    "                    'source': u,\n",
    "                    'impact': data.get('impact', ''),\n",
    "                    'target': v\n",
    "                })\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    if tuples_data:\n",
    "        tuples_df = pd.DataFrame(tuples_data)\n",
    "        tuples_df = tuples_df.sort_values('date')\n",
    "        tuples_df.to_csv(output_tuples_csv, index=False, encoding='utf-8-sig')\n",
    "        print(f\"‚úì ƒê√£ export {len(tuples_df)} tuples v√†o: {output_tuples_csv}\")\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"‚úÖ HO√ÄN TH√ÄNH!\")\n",
    "    print(f\"üìä T·ªïng s·ªë entities: {len(entities_df)}\")\n",
    "    print(f\"üîπ Unique entities: {len(canonical_entities)}\")\n",
    "    print(f\"üìà Graph nodes: {len(G.nodes())}\")\n",
    "    print(f\"üîó Graph edges: {len(G.edges())}\")\n",
    "    print(f\"üìÅ Files ƒë√£ t·∫°o:\")\n",
    "    print(f\"   - {output_csv}\")\n",
    "    print(f\"   - {output_graph_pkl}\")\n",
    "    print(f\"   - {output_canonical_pkl}\")\n",
    "    print(f\"   - {output_tuples_csv}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    return entities_df, G, canonical_entities\n",
    "\n",
    "print(\"‚úì ƒê√£ ƒë·ªãnh nghƒ©a h√†m extract_entities_from_news() v·ªõi workflow ƒë·∫ßy ƒë·ªß nh∆∞ TRR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea251fe5",
   "metadata": {},
   "source": [
    "## üöÄ Ch·∫°y tr√≠ch xu·∫•t th·ª±c th·ªÉ v√† m·ªëi quan h·ªá (Workflow ƒë·∫ßy ƒë·ªß)\n",
    "\n",
    "### Parameters Quan Tr·ªçng:\n",
    "\n",
    "- **csv_path**: File CSV input (`summarized_news_with_stocks_merged.csv`)\n",
    "- **start_date**: `\"2025-01-01\"` - Ch·ªâ x·ª≠ l√Ω tin t·ª´ ng√†y n√†y tr·ªü ƒëi\n",
    "- **max_articles**: `100` - S·ªë tin t·ªëi ƒëa (test), b·ªè ƒë·ªÉ ch·∫°y h·∫øt\n",
    "- **max_iterations**: `2` - S·ªë v√≤ng l·∫∑p relation extraction (gi·ªëng TRR)\n",
    "- **batch_size**: `5` - S·ªë entities x·ª≠ l√Ω trong m·ªôt batch API call\n",
    "\n",
    "### Workflow:\n",
    "1. **Load & Filter Data** - ƒê·ªçc CSV, l·ªçc theo date range\n",
    "2. **For each article:**\n",
    "   - **Phase 1:** Extract initial entities ‚Üí Add to graph\n",
    "   - **Phase 2:** Relation extraction v·ªõi 2 iterations:\n",
    "     - Iteration 1: Extract relations t·ª´ initial entities\n",
    "     - Iteration 2: Extract relations t·ª´ entities m·ªõi t√¨m ƒë∆∞·ª£c\n",
    "   - X·ª≠ l√Ω entities trong batches (5 entities/call)\n",
    "3. **Save Outputs:**\n",
    "   - `entities_extracted.csv` v·ªõi iteration info\n",
    "   - `knowledge_graph.pkl` - NetworkX graph\n",
    "   - `canonical_entities.pkl` - Entity set\n",
    "   - `graph_tuples_step1.csv` - **Tuples CSV gi·ªëng TRR**\n",
    "\n",
    "### Outputs:\n",
    "- ‚úÖ DataFrame v·ªõi iteration tracking\n",
    "- ‚úÖ NetworkX DiGraph v·ªõi article, entity, stock nodes\n",
    "- ‚úÖ CSV tuples format: `(date, source, impact, target)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "aeb6e205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìñ ƒêang ƒë·ªçc d·ªØ li·ªáu t·ª´ summarized_news_with_stocks_merged.csv...\n",
      "‚úì ƒê√£ ƒë·ªçc 24571 tin t·ª©c‚úì ƒê√£ ƒë·ªçc 24571 tin t·ª©c\n",
      "‚úì Dataset h·ª£p l·ªá v·ªõi c√°c c·ªôt: ['postID', 'stockCodes', 'title', 'description', 'date']\n",
      "‚úì L·ªçc t·ª´ ng√†y 2025-01-01: c√≤n 24571 tin\n",
      "‚úì Gi·ªõi h·∫°n xu·ªëng 10 tin ƒë·ªÉ x·ª≠ l√Ω\n",
      "\n",
      "============================================================\n",
      "üîç B·∫ÆT ƒê·∫¶U TR√çCH XU·∫§T TH·ª∞C TH·ªÇ V√Ä M·ªêI QUAN H·ªÜ\n",
      "============================================================\n",
      "\n",
      "\n",
      "[1/10] üì∞ S·ªë l∆∞·ª£ng h√≥a ƒë∆°n kh·ªüi t·∫°o t·ª´ m√°y t√≠nh ti·ªÅn tƒÉng g·∫•p 13 l·∫ßn n...\n",
      "   üìå Phase 1: Tr√≠ch xu·∫•t entities ban ƒë·∫ßu...\n",
      "\n",
      "‚úì Dataset h·ª£p l·ªá v·ªõi c√°c c·ªôt: ['postID', 'stockCodes', 'title', 'description', 'date']\n",
      "‚úì L·ªçc t·ª´ ng√†y 2025-01-01: c√≤n 24571 tin\n",
      "‚úì Gi·ªõi h·∫°n xu·ªëng 10 tin ƒë·ªÉ x·ª≠ l√Ω\n",
      "\n",
      "============================================================\n",
      "üîç B·∫ÆT ƒê·∫¶U TR√çCH XU·∫§T TH·ª∞C TH·ªÇ V√Ä M·ªêI QUAN H·ªÜ\n",
      "============================================================\n",
      "\n",
      "\n",
      "[1/10] üì∞ S·ªë l∆∞·ª£ng h√≥a ƒë∆°n kh·ªüi t·∫°o t·ª´ m√°y t√≠nh ti·ªÅn tƒÉng g·∫•p 13 l·∫ßn n...\n",
      "   üìå Phase 1: Tr√≠ch xu·∫•t entities ban ƒë·∫ßu...\n",
      "‚ôª ƒê√£ reset error counters\n",
      "‚ôª ƒê√£ reset error counters\n",
      "   ‚úì Tr√≠ch xu·∫•t ƒë∆∞·ª£c 3 th·ª±c th·ªÉ ban ƒë·∫ßu\n",
      "   üîó Phase 2: Tr√≠ch xu·∫•t m·ªëi quan h·ªá (max 2 iterations)...\n",
      "   üîÑ Iteration 1: X·ª≠ l√Ω 3 entities...\n",
      "      Batch 1: 3 entities\n",
      "   ‚úì Tr√≠ch xu·∫•t ƒë∆∞·ª£c 3 th·ª±c th·ªÉ ban ƒë·∫ßu\n",
      "   üîó Phase 2: Tr√≠ch xu·∫•t m·ªëi quan h·ªá (max 2 iterations)...\n",
      "   üîÑ Iteration 1: X·ª≠ l√Ω 3 entities...\n",
      "      Batch 1: 3 entities\n",
      "‚ôª ƒê√£ reset error counters\n",
      "‚ôª ƒê√£ reset error counters\n",
      "   ‚úì Iteration 1: Ph√°t hi·ªán 12 entities m·ªõi\n",
      "   üîÑ Iteration 2: X·ª≠ l√Ω 12 entities...\n",
      "      Batch 1: 12 entities\n",
      "   ‚úì Iteration 1: Ph√°t hi·ªán 12 entities m·ªõi\n",
      "   üîÑ Iteration 2: X·ª≠ l√Ω 12 entities...\n",
      "      Batch 1: 12 entities\n",
      "‚ôª ƒê√£ reset error counters\n",
      "‚ôª ƒê√£ reset error counters\n",
      "   ‚úì Iteration 2: Ph√°t hi·ªán 42 entities m·ªõi\n",
      "   ‚úÖ Ho√†n th√†nh x·ª≠ l√Ω tin 1\n",
      "\n",
      "[2/10] üì∞ T·ª∑ ph√∫ Mark Zuckerberg v√† Meta h∆∞·ªõng t·ªõi ƒëi·ªÅu g√¨ nƒÉm 2025?...\n",
      "   üìå Phase 1: Tr√≠ch xu·∫•t entities ban ƒë·∫ßu...\n",
      "   ‚úì Iteration 2: Ph√°t hi·ªán 42 entities m·ªõi\n",
      "   ‚úÖ Ho√†n th√†nh x·ª≠ l√Ω tin 1\n",
      "\n",
      "[2/10] üì∞ T·ª∑ ph√∫ Mark Zuckerberg v√† Meta h∆∞·ªõng t·ªõi ƒëi·ªÅu g√¨ nƒÉm 2025?...\n",
      "   üìå Phase 1: Tr√≠ch xu·∫•t entities ban ƒë·∫ßu...\n",
      "‚ôª ƒê√£ reset error counters\n",
      "‚ôª ƒê√£ reset error counters\n",
      "   ‚úì Tr√≠ch xu·∫•t ƒë∆∞·ª£c 3 th·ª±c th·ªÉ ban ƒë·∫ßu\n",
      "   üîó Phase 2: Tr√≠ch xu·∫•t m·ªëi quan h·ªá (max 2 iterations)...\n",
      "   üîÑ Iteration 1: X·ª≠ l√Ω 3 entities...\n",
      "      Batch 1: 3 entities\n",
      "   ‚úì Tr√≠ch xu·∫•t ƒë∆∞·ª£c 3 th·ª±c th·ªÉ ban ƒë·∫ßu\n",
      "   üîó Phase 2: Tr√≠ch xu·∫•t m·ªëi quan h·ªá (max 2 iterations)...\n",
      "   üîÑ Iteration 1: X·ª≠ l√Ω 3 entities...\n",
      "      Batch 1: 3 entities\n",
      "‚ôª ƒê√£ reset error counters\n",
      "‚ôª ƒê√£ reset error counters\n",
      "   ‚úì Iteration 1: Ph√°t hi·ªán 18 entities m·ªõi\n",
      "   üîÑ Iteration 2: X·ª≠ l√Ω 18 entities...\n",
      "      Batch 1: 18 entities\n",
      "   ‚úì Iteration 1: Ph√°t hi·ªán 18 entities m·ªõi\n",
      "   üîÑ Iteration 2: X·ª≠ l√Ω 18 entities...\n",
      "      Batch 1: 18 entities\n",
      "‚ôª ƒê√£ reset error counters\n",
      "‚ôª ƒê√£ reset error counters\n",
      "   ‚úì Iteration 2: Ph√°t hi·ªán 42 entities m·ªõi\n",
      "   ‚úÖ Ho√†n th√†nh x·ª≠ l√Ω tin 2\n",
      "\n",
      "[3/10] üì∞ ƒê·∫ßu t∆∞ x√¢y d·ª±ng, kinh doanh k·∫øt c·∫•u h·∫° t·∫ßng khu c√¥ng nghi·ªáp ...\n",
      "   üìå Phase 1: Tr√≠ch xu·∫•t entities ban ƒë·∫ßu...\n",
      "   ‚úì Iteration 2: Ph√°t hi·ªán 42 entities m·ªõi\n",
      "   ‚úÖ Ho√†n th√†nh x·ª≠ l√Ω tin 2\n",
      "\n",
      "[3/10] üì∞ ƒê·∫ßu t∆∞ x√¢y d·ª±ng, kinh doanh k·∫øt c·∫•u h·∫° t·∫ßng khu c√¥ng nghi·ªáp ...\n",
      "   üìå Phase 1: Tr√≠ch xu·∫•t entities ban ƒë·∫ßu...\n",
      "‚ôª ƒê√£ reset error counters\n",
      "‚ôª ƒê√£ reset error counters\n",
      "   ‚úì Tr√≠ch xu·∫•t ƒë∆∞·ª£c 3 th·ª±c th·ªÉ ban ƒë·∫ßu\n",
      "   üîó Phase 2: Tr√≠ch xu·∫•t m·ªëi quan h·ªá (max 2 iterations)...\n",
      "   üîÑ Iteration 1: X·ª≠ l√Ω 3 entities...\n",
      "      Batch 1: 3 entities\n",
      "   ‚úì Tr√≠ch xu·∫•t ƒë∆∞·ª£c 3 th·ª±c th·ªÉ ban ƒë·∫ßu\n",
      "   üîó Phase 2: Tr√≠ch xu·∫•t m·ªëi quan h·ªá (max 2 iterations)...\n",
      "   üîÑ Iteration 1: X·ª≠ l√Ω 3 entities...\n",
      "      Batch 1: 3 entities\n",
      "‚ôª ƒê√£ reset error counters\n",
      "‚ôª ƒê√£ reset error counters\n",
      "   ‚úì Iteration 1: Ph√°t hi·ªán 14 entities m·ªõi\n",
      "   üîÑ Iteration 2: X·ª≠ l√Ω 14 entities...\n",
      "      Batch 1: 14 entities\n",
      "   ‚úì Iteration 1: Ph√°t hi·ªán 14 entities m·ªõi\n",
      "   üîÑ Iteration 2: X·ª≠ l√Ω 14 entities...\n",
      "      Batch 1: 14 entities\n",
      "‚ôª ƒê√£ reset error counters\n",
      "‚ôª ƒê√£ reset error counters\n",
      "   ‚úì Iteration 2: Ph√°t hi·ªán 41 entities m·ªõi\n",
      "   ‚úÖ Ho√†n th√†nh x·ª≠ l√Ω tin 3\n",
      "\n",
      "[4/10] üì∞ Qu·∫£ng Ninh quy·∫øt t√¢m v∆∞·ª£t m·ªëc tƒÉng tr∆∞·ªüng kinh t·∫ø 12% nƒÉm 20...\n",
      "   üìå Phase 1: Tr√≠ch xu·∫•t entities ban ƒë·∫ßu...\n",
      "   ‚úì Iteration 2: Ph√°t hi·ªán 41 entities m·ªõi\n",
      "   ‚úÖ Ho√†n th√†nh x·ª≠ l√Ω tin 3\n",
      "\n",
      "[4/10] üì∞ Qu·∫£ng Ninh quy·∫øt t√¢m v∆∞·ª£t m·ªëc tƒÉng tr∆∞·ªüng kinh t·∫ø 12% nƒÉm 20...\n",
      "   üìå Phase 1: Tr√≠ch xu·∫•t entities ban ƒë·∫ßu...\n",
      "‚ôª ƒê√£ reset error counters\n",
      "‚ôª ƒê√£ reset error counters\n",
      "   ‚úì Tr√≠ch xu·∫•t ƒë∆∞·ª£c 5 th·ª±c th·ªÉ ban ƒë·∫ßu\n",
      "   üîó Phase 2: Tr√≠ch xu·∫•t m·ªëi quan h·ªá (max 2 iterations)...\n",
      "   üîÑ Iteration 1: X·ª≠ l√Ω 5 entities...\n",
      "      Batch 1: 5 entities\n",
      "   ‚úì Tr√≠ch xu·∫•t ƒë∆∞·ª£c 5 th·ª±c th·ªÉ ban ƒë·∫ßu\n",
      "   üîó Phase 2: Tr√≠ch xu·∫•t m·ªëi quan h·ªá (max 2 iterations)...\n",
      "   üîÑ Iteration 1: X·ª≠ l√Ω 5 entities...\n",
      "      Batch 1: 5 entities\n",
      "‚ôª ƒê√£ reset error counters\n",
      "‚ôª ƒê√£ reset error counters\n",
      "   ‚úì Iteration 1: Ph√°t hi·ªán 22 entities m·ªõi\n",
      "   üîÑ Iteration 2: X·ª≠ l√Ω 22 entities...\n",
      "      Batch 1: 22 entities\n",
      "   ‚úì Iteration 1: Ph√°t hi·ªán 22 entities m·ªõi\n",
      "   üîÑ Iteration 2: X·ª≠ l√Ω 22 entities...\n",
      "      Batch 1: 22 entities\n",
      "‚ôª ƒê√£ reset error counters\n",
      "‚ôª ƒê√£ reset error counters\n",
      "   ‚úì Iteration 2: Ph√°t hi·ªán 48 entities m·ªõi\n",
      "   ‚úÖ Ho√†n th√†nh x·ª≠ l√Ω tin 4\n",
      "\n",
      "[5/10] üì∞ NƒÉm 2025: ƒê·ªìng Th√°p ph·∫•n ƒë·∫•u kim ng·∫°ch xu·∫•t kh·∫©u ƒë·∫°t 2,1 t·ª∑ ...\n",
      "   üìå Phase 1: Tr√≠ch xu·∫•t entities ban ƒë·∫ßu...\n",
      "   ‚úì Iteration 2: Ph√°t hi·ªán 48 entities m·ªõi\n",
      "   ‚úÖ Ho√†n th√†nh x·ª≠ l√Ω tin 4\n",
      "\n",
      "[5/10] üì∞ NƒÉm 2025: ƒê·ªìng Th√°p ph·∫•n ƒë·∫•u kim ng·∫°ch xu·∫•t kh·∫©u ƒë·∫°t 2,1 t·ª∑ ...\n",
      "   üìå Phase 1: Tr√≠ch xu·∫•t entities ban ƒë·∫ßu...\n",
      "‚ôª ƒê√£ reset error counters\n",
      "‚ôª ƒê√£ reset error counters\n",
      "   ‚úì Tr√≠ch xu·∫•t ƒë∆∞·ª£c 3 th·ª±c th·ªÉ ban ƒë·∫ßu\n",
      "   üîó Phase 2: Tr√≠ch xu·∫•t m·ªëi quan h·ªá (max 2 iterations)...\n",
      "   üîÑ Iteration 1: X·ª≠ l√Ω 3 entities...\n",
      "      Batch 1: 3 entities\n",
      "   ‚úì Tr√≠ch xu·∫•t ƒë∆∞·ª£c 3 th·ª±c th·ªÉ ban ƒë·∫ßu\n",
      "   üîó Phase 2: Tr√≠ch xu·∫•t m·ªëi quan h·ªá (max 2 iterations)...\n",
      "   üîÑ Iteration 1: X·ª≠ l√Ω 3 entities...\n",
      "      Batch 1: 3 entities\n",
      "‚ôª ƒê√£ reset error counters\n",
      "‚ôª ƒê√£ reset error counters\n",
      "   ‚úì Iteration 1: Ph√°t hi·ªán 18 entities m·ªõi\n",
      "   üîÑ Iteration 2: X·ª≠ l√Ω 18 entities...\n",
      "      Batch 1: 18 entities\n",
      "   ‚úì Iteration 1: Ph√°t hi·ªán 18 entities m·ªõi\n",
      "   üîÑ Iteration 2: X·ª≠ l√Ω 18 entities...\n",
      "      Batch 1: 18 entities\n",
      "‚ôª ƒê√£ reset error counters\n",
      "‚ôª ƒê√£ reset error counters\n",
      "   ‚úì Iteration 2: Ph√°t hi·ªán 47 entities m·ªõi\n",
      "   ‚úÖ Ho√†n th√†nh x·ª≠ l√Ω tin 5\n",
      "\n",
      "[6/10] üì∞ N√¥ng nghi·ªáp tƒÉng tr∆∞·ªüng ·∫•n t∆∞·ª£ng...\n",
      "   üìå Phase 1: Tr√≠ch xu·∫•t entities ban ƒë·∫ßu...\n",
      "   ‚úì Iteration 2: Ph√°t hi·ªán 47 entities m·ªõi\n",
      "   ‚úÖ Ho√†n th√†nh x·ª≠ l√Ω tin 5\n",
      "\n",
      "[6/10] üì∞ N√¥ng nghi·ªáp tƒÉng tr∆∞·ªüng ·∫•n t∆∞·ª£ng...\n",
      "   üìå Phase 1: Tr√≠ch xu·∫•t entities ban ƒë·∫ßu...\n",
      "‚ôª ƒê√£ reset error counters\n",
      "‚ôª ƒê√£ reset error counters\n",
      "   ‚úì Tr√≠ch xu·∫•t ƒë∆∞·ª£c 2 th·ª±c th·ªÉ ban ƒë·∫ßu\n",
      "   üîó Phase 2: Tr√≠ch xu·∫•t m·ªëi quan h·ªá (max 2 iterations)...\n",
      "   üîÑ Iteration 1: X·ª≠ l√Ω 2 entities...\n",
      "      Batch 1: 2 entities\n",
      "   ‚úì Tr√≠ch xu·∫•t ƒë∆∞·ª£c 2 th·ª±c th·ªÉ ban ƒë·∫ßu\n",
      "   üîó Phase 2: Tr√≠ch xu·∫•t m·ªëi quan h·ªá (max 2 iterations)...\n",
      "   üîÑ Iteration 1: X·ª≠ l√Ω 2 entities...\n",
      "      Batch 1: 2 entities\n",
      "‚ôª ƒê√£ reset error counters\n",
      "‚ôª ƒê√£ reset error counters\n",
      "   ‚úì Iteration 1: Ph√°t hi·ªán 10 entities m·ªõi\n",
      "   üîÑ Iteration 2: X·ª≠ l√Ω 10 entities...\n",
      "      Batch 1: 10 entities\n",
      "   ‚úì Iteration 1: Ph√°t hi·ªán 10 entities m·ªõi\n",
      "   üîÑ Iteration 2: X·ª≠ l√Ω 10 entities...\n",
      "      Batch 1: 10 entities\n",
      "‚ôª ƒê√£ reset error counters\n",
      "‚ôª ƒê√£ reset error counters\n",
      "   ‚úì Iteration 2: Ph√°t hi·ªán 33 entities m·ªõi\n",
      "   ‚úÖ Ho√†n th√†nh x·ª≠ l√Ω tin 6\n",
      "\n",
      "[7/10] üì∞ H√† N·ªôi x·ª©ng ƒë√°ng v·ªõi vai tr√≤ trung t√¢m kinh t·∫ø...\n",
      "   üìå Phase 1: Tr√≠ch xu·∫•t entities ban ƒë·∫ßu...\n",
      "   ‚úì Iteration 2: Ph√°t hi·ªán 33 entities m·ªõi\n",
      "   ‚úÖ Ho√†n th√†nh x·ª≠ l√Ω tin 6\n",
      "\n",
      "[7/10] üì∞ H√† N·ªôi x·ª©ng ƒë√°ng v·ªõi vai tr√≤ trung t√¢m kinh t·∫ø...\n",
      "   üìå Phase 1: Tr√≠ch xu·∫•t entities ban ƒë·∫ßu...\n",
      "‚ôª ƒê√£ reset error counters\n",
      "‚ôª ƒê√£ reset error counters\n",
      "   ‚úì Tr√≠ch xu·∫•t ƒë∆∞·ª£c 3 th·ª±c th·ªÉ ban ƒë·∫ßu\n",
      "   üîó Phase 2: Tr√≠ch xu·∫•t m·ªëi quan h·ªá (max 2 iterations)...\n",
      "   üîÑ Iteration 1: X·ª≠ l√Ω 3 entities...\n",
      "      Batch 1: 3 entities\n",
      "   ‚úì Tr√≠ch xu·∫•t ƒë∆∞·ª£c 3 th·ª±c th·ªÉ ban ƒë·∫ßu\n",
      "   üîó Phase 2: Tr√≠ch xu·∫•t m·ªëi quan h·ªá (max 2 iterations)...\n",
      "   üîÑ Iteration 1: X·ª≠ l√Ω 3 entities...\n",
      "      Batch 1: 3 entities\n",
      "‚ôª ƒê√£ reset error counters\n",
      "‚ôª ƒê√£ reset error counters\n",
      "   ‚úì Iteration 1: Ph√°t hi·ªán 13 entities m·ªõi\n",
      "   üîÑ Iteration 2: X·ª≠ l√Ω 13 entities...\n",
      "      Batch 1: 13 entities\n",
      "   ‚úì Iteration 1: Ph√°t hi·ªán 13 entities m·ªõi\n",
      "   üîÑ Iteration 2: X·ª≠ l√Ω 13 entities...\n",
      "      Batch 1: 13 entities\n",
      "‚ôª ƒê√£ reset error counters\n",
      "‚ôª ƒê√£ reset error counters\n",
      "   ‚úì Iteration 2: Ph√°t hi·ªán 51 entities m·ªõi\n",
      "   ‚úÖ Ho√†n th√†nh x·ª≠ l√Ω tin 7\n",
      "\n",
      "[8/10] üì∞ ThƒÉng h·∫°ng trong chu·ªói gi√° tr·ªã, h√†ng Made in Vietnam ƒë∆∞·ª£c ƒë√°...\n",
      "   üìå Phase 1: Tr√≠ch xu·∫•t entities ban ƒë·∫ßu...\n",
      "   ‚úì Iteration 2: Ph√°t hi·ªán 51 entities m·ªõi\n",
      "   ‚úÖ Ho√†n th√†nh x·ª≠ l√Ω tin 7\n",
      "\n",
      "[8/10] üì∞ ThƒÉng h·∫°ng trong chu·ªói gi√° tr·ªã, h√†ng Made in Vietnam ƒë∆∞·ª£c ƒë√°...\n",
      "   üìå Phase 1: Tr√≠ch xu·∫•t entities ban ƒë·∫ßu...\n",
      "‚ôª ƒê√£ reset error counters\n",
      "‚ôª ƒê√£ reset error counters\n",
      "   ‚úì Tr√≠ch xu·∫•t ƒë∆∞·ª£c 3 th·ª±c th·ªÉ ban ƒë·∫ßu\n",
      "   üîó Phase 2: Tr√≠ch xu·∫•t m·ªëi quan h·ªá (max 2 iterations)...\n",
      "   üîÑ Iteration 1: X·ª≠ l√Ω 3 entities...\n",
      "      Batch 1: 3 entities\n",
      "   ‚úì Tr√≠ch xu·∫•t ƒë∆∞·ª£c 3 th·ª±c th·ªÉ ban ƒë·∫ßu\n",
      "   üîó Phase 2: Tr√≠ch xu·∫•t m·ªëi quan h·ªá (max 2 iterations)...\n",
      "   üîÑ Iteration 1: X·ª≠ l√Ω 3 entities...\n",
      "      Batch 1: 3 entities\n",
      "‚ôª ƒê√£ reset error counters\n",
      "‚ôª ƒê√£ reset error counters\n",
      "   ‚úì Iteration 1: Ph√°t hi·ªán 12 entities m·ªõi\n",
      "   üîÑ Iteration 2: X·ª≠ l√Ω 12 entities...\n",
      "      Batch 1: 12 entities\n",
      "   ‚úì Iteration 1: Ph√°t hi·ªán 12 entities m·ªõi\n",
      "   üîÑ Iteration 2: X·ª≠ l√Ω 12 entities...\n",
      "      Batch 1: 12 entities\n",
      "‚ôª ƒê√£ reset error counters\n",
      "‚ôª ƒê√£ reset error counters\n",
      "   ‚úì Iteration 2: Ph√°t hi·ªán 36 entities m·ªõi\n",
      "   ‚úÖ Ho√†n th√†nh x·ª≠ l√Ω tin 8\n",
      "\n",
      "[9/10] üì∞ T∆∞·ªüng s·∫Øp ƒë∆∞·ª£c nh·∫≠n nh√†, h∆°n 100 ng∆∞·ªùi b·∫•t ng·ªù b·ªã ch·ªß ƒë·∫ßu t∆∞...\n",
      "   üìå Phase 1: Tr√≠ch xu·∫•t entities ban ƒë·∫ßu...\n",
      "   ‚úì Iteration 2: Ph√°t hi·ªán 36 entities m·ªõi\n",
      "   ‚úÖ Ho√†n th√†nh x·ª≠ l√Ω tin 8\n",
      "\n",
      "[9/10] üì∞ T∆∞·ªüng s·∫Øp ƒë∆∞·ª£c nh·∫≠n nh√†, h∆°n 100 ng∆∞·ªùi b·∫•t ng·ªù b·ªã ch·ªß ƒë·∫ßu t∆∞...\n",
      "   üìå Phase 1: Tr√≠ch xu·∫•t entities ban ƒë·∫ßu...\n",
      "‚ôª ƒê√£ reset error counters\n",
      "‚ôª ƒê√£ reset error counters\n",
      "   ‚úì Tr√≠ch xu·∫•t ƒë∆∞·ª£c 5 th·ª±c th·ªÉ ban ƒë·∫ßu\n",
      "   üîó Phase 2: Tr√≠ch xu·∫•t m·ªëi quan h·ªá (max 2 iterations)...\n",
      "   üîÑ Iteration 1: X·ª≠ l√Ω 3 entities...\n",
      "      Batch 1: 3 entities\n",
      "   ‚úì Tr√≠ch xu·∫•t ƒë∆∞·ª£c 5 th·ª±c th·ªÉ ban ƒë·∫ßu\n",
      "   üîó Phase 2: Tr√≠ch xu·∫•t m·ªëi quan h·ªá (max 2 iterations)...\n",
      "   üîÑ Iteration 1: X·ª≠ l√Ω 3 entities...\n",
      "      Batch 1: 3 entities\n",
      "‚ôª ƒê√£ reset error counters\n",
      "‚ôª ƒê√£ reset error counters\n",
      "   ‚úì Iteration 1: Ph√°t hi·ªán 12 entities m·ªõi\n",
      "   üîÑ Iteration 2: X·ª≠ l√Ω 12 entities...\n",
      "      Batch 1: 12 entities\n",
      "   ‚úì Iteration 1: Ph√°t hi·ªán 12 entities m·ªõi\n",
      "   üîÑ Iteration 2: X·ª≠ l√Ω 12 entities...\n",
      "      Batch 1: 12 entities\n",
      "‚ôª ƒê√£ reset error counters\n",
      "‚ôª ƒê√£ reset error counters\n",
      "   ‚úì Iteration 2: Ph√°t hi·ªán 39 entities m·ªõi\n",
      "   ‚úÖ Ho√†n th√†nh x·ª≠ l√Ω tin 9\n",
      "\n",
      "[10/10] üì∞ B·∫•t ƒë·ªông s·∫£n ph·ª•c h·ªìi c√≥ l√†m ·∫•m th·ªã tr∆∞·ªùng tr√°i phi·∫øu doanh ...\n",
      "   üìå Phase 1: Tr√≠ch xu·∫•t entities ban ƒë·∫ßu...\n",
      "   ‚úì Iteration 2: Ph√°t hi·ªán 39 entities m·ªõi\n",
      "   ‚úÖ Ho√†n th√†nh x·ª≠ l√Ω tin 9\n",
      "\n",
      "[10/10] üì∞ B·∫•t ƒë·ªông s·∫£n ph·ª•c h·ªìi c√≥ l√†m ·∫•m th·ªã tr∆∞·ªùng tr√°i phi·∫øu doanh ...\n",
      "   üìå Phase 1: Tr√≠ch xu·∫•t entities ban ƒë·∫ßu...\n",
      "‚ôª ƒê√£ reset error counters\n",
      "‚ôª ƒê√£ reset error counters\n",
      "   ‚úì Tr√≠ch xu·∫•t ƒë∆∞·ª£c 5 th·ª±c th·ªÉ ban ƒë·∫ßu\n",
      "   üîó Phase 2: Tr√≠ch xu·∫•t m·ªëi quan h·ªá (max 2 iterations)...\n",
      "   üîÑ Iteration 1: X·ª≠ l√Ω 3 entities...\n",
      "      Batch 1: 3 entities\n",
      "   ‚úì Tr√≠ch xu·∫•t ƒë∆∞·ª£c 5 th·ª±c th·ªÉ ban ƒë·∫ßu\n",
      "   üîó Phase 2: Tr√≠ch xu·∫•t m·ªëi quan h·ªá (max 2 iterations)...\n",
      "   üîÑ Iteration 1: X·ª≠ l√Ω 3 entities...\n",
      "      Batch 1: 3 entities\n",
      "‚ôª ƒê√£ reset error counters\n",
      "‚ôª ƒê√£ reset error counters\n",
      "   ‚úì Iteration 1: Ph√°t hi·ªán 18 entities m·ªõi\n",
      "   üîÑ Iteration 2: X·ª≠ l√Ω 18 entities...\n",
      "      Batch 1: 18 entities\n",
      "   ‚úì Iteration 1: Ph√°t hi·ªán 18 entities m·ªõi\n",
      "   üîÑ Iteration 2: X·ª≠ l√Ω 18 entities...\n",
      "      Batch 1: 18 entities\n",
      "‚ôª ƒê√£ reset error counters\n",
      "‚ôª ƒê√£ reset error counters\n",
      "   ‚úì Iteration 2: Ph√°t hi·ªán 167 entities m·ªõi\n",
      "   ‚úÖ Ho√†n th√†nh x·ª≠ l√Ω tin 10\n",
      "\n",
      "üíæ ƒê√£ l∆∞u entities v√†o: entities_extracted.csv\n",
      "üíæ ƒê√£ l∆∞u graph v√†o: knowledge_graph.pkl\n",
      "üíæ ƒê√£ l∆∞u canonical entities v√†o: canonical_entities.pkl\n",
      "\n",
      "üì¶ ƒêang export graph tuples...\n",
      "‚úì ƒê√£ export 178 tuples v√†o: graph_tuples_step1.csv\n",
      "\n",
      "============================================================\n",
      "‚úÖ HO√ÄN TH√ÄNH!\n",
      "üìä T·ªïng s·ªë entities: 730\n",
      "üîπ Unique entities: 19\n",
      "üìà Graph nodes: 29\n",
      "üîó Graph edges: 211\n",
      "üìÅ Files ƒë√£ t·∫°o:\n",
      "   - entities_extracted.csv\n",
      "   - knowledge_graph.pkl\n",
      "   - canonical_entities.pkl\n",
      "   - graph_tuples_step1.csv\n",
      "============================================================\n",
      "   ‚úì Iteration 2: Ph√°t hi·ªán 167 entities m·ªõi\n",
      "   ‚úÖ Ho√†n th√†nh x·ª≠ l√Ω tin 10\n",
      "\n",
      "üíæ ƒê√£ l∆∞u entities v√†o: entities_extracted.csv\n",
      "üíæ ƒê√£ l∆∞u graph v√†o: knowledge_graph.pkl\n",
      "üíæ ƒê√£ l∆∞u canonical entities v√†o: canonical_entities.pkl\n",
      "\n",
      "üì¶ ƒêang export graph tuples...\n",
      "‚úì ƒê√£ export 178 tuples v√†o: graph_tuples_step1.csv\n",
      "\n",
      "============================================================\n",
      "‚úÖ HO√ÄN TH√ÄNH!\n",
      "üìä T·ªïng s·ªë entities: 730\n",
      "üîπ Unique entities: 19\n",
      "üìà Graph nodes: 29\n",
      "üîó Graph edges: 211\n",
      "üìÅ Files ƒë√£ t·∫°o:\n",
      "   - entities_extracted.csv\n",
      "   - knowledge_graph.pkl\n",
      "   - canonical_entities.pkl\n",
      "   - graph_tuples_step1.csv\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CH·∫†Y TR√çCH XU·∫§T TH·ª∞C TH·ªÇ V√Ä M·ªêI QUAN H·ªÜ\n",
    "# ============================================================\n",
    "\n",
    "# Ch·∫°y v·ªõi file summarized_news_with_stocks_merged.csv t·ª´ ng√†y 2025-01-01\n",
    "entities_df, G, canonical_entities = extract_entities_from_news(\n",
    "    csv_path=\"summarized_news_with_stocks_merged.csv\",\n",
    "    output_csv=\"entities_extracted.csv\",\n",
    "    output_graph_pkl=\"knowledge_graph.pkl\",\n",
    "    output_canonical_pkl=\"canonical_entities.pkl\",\n",
    "    output_tuples_csv=\"graph_tuples_step1.csv\",\n",
    "    start_date=\"2025-01-01\",\n",
    "    end_date=None,\n",
    "    max_articles=10,  # Test v·ªõi 100 tin tr∆∞·ªõc, sau ƒë√≥ c√≥ th·ªÉ b·ªè ƒë·ªÉ ch·∫°y h·∫øt\n",
    "    max_iterations=2,  # S·ªë v√≤ng l·∫∑p relation extraction (gi·ªëng TRR)\n",
    "    batch_size=500  # S·ªë entities x·ª≠ l√Ω trong m·ªôt batch\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a79ca8e",
   "metadata": {},
   "source": [
    "## üìä Ph√¢n t√≠ch k·∫øt qu·∫£\n",
    "\n",
    "Xem c√°c th·ª±c th·ªÉ ƒë√£ tr√≠ch xu·∫•t:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6611160e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä T·ªîNG QUAN K·∫æT QU·∫¢\n",
      "============================================================\n",
      "\n",
      "T·ªïng s·ªë entities tr√≠ch xu·∫•t: 730\n",
      "Unique entities: 19\n",
      "Graph nodes: 29\n",
      "Graph edges: 211\n",
      "\n",
      "üìà PH√ÇN LO·∫†I THEO IMPACT:\n",
      "\n",
      "impact\n",
      "POSITIVE    690\n",
      "NEGATIVE     40\n",
      "Name: count, dtype: int64\n",
      "\n",
      "üìå PH√ÇN LO·∫†I THEO ENTITY TYPE:\n",
      "\n",
      "entity_type\n",
      "entity    726\n",
      "stock       4\n",
      "Name: count, dtype: int64\n",
      "\n",
      "üîÑ PH√ÇN LO·∫†I THEO ITERATION:\n",
      "\n",
      "iteration\n",
      "0     35\n",
      "1    149\n",
      "2    546\n",
      "Name: count, dtype: int64\n",
      "\n",
      "============================================================\n",
      "M·∫™U 10 ENTITIES ƒê·∫¶U TI√äN:\n",
      "============================================================\n",
      "\n",
      "[1] üì∞ S·ªë l∆∞·ª£ng h√≥a ƒë∆°n kh·ªüi t·∫°o t·ª´ m√°y t√≠nh ti·ªÅn tƒÉng g·∫•...\n",
      "üè∑Ô∏è  Entity: Ng√†nh b√°n l·∫ª Vi·ªát Nam (entity)\n",
      "‚úÖ Impact: POSITIVE\n",
      "üîÑ Iteration: 0\n",
      "üìù S·ªë l∆∞·ª£ng h√≥a ƒë∆°n ƒëi·ªán t·ª≠ t·ª´ m√°y t√≠nh ti·ªÅn tƒÉng m·∫°nh trong nƒÉm 2024, cho th·∫•y s·ª± chuy·ªÉn ƒë·ªïi s·ªë ƒëang d...\n",
      "------------------------------------------------------------\n",
      "\n",
      "[1] üì∞ S·ªë l∆∞·ª£ng h√≥a ƒë∆°n kh·ªüi t·∫°o t·ª´ m√°y t√≠nh ti·ªÅn tƒÉng g·∫•...\n",
      "üè∑Ô∏è  Entity: Ng√†nh c√¥ng ngh·ªá Vi·ªát Nam (entity)\n",
      "‚úÖ Impact: POSITIVE\n",
      "üîÑ Iteration: 0\n",
      "üìù C√°c doanh nghi·ªáp c√¥ng ngh·ªá cung c·∫•p gi·∫£i ph√°p h√≥a ƒë∆°n ƒëi·ªán t·ª≠ t·ª´ m√°y t√≠nh ti·ªÅn c√≥ th·ªÉ ch·ª©ng ki·∫øn s·ª± ...\n",
      "------------------------------------------------------------\n",
      "\n",
      "[1] üì∞ S·ªë l∆∞·ª£ng h√≥a ƒë∆°n kh·ªüi t·∫°o t·ª´ m√°y t√≠nh ti·ªÅn tƒÉng g·∫•...\n",
      "üè∑Ô∏è  Entity: Ng√†nh b√°n l·∫ª Vi·ªát Nam (entity)\n",
      "‚ùå Impact: NEGATIVE\n",
      "üîÑ Iteration: 0\n",
      "üìù T·ª∑ l·ªá s·ª≠ d·ª•ng h√≥a ƒë∆°n ƒëi·ªán t·ª≠ t·ª´ m√°y t√≠nh ti·ªÅn m·ªõi ƒë·∫°t g·∫ßn 5% cho th·∫•y s·ª± ch·∫≠m tr·ªÖ trong vi·ªác ƒë·∫°t m·ª•...\n",
      "------------------------------------------------------------\n",
      "\n",
      "[1] üì∞ S·ªë l∆∞·ª£ng h√≥a ƒë∆°n kh·ªüi t·∫°o t·ª´ m√°y t√≠nh ti·ªÅn tƒÉng g·∫•...\n",
      "üè∑Ô∏è  Entity: Ng√†nh c√¥ng ngh·ªá Vi·ªát Nam (entity)\n",
      "‚úÖ Impact: POSITIVE\n",
      "üîÑ Iteration: 1\n",
      "üìù Nhu c·∫ßu v·ªÅ c√°c gi·∫£i ph√°p h√≥a ƒë∆°n ƒëi·ªán t·ª≠ t·ª´ m√°y t√≠nh ti·ªÅn tƒÉng l√™n, th√∫c ƒë·∫©y c√°c doanh nghi·ªáp c√¥ng n...\n",
      "------------------------------------------------------------\n",
      "\n",
      "[1] üì∞ S·ªë l∆∞·ª£ng h√≥a ƒë∆°n kh·ªüi t·∫°o t·ª´ m√°y t√≠nh ti·ªÅn tƒÉng g·∫•...\n",
      "üè∑Ô∏è  Entity: Doanh nghi·ªáp b√°n l·∫ª Vi·ªát Nam (entity)\n",
      "‚úÖ Impact: POSITIVE\n",
      "üîÑ Iteration: 1\n",
      "üìù C√≥ th·ªÉ c·∫£i thi·ªán hi·ªáu qu·∫£ qu·∫£n l√Ω, minh b·∫°ch h√≥a giao d·ªãch v√† gi·∫£m thi·ªÉu sai s√≥t nh·ªù vi·ªác √°p d·ª•ng h√≥...\n",
      "------------------------------------------------------------\n",
      "\n",
      "[1] üì∞ S·ªë l∆∞·ª£ng h√≥a ƒë∆°n kh·ªüi t·∫°o t·ª´ m√°y t√≠nh ti·ªÅn tƒÉng g·∫•...\n",
      "üè∑Ô∏è  Entity: Ng∆∞·ªùi ti√™u d√πng Vi·ªát Nam (entity)\n",
      "‚úÖ Impact: POSITIVE\n",
      "üîÑ Iteration: 1\n",
      "üìù C√≥ th·ªÉ nh·∫≠n ƒë∆∞·ª£c h√≥a ƒë∆°n ch√≠nh x√°c v√† minh b·∫°ch h∆°n, tƒÉng c∆∞·ªùng ni·ªÅm tin v√†o c√°c giao d·ªãch mua s·∫Øm....\n",
      "------------------------------------------------------------\n",
      "\n",
      "[1] üì∞ S·ªë l∆∞·ª£ng h√≥a ƒë∆°n kh·ªüi t·∫°o t·ª´ m√°y t√≠nh ti·ªÅn tƒÉng g·∫•...\n",
      "üè∑Ô∏è  Entity: C∆° quan thu·∫ø Vi·ªát Nam (entity)\n",
      "‚úÖ Impact: POSITIVE\n",
      "üîÑ Iteration: 1\n",
      "üìù C√≥ th·ªÉ g·∫∑p kh√≥ khƒÉn trong vi·ªác qu·∫£n l√Ω v√† thu th·∫≠p d·ªØ li·ªáu thu·∫ø do t·ª∑ l·ªá √°p d·ª•ng h√≥a ƒë∆°n ƒëi·ªán t·ª≠ c√≤n...\n",
      "------------------------------------------------------------\n",
      "\n",
      "[1] üì∞ S·ªë l∆∞·ª£ng h√≥a ƒë∆°n kh·ªüi t·∫°o t·ª´ m√°y t√≠nh ti·ªÅn tƒÉng g·∫•...\n",
      "üè∑Ô∏è  Entity: Doanh nghi·ªáp b√°n l·∫ª Vi·ªát Nam (entity)\n",
      "‚úÖ Impact: POSITIVE\n",
      "üîÑ Iteration: 1\n",
      "üìù C√≥ th·ªÉ ƒë·ªëi m·∫∑t v·ªõi chi ph√≠ ƒë·∫ßu t∆∞ ban ƒë·∫ßu cho h·ªá th·ªëng h√≥a ƒë∆°n ƒëi·ªán t·ª≠, ƒë·∫∑c bi·ªát l√† c√°c doanh nghi·ªáp...\n",
      "------------------------------------------------------------\n",
      "\n",
      "[1] üì∞ S·ªë l∆∞·ª£ng h√≥a ƒë∆°n kh·ªüi t·∫°o t·ª´ m√°y t√≠nh ti·ªÅn tƒÉng g·∫•...\n",
      "üè∑Ô∏è  Entity: Ng√†nh b√°n l·∫ª Vi·ªát Nam (entity)\n",
      "‚úÖ Impact: POSITIVE\n",
      "üîÑ Iteration: 1\n",
      "üìù S·ª± ch·∫≠m tr·ªÖ trong chuy·ªÉn ƒë·ªïi s·ªë c√≥ th·ªÉ khi·∫øn ng√†nh k√©m c·∫°nh tranh h∆°n so v·ªõi c√°c th·ªã tr∆∞·ªùng ph√°t tri...\n",
      "------------------------------------------------------------\n",
      "\n",
      "[1] üì∞ S·ªë l∆∞·ª£ng h√≥a ƒë∆°n kh·ªüi t·∫°o t·ª´ m√°y t√≠nh ti·ªÅn tƒÉng g·∫•...\n",
      "üè∑Ô∏è  Entity: Ng√†nh b√°n l·∫ª Vi·ªát Nam (entity)\n",
      "‚úÖ Impact: POSITIVE\n",
      "üîÑ Iteration: 1\n",
      "üìù C√°c doanh nghi·ªáp c√¥ng ngh·ªá cung c·∫•p gi·∫£i ph√°p h√≥a ƒë∆°n ƒëi·ªán t·ª≠ t·ª´ m√°y t√≠nh ti·ªÅn c√≥ th·ªÉ ch·ª©ng ki·∫øn s·ª± ...\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "üì¶ GRAPH TUPLES ANALYSIS:\n",
      "‚úì ƒê√£ load 178 tuples t·ª´ graph_tuples_step1.csv\n",
      "\n",
      "üîó Sample tuples (10 ƒë·∫ßu ti√™n):\n",
      "\n",
      "      date                                     source   impact                                  target\n",
      "2025-01-01                      Ng√†nh b√°n l·∫ª Vi·ªát Nam POSITIVE                Ng√†nh c√¥ng ngh·ªá Vi·ªát Nam\n",
      "2025-01-01                 Ng√†nh n√¥ng nghi·ªáp Vi·ªát Nam POSITIVE         Doanh nghi·ªáp c√¥ng ngh·ªá Vi·ªát Nam\n",
      "2025-01-01                 Ng√†nh n√¥ng nghi·ªáp Vi·ªát Nam POSITIVE                 Ng∆∞·ªùi lao ƒë·ªông Vi·ªát Nam\n",
      "2025-01-01                 Ng√†nh n√¥ng nghi·ªáp Vi·ªát Nam POSITIVE                   C∆° quan thu·∫ø Vi·ªát Nam\n",
      "2025-01-01                 Ng√†nh n√¥ng nghi·ªáp Vi·ªát Nam POSITIVE            Doanh nghi·ªáp b√°n l·∫ª Vi·ªát Nam\n",
      "2025-01-01                 Ng√†nh n√¥ng nghi·ªáp Vi·ªát Nam POSITIVE              Ng√†nh c√¥ng nghi·ªáp Vi·ªát Nam\n",
      "2025-01-01                 Ng√†nh n√¥ng nghi·ªáp Vi·ªát Nam POSITIVE              Ng√†nh n√¥ng nghi·ªáp Vi·ªát Nam\n",
      "2025-01-01 Doanh nghi·ªáp c√¥ng ngh·ªá Vi·ªát Nam MWG-B√°n l·∫ª POSITIVE                Ng√†nh Ng√¢n h√†ng Vi·ªát Nam\n",
      "2025-01-01 Doanh nghi·ªáp c√¥ng ngh·ªá Vi·ªát Nam MWG-B√°n l·∫ª POSITIVE            Doanh nghi·ªáp b√°n l·∫ª Vi·ªát Nam\n",
      "2025-01-01                 Ng√†nh n√¥ng nghi·ªáp Vi·ªát Nam POSITIVE Doanh nghi·ªáp b√°n l·∫ª Vi·ªát Nam MWG-B√°n l·∫ª\n",
      "\n",
      "üìä Ph√¢n lo·∫°i theo impact:\n",
      "\n",
      "impact\n",
      "POSITIVE    158\n",
      "NEGATIVE     20\n",
      "Name: count, dtype: int64\n",
      "\n",
      "üìÖ Ph√¢n b·ªë theo ng√†y:\n",
      "\n",
      "date\n",
      "2025-01-01    178\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# PH√ÇN T√çCH K·∫æT QU·∫¢\n",
    "# ============================================================\n",
    "\n",
    "print(f\"üìä T·ªîNG QUAN K·∫æT QU·∫¢\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "print(f\"T·ªïng s·ªë entities tr√≠ch xu·∫•t: {len(entities_df)}\")\n",
    "print(f\"Unique entities: {len(canonical_entities)}\")\n",
    "print(f\"Graph nodes: {len(G.nodes())}\")\n",
    "print(f\"Graph edges: {len(G.edges())}\")\n",
    "\n",
    "print(f\"\\nüìà PH√ÇN LO·∫†I THEO IMPACT:\\n\")\n",
    "print(entities_df['impact'].value_counts())\n",
    "\n",
    "print(f\"\\nüìå PH√ÇN LO·∫†I THEO ENTITY TYPE:\\n\")\n",
    "print(entities_df['entity_type'].value_counts())\n",
    "\n",
    "print(f\"\\nüîÑ PH√ÇN LO·∫†I THEO ITERATION:\\n\")\n",
    "print(entities_df['iteration'].value_counts().sort_index())\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"M·∫™U 10 ENTITIES ƒê·∫¶U TI√äN:\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "for idx, row in entities_df.head(10).iterrows():\n",
    "    print(f\"[{row['article_id']}] üì∞ {row['article_title'][:50]}...\")\n",
    "    print(f\"üè∑Ô∏è  Entity: {row['entity']} ({row['entity_type']})\")\n",
    "    print(f\"{'‚úÖ' if row['impact'] == 'POSITIVE' else '‚ùå'} Impact: {row['impact']}\")\n",
    "    print(f\"üîÑ Iteration: {row['iteration']}\")\n",
    "    print(f\"üìù {row['explanation'][:100]}...\")\n",
    "    print(f\"{'-'*60}\\n\")\n",
    "\n",
    "# Ph√¢n t√≠ch graph tuples\n",
    "print(f\"\\nüì¶ GRAPH TUPLES ANALYSIS:\")\n",
    "try:\n",
    "    tuples_df = pd.read_csv(\"graph_tuples_step1.csv\")\n",
    "    print(f\"‚úì ƒê√£ load {len(tuples_df)} tuples t·ª´ graph_tuples_step1.csv\")\n",
    "    print(f\"\\nüîó Sample tuples (10 ƒë·∫ßu ti√™n):\\n\")\n",
    "    print(tuples_df.head(10).to_string(index=False))\n",
    "    \n",
    "    print(f\"\\nüìä Ph√¢n lo·∫°i theo impact:\\n\")\n",
    "    print(tuples_df['impact'].value_counts())\n",
    "    \n",
    "    print(f\"\\nüìÖ Ph√¢n b·ªë theo ng√†y:\\n\")\n",
    "    print(tuples_df['date'].value_counts().head(10))\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Ch∆∞a c√≥ file graph_tuples_step1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9c143bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî• TOP 15 ENTITIES ƒê∆Ø·ª¢C ƒê·ªÄ C·∫¨P NHI·ªÄU NH·∫§T:\n",
      "\n",
      "============================================================\n",
      "\n",
      " 1. Doanh nghi·ªáp b√°n l·∫ª Vi·ªát Nam\n",
      "    T·ªïng: 132 l·∫ßn | ‚úÖ 127 | ‚ùå 5\n",
      "\n",
      " 2. Ng∆∞·ªùi lao ƒë·ªông Vi·ªát Nam\n",
      "    T·ªïng: 95 l·∫ßn | ‚úÖ 92 | ‚ùå 3\n",
      "\n",
      " 3. Ng∆∞·ªùi ti√™u d√πng Vi·ªát Nam\n",
      "    T·ªïng: 92 l·∫ßn | ‚úÖ 88 | ‚ùå 4\n",
      "\n",
      " 4. Doanh nghi·ªáp c√¥ng ngh·ªá Vi·ªát Nam\n",
      "    T·ªïng: 82 l·∫ßn | ‚úÖ 80 | ‚ùå 2\n",
      "\n",
      " 5. C∆° quan thu·∫ø Vi·ªát Nam\n",
      "    T·ªïng: 73 l·∫ßn | ‚úÖ 69 | ‚ùå 4\n",
      "\n",
      " 6. Ng√†nh c√¥ng nghi·ªáp Vi·ªát Nam\n",
      "    T·ªïng: 58 l·∫ßn | ‚úÖ 58 | ‚ùå 0\n",
      "\n",
      " 7. Ng√†nh b√°n l·∫ª Vi·ªát Nam\n",
      "    T·ªïng: 57 l·∫ßn | ‚úÖ 50 | ‚ùå 7\n",
      "\n",
      " 8. Ng√†nh c√¥ng ngh·ªá Vi·ªát Nam\n",
      "    T·ªïng: 38 l·∫ßn | ‚úÖ 33 | ‚ùå 5\n",
      "\n",
      " 9. Doanh nghi·ªáp b·∫•t ƒë·ªông s·∫£n Vi·ªát Nam\n",
      "    T·ªïng: 26 l·∫ßn | ‚úÖ 26 | ‚ùå 0\n",
      "\n",
      "10. Doanh nghi·ªáp Vi·ªát Nam\n",
      "    T·ªïng: 23 l·∫ßn | ‚úÖ 20 | ‚ùå 3\n",
      "\n",
      "11. Ng√†nh Ng√¢n h√†ng Vi·ªát Nam\n",
      "    T·ªïng: 22 l·∫ßn | ‚úÖ 22 | ‚ùå 0\n",
      "\n",
      "12. Doanh nghi·ªáp x√¢y d·ª±ng Vi·ªát Nam\n",
      "    T·ªïng: 11 l·∫ßn | ‚úÖ 11 | ‚ùå 0\n",
      "\n",
      "13. Ng√†nh b·∫•t ƒë·ªông s·∫£n Vi·ªát Nam\n",
      "    T·ªïng: 10 l·∫ßn | ‚úÖ 5 | ‚ùå 5\n",
      "\n",
      "14. Ng√†nh n√¥ng nghi·ªáp Vi·ªát Nam\n",
      "    T·ªïng: 7 l·∫ßn | ‚úÖ 7 | ‚ùå 0\n",
      "\n",
      "15. VIC-B·∫•t ƒë·ªông s·∫£n\n",
      "    T·ªïng: 2 l·∫ßn | ‚úÖ 1 | ‚ùå 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Xem c√°c entity ƒë∆∞·ª£c ƒë·ªÅ c·∫≠p nhi·ªÅu nh·∫•t\n",
    "from collections import Counter\n",
    "\n",
    "entity_counts = Counter(entities_df['entity'])\n",
    "\n",
    "print(f\"üî• TOP 15 ENTITIES ƒê∆Ø·ª¢C ƒê·ªÄ C·∫¨P NHI·ªÄU NH·∫§T:\\n\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "for i, (entity, count) in enumerate(entity_counts.most_common(15), 1):\n",
    "    # ƒê·∫øm positive v√† negative\n",
    "    pos_count = len(entities_df[(entities_df['entity'] == entity) & (entities_df['impact'] == 'POSITIVE')])\n",
    "    neg_count = len(entities_df[(entities_df['entity'] == entity) & (entities_df['impact'] == 'NEGATIVE')])\n",
    "    \n",
    "    print(f\"{i:2d}. {entity}\")\n",
    "    print(f\"    T·ªïng: {count} l·∫ßn | ‚úÖ {pos_count} | ‚ùå {neg_count}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2d87c6",
   "metadata": {},
   "source": [
    "## üîç T√¨m ki·∫øm v√† ph√¢n t√≠ch\n",
    "\n",
    "### T√¨m th√¥ng tin v·ªÅ m·ªôt entity c·ª• th·ªÉ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ba2a08a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_entity(entity_name, entities_df):\n",
    "    \"\"\"\n",
    "    T√¨m ki·∫øm t·∫•t c·∫£ th√¥ng tin v·ªÅ m·ªôt entity\n",
    "    \"\"\"\n",
    "    # T√¨m ki·∫øm case-insensitive\n",
    "    results = entities_df[entities_df['entity'].str.lower().str.contains(entity_name.lower(), na=False)]\n",
    "    \n",
    "    if len(results) == 0:\n",
    "        print(f\"‚ùå Kh√¥ng t√¨m th·∫•y entity: {entity_name}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"üîç T√åM TH·∫§Y {len(results)} MENTIONS V·ªÄ '{entity_name.upper()}'\\n\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    for idx, row in results.iterrows():\n",
    "        print(f\"[{row['article_id']}] üì∞ {row['article_title']}\")\n",
    "        print(f\"üìÖ {row['date']}\")\n",
    "        print(f\"üè∑Ô∏è  {row['entity']} ({row['entity_type']})\")\n",
    "        print(f\"{'‚úÖ' if row['impact'] == 'POSITIVE' else '‚ùå'} {row['impact']}\")\n",
    "        print(f\"üìù {row['explanation']}\")\n",
    "        print(f\"{'-'*60}\\n\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# V√≠ d·ª•: T√¨m th√¥ng tin v·ªÅ FPT\n",
    "# search_entity(\"FPT\", entities_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "505062d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä PH√ÇN T√çCH KNOWLEDGE GRAPH\n",
      "\n",
      "============================================================\n",
      "\n",
      "T·ªïng s·ªë nodes: 29\n",
      "T·ªïng s·ªë edges: 211\n",
      "\n",
      "üìå Ph√¢n lo·∫°i nodes:\n",
      "   article: 10\n",
      "   entity: 14\n",
      "   stock: 5\n",
      "\n",
      "üî• TOP 10 ENTITIES ƒê∆Ø·ª¢C NH·∫ÆC ƒê·∫æN/·∫¢NH H∆Ø·ªûNG NHI·ªÄU NH·∫§T:\n",
      "\n",
      " 1. [entity] Doanh nghi·ªáp b√°n l·∫ª Vi·ªát Nam: 21 connections\n",
      " 2. [entity] Ng∆∞·ªùi ti√™u d√πng Vi·ªát Nam: 20 connections\n",
      " 3. [entity] Ng√†nh c√¥ng nghi·ªáp Vi·ªát Nam: 20 connections\n",
      " 4. [entity] Ng∆∞·ªùi lao ƒë·ªông Vi·ªát Nam: 19 connections\n",
      " 5. [entity] Ng√†nh b√°n l·∫ª Vi·ªát Nam: 16 connections\n",
      " 6. [entity] C∆° quan thu·∫ø Vi·ªát Nam: 16 connections\n",
      " 7. [entity] Ng√†nh c√¥ng ngh·ªá Vi·ªát Nam: 13 connections\n",
      " 8. [entity] Doanh nghi·ªáp Vi·ªát Nam: 13 connections\n",
      " 9. [entity] Doanh nghi·ªáp c√¥ng ngh·ªá Vi·ªát Nam: 12 connections\n",
      "10. [stock] Doanh nghi·ªáp b√°n l·∫ª Vi·ªát Nam MWG-B√°n l·∫ª: 11 connections\n",
      "\n",
      "üåä TOP 10 ENTITIES ·∫¢NH H∆Ø·ªûNG NHI·ªÄU NH·∫§T ƒê·∫æN ENTITIES KH√ÅC:\n",
      "\n",
      " 1. [entity] C∆° quan thu·∫ø Vi·ªát Nam: 17 connections\n",
      " 2. [entity] Ng∆∞·ªùi lao ƒë·ªông Vi·ªát Nam: 14 connections\n",
      " 3. [entity] Ng∆∞·ªùi ti√™u d√πng Vi·ªát Nam: 13 connections\n",
      " 4. [entity] Ng√†nh c√¥ng nghi·ªáp Vi·ªát Nam: 13 connections\n",
      " 5. [entity] Ng√†nh b√°n l·∫ª Vi·ªát Nam: 12 connections\n",
      " 6. [entity] Ng√†nh c√¥ng ngh·ªá Vi·ªát Nam: 12 connections\n",
      " 7. [entity] Doanh nghi·ªáp c√¥ng ngh·ªá Vi·ªát Nam: 12 connections\n",
      " 8. [entity] Ng√†nh b·∫•t ƒë·ªông s·∫£n Vi·ªát Nam: 12 connections\n",
      " 9. [entity] Doanh nghi·ªáp b√°n l·∫ª Vi·ªát Nam: 11 connections\n",
      "10. [entity] Doanh nghi·ªáp Vi·ªát Nam: 10 connections\n",
      "\n",
      "üìà PH√ÇN T√çCH C√ÅC C·ªî PHI·∫æU TRONG PORTFOLIO:\n",
      "\n",
      "   Doanh nghi·ªáp b√°n l·∫ª Vi·ªát Nam MWG-B√°n l·∫ª:\n",
      "      In: 11 (‚úÖ 10 | ‚ùå 1)\n",
      "      Out: 6\n",
      "   Doanh nghi·ªáp c√¥ng ngh·ªá Vi·ªát Nam MWG-B√°n l·∫ª:\n",
      "      In: 3 (‚úÖ 2 | ‚ùå 1)\n",
      "      Out: 6\n",
      "   VIC-B·∫•t ƒë·ªông s·∫£n:\n",
      "      In: 5 (‚úÖ 2 | ‚ùå 3)\n",
      "      Out: 6\n",
      "   VHM-B·∫•t ƒë·ªông s·∫£n:\n",
      "      In: 4 (‚úÖ 2 | ‚ùå 2)\n",
      "      Out: 6\n",
      "   Doanh nghi·ªáp b·∫•t ƒë·ªông s·∫£n Vi·ªát Nam MWG-B√°n l·∫ª:\n",
      "      In: 0 (‚úÖ 0 | ‚ùå 0)\n",
      "      Out: 3\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# PH√ÇN T√çCH GRAPH - NETWORK STRUCTURE\n",
    "# ============================================================\n",
    "\n",
    "def analyze_graph(G):\n",
    "    \"\"\"\n",
    "    Ph√¢n t√≠ch knowledge graph ƒë√£ x√¢y d·ª±ng (gi·ªëng TRR_model)\n",
    "    \"\"\"\n",
    "    print(f\"üìä PH√ÇN T√çCH KNOWLEDGE GRAPH\\n\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Th·ªëng k√™ c∆° b·∫£n\n",
    "    print(f\"T·ªïng s·ªë nodes: {len(G.nodes())}\")\n",
    "    print(f\"T·ªïng s·ªë edges: {len(G.edges())}\")\n",
    "    \n",
    "    # Ph√¢n lo·∫°i nodes\n",
    "    node_types = {}\n",
    "    for node, data in G.nodes(data=True):\n",
    "        node_type = data.get('type', 'unknown')\n",
    "        node_types[node_type] = node_types.get(node_type, 0) + 1\n",
    "    \n",
    "    print(f\"\\nüìå Ph√¢n lo·∫°i nodes:\")\n",
    "    for ntype, count in node_types.items():\n",
    "        print(f\"   {ntype}: {count}\")\n",
    "    \n",
    "    # T√¨m nodes c√≥ nhi·ªÅu k·∫øt n·ªëi nh·∫•t\n",
    "    entity_nodes = [n for n, d in G.nodes(data=True) if d.get('type') in ['entity', 'stock']]\n",
    "    \n",
    "    if entity_nodes:\n",
    "        # In-degree: s·ªë l∆∞·ª£ng b√†i b√°o/entity li√™n k·∫øt ƒë·∫øn\n",
    "        in_degrees = [(node, G.in_degree(node)) for node in entity_nodes]\n",
    "        in_degrees.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        print(f\"\\nüî• TOP 10 ENTITIES ƒê∆Ø·ª¢C NH·∫ÆC ƒê·∫æN/·∫¢NH H∆Ø·ªûNG NHI·ªÄU NH·∫§T:\\n\")\n",
    "        for i, (node, degree) in enumerate(in_degrees[:10], 1):\n",
    "            node_type = G.nodes[node].get('type', 'unknown')\n",
    "            print(f\"{i:2d}. [{node_type}] {node}: {degree} connections\")\n",
    "        \n",
    "        # Out-degree: s·ªë l∆∞·ª£ng entity m√† node n√†y ·∫£nh h∆∞·ªüng ƒë·∫øn\n",
    "        out_degrees = [(node, G.out_degree(node)) for node in entity_nodes]\n",
    "        out_degrees.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        print(f\"\\nüåä TOP 10 ENTITIES ·∫¢NH H∆Ø·ªûNG NHI·ªÄU NH·∫§T ƒê·∫æN ENTITIES KH√ÅC:\\n\")\n",
    "        for i, (node, degree) in enumerate(out_degrees[:10], 1):\n",
    "            node_type = G.nodes[node].get('type', 'unknown')\n",
    "            print(f\"{i:2d}. [{node_type}] {node}: {degree} connections\")\n",
    "    \n",
    "    # Ph√¢n t√≠ch stock nodes ƒë·∫∑c bi·ªát\n",
    "    stock_nodes = [n for n, d in G.nodes(data=True) if d.get('type') == 'stock']\n",
    "    if stock_nodes:\n",
    "        print(f\"\\nüìà PH√ÇN T√çCH C√ÅC C·ªî PHI·∫æU TRONG PORTFOLIO:\\n\")\n",
    "        for stock in stock_nodes:\n",
    "            in_deg = G.in_degree(stock)\n",
    "            out_deg = G.out_degree(stock)\n",
    "            \n",
    "            # Count positive vs negative impacts\n",
    "            pos_count = len([e for e in G.in_edges(stock, data=True) if e[2].get('impact') == 'POSITIVE'])\n",
    "            neg_count = len([e for e in G.in_edges(stock, data=True) if e[2].get('impact') == 'NEGATIVE'])\n",
    "            \n",
    "            print(f\"   {stock}:\")\n",
    "            print(f\"      In: {in_deg} (‚úÖ {pos_count} | ‚ùå {neg_count})\")\n",
    "            print(f\"      Out: {out_deg}\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "\n",
    "# Ch·∫°y ph√¢n t√≠ch\n",
    "analyze_graph(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b70144",
   "metadata": {},
   "source": [
    "## üíæ L∆∞u Graph ƒë·ªÉ s·ª≠ d·ª•ng sau\n",
    "\n",
    "L∆∞u graph v√†o file pickle ƒë·ªÉ s·ª≠ d·ª•ng cho c√°c b∆∞·ªõc ti·∫øp theo (relation extraction, attention mechanism...):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cbcc08b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ C√ÅC FILES ƒê√É ƒê∆Ø·ª¢C L∆ØU T·ª∞ ƒê·ªòNG:\n",
      "\n",
      "1. entities_extracted.csv - DataFrame ch·ª©a t·∫•t c·∫£ entities v·ªõi iteration info\n",
      "2. knowledge_graph.pkl - NetworkX DiGraph (c·∫•u tr√∫c y h·ªát TRR)\n",
      "3. canonical_entities.pkl - Set c√°c entity canonical\n",
      "4. graph_tuples_step1.csv - CSV tuples (date, source, impact, target)\n",
      "\n",
      "============================================================\n",
      "üìñ C√ÅCH LOAD L·∫†I C√ÅC FILES:\n",
      "\n",
      "\n",
      "# Load entities DataFrame\n",
      "import pandas as pd\n",
      "entities_df = pd.read_csv('entities_extracted.csv')\n",
      "\n",
      "# Load NetworkX graph\n",
      "import pickle\n",
      "with open('knowledge_graph.pkl', 'rb') as f:\n",
      "    G = pickle.load(f)\n",
      "\n",
      "# Load canonical entities\n",
      "with open('canonical_entities.pkl', 'rb') as f:\n",
      "    canonical_entities = pickle.load(f)\n",
      "\n",
      "# Load graph tuples\n",
      "tuples_df = pd.read_csv('graph_tuples_step1.csv')\n",
      "\n",
      "============================================================\n",
      "üîó C·∫§U TR√öC GRAPH TUPLES CSV:\n",
      "\n",
      "Columns: date, source, impact, target\n",
      "Format: YYYY-MM-DD, Entity_Name, POSITIVE/NEGATIVE, Target_Entity\n",
      "\n",
      "V√≠ d·ª•:\n",
      "2025-01-01, FPT-C√¥ng ngh·ªá, POSITIVE TO, Ng√†nh c√¥ng ngh·ªá Vi·ªát Nam\n",
      "2025-01-01, Ng√†nh c√¥ng ngh·ªá Vi·ªát Nam, NEGATIVE TO, Ng√†nh s·∫£n xu·∫•t Vi·ªát Nam\n",
      "\n",
      "============================================================\n",
      "‚ú® GRAPH STRUCTURE HO√ÄN CH·ªàNH GI·ªêNG TRR_MODEL!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ƒê√É L∆ØU C√ÅC FILES - H∆Ø·ªöNG D·∫™N S·ª¨ D·ª§NG\n",
    "# ============================================================\n",
    "\n",
    "print(\"‚úÖ C√ÅC FILES ƒê√É ƒê∆Ø·ª¢C L∆ØU T·ª∞ ƒê·ªòNG:\\n\")\n",
    "print(\"1. entities_extracted.csv - DataFrame ch·ª©a t·∫•t c·∫£ entities v·ªõi iteration info\")\n",
    "print(\"2. knowledge_graph.pkl - NetworkX DiGraph (c·∫•u tr√∫c y h·ªát TRR)\")\n",
    "print(\"3. canonical_entities.pkl - Set c√°c entity canonical\")\n",
    "print(\"4. graph_tuples_step1.csv - CSV tuples (date, source, impact, target)\\n\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üìñ C√ÅCH LOAD L·∫†I C√ÅC FILES:\\n\")\n",
    "\n",
    "print(\"\"\"\n",
    "# Load entities DataFrame\n",
    "import pandas as pd\n",
    "entities_df = pd.read_csv('entities_extracted.csv')\n",
    "\n",
    "# Load NetworkX graph\n",
    "import pickle\n",
    "with open('knowledge_graph.pkl', 'rb') as f:\n",
    "    G = pickle.load(f)\n",
    "\n",
    "# Load canonical entities\n",
    "with open('canonical_entities.pkl', 'rb') as f:\n",
    "    canonical_entities = pickle.load(f)\n",
    "\n",
    "# Load graph tuples\n",
    "tuples_df = pd.read_csv('graph_tuples_step1.csv')\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üîó C·∫§U TR√öC GRAPH TUPLES CSV:\\n\")\n",
    "print(\"Columns: date, source, impact, target\")\n",
    "print(\"Format: YYYY-MM-DD, Entity_Name, POSITIVE/NEGATIVE, Target_Entity\")\n",
    "print(\"\\nV√≠ d·ª•:\")\n",
    "print(\"2025-01-01, FPT-C√¥ng ngh·ªá, POSITIVE TO, Ng√†nh c√¥ng ngh·ªá Vi·ªát Nam\")\n",
    "print(\"2025-01-01, Ng√†nh c√¥ng ngh·ªá Vi·ªát Nam, NEGATIVE TO, Ng√†nh s·∫£n xu·∫•t Vi·ªát Nam\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚ú® GRAPH STRUCTURE HO√ÄN CH·ªàNH GI·ªêNG TRR_MODEL!\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
