{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "734e43e5",
   "metadata": {},
   "source": [
    "# ğŸ¤– Entity Extractor - TRR Step 1: TrÃ­ch xuáº¥t thá»±c thá»ƒ\n",
    "\n",
    "## Má»¥c tiÃªu:\n",
    "TrÃ­ch xuáº¥t cÃ¡c thá»±c thá»ƒ (mÃ£ cá»• phiáº¿u, ngÃ nh nghá», cÃ´ng ty, thá»‹ trÆ°á»ng) tá»« tin tá»©c Ä‘Ã£ tÃ³m táº¯t.\n",
    "\n",
    "## Input:\n",
    "- File CSV: `summarized_news_with_stocks_merged.csv`\n",
    "- Columns: `postID, stockCodes, title, description, date`\n",
    "\n",
    "## Output:\n",
    "- `entities_extracted.csv`: DataFrame chá»©a cÃ¡c entity Ä‘Æ°á»£c trÃ­ch xuáº¥t\n",
    "- `knowledge_graph.pkl`: NetworkX DiGraph\n",
    "- `canonical_entities.pkl`: Set cÃ¡c entity canonical\n",
    "- `graph_tuples_step1.csv`: Tuples (date, source, impact, target) Ä‘á»ƒ phÃ¢n tÃ­ch\n",
    "\n",
    "## Prompt Strategy:\n",
    "- **Focus mÃ£ cá»• phiáº¿u:** Æ¯u tiÃªn 10 mÃ£ trong portfolio\n",
    "- **Focus ngÃ nh VN:** Ghi rÃµ \"NgÃ nh [tÃªn] Viá»‡t Nam\"\n",
    "- **Focus thá»‹ trÆ°á»ng:** PhÃ¢n biá»‡t VN/nÆ°á»›c ngoÃ i\n",
    "- **Chi tiáº¿t entity:** Báº¯t buá»™c trÃ­ch dáº«n sá»‘ liá»‡u cá»¥ thá»ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c679e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ ÄÃ£ import cÃ¡c thÆ° viá»‡n cáº§n thiáº¿t\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "import google.generativeai as genai\n",
    "import networkx as nx\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "print(\"âœ“ ÄÃ£ import cÃ¡c thÆ° viá»‡n cáº§n thiáº¿t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68c32e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ ÄÃ£ Ä‘á»‹nh nghÄ©a APIKeyManager\n",
      "âœ“ PhÃ¡t hiá»‡n 4 API keys\n",
      "ğŸ”‘ Äang sá»­ dá»¥ng: GOOGLE_API_KEY (Key 1/4)\n"
     ]
    }
   ],
   "source": [
    "class APIKeyManager:\n",
    "    \"\"\"\n",
    "    Quáº£n lÃ½ nhiá»u Google API keys vÃ  tá»± Ä‘á»™ng chuyá»ƒn Ä‘á»•i khi gáº·p lá»—i.\n",
    "    \n",
    "    Quy táº¯c:\n",
    "    - Má»—i key Ä‘Æ°á»£c thá»­ tá»‘i Ä‘a 2 láº§n\n",
    "    - Sau 2 láº§n lá»—i â†’ tá»± Ä‘á»™ng chuyá»ƒn key tiáº¿p theo\n",
    "    - Háº¿t key â†’ bÃ¡o lá»—i\n",
    "    \"\"\"\n",
    "    \n",
    "    MAX_RETRIES_PER_KEY = 2  # Sá»‘ láº§n thá»­ tá»‘i Ä‘a cho má»—i key\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Khá»Ÿi táº¡o vÃ  load táº¥t cáº£ API keys tá»« .env\"\"\"\n",
    "        # Load cÃ¡c API keys tá»« environment\n",
    "        self.keys = [\n",
    "            (\"GOOGLE_API_KEY\", os.getenv(\"GOOGLE_API_KEY\")),\n",
    "            (\"GOOGLE_API_KEY_2\", os.getenv(\"GOOGLE_API_KEY_2\")),\n",
    "            (\"GOOGLE_API_KEY_3\", os.getenv(\"GOOGLE_API_KEY_3\")),\n",
    "            (\"GOOGLE_API_KEY_4\", os.getenv(\"GOOGLE_API_KEY_4\")),\n",
    "        ]\n",
    "        \n",
    "        # Chá»‰ giá»¯ láº¡i cÃ¡c key há»£p lá»‡ (khÃ´ng None)\n",
    "        self.keys = [(name, key) for name, key in self.keys if key]\n",
    "        \n",
    "        if not self.keys:\n",
    "            raise ValueError(\"âŒ KhÃ´ng tÃ¬m tháº¥y API key! Kiá»ƒm tra file .env\")\n",
    "        \n",
    "        # Khá»Ÿi táº¡o tráº¡ng thÃ¡i\n",
    "        self.current_index = 0\n",
    "        self.error_counts = {name: 0 for name, _ in self.keys}  # Äáº¿m lá»—i má»—i key\n",
    "        \n",
    "        print(f\"âœ“ PhÃ¡t hiá»‡n {len(self.keys)} API keys\")\n",
    "        self._activate_key(0)\n",
    "    \n",
    "    def _activate_key(self, index):\n",
    "        \"\"\"KÃ­ch hoáº¡t API key táº¡i vá»‹ trÃ­ index\"\"\"\n",
    "        if index >= len(self.keys):\n",
    "            raise Exception(\"âŒ ÄÃ£ háº¿t táº¥t cáº£ API keys!\")\n",
    "        \n",
    "        self.current_index = index\n",
    "        key_name, key_value = self.keys[index]\n",
    "        \n",
    "        # Cáº¥u hÃ¬nh Google AI vá»›i key má»›i\n",
    "        genai.configure(api_key=key_value)\n",
    "        \n",
    "        print(f\"ğŸ”‘ Äang sá»­ dá»¥ng: {key_name} (Key {index + 1}/{len(self.keys)})\")\n",
    "    \n",
    "    def get_current_key(self):\n",
    "        \"\"\"Láº¥y API key hiá»‡n táº¡i\"\"\"\n",
    "        return self.keys[self.current_index][1]\n",
    "    \n",
    "    def get_models(self):\n",
    "        \"\"\"\n",
    "        Táº¡o cÃ¡c model AI vá»›i API key hiá»‡n táº¡i.\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (model, model_more_temp, model_pro)\n",
    "        \"\"\"\n",
    "        current_key = self.get_current_key()\n",
    "        \n",
    "        model = ChatGoogleGenerativeAI(\n",
    "            model=\"gemini-2.0-flash-lite\", \n",
    "            temperature=0.02,\n",
    "            google_api_key=current_key\n",
    "        )\n",
    "        \n",
    "        model_more_temp = ChatGoogleGenerativeAI(\n",
    "            model=\"gemini-2.0-flash-lite\", \n",
    "            temperature=0.1,\n",
    "            google_api_key=current_key\n",
    "        )\n",
    "        \n",
    "        model_pro = ChatGoogleGenerativeAI(\n",
    "            model=\"gemini-2.5-pro-exp-03-25\", \n",
    "            temperature=0.1,\n",
    "            google_api_key=current_key\n",
    "        )\n",
    "        \n",
    "        return model, model_more_temp, model_pro\n",
    "    \n",
    "    def on_error(self):\n",
    "        \"\"\"\n",
    "        Xá»­ lÃ½ khi gáº·p lá»—i API.\n",
    "        \n",
    "        Returns:\n",
    "            bool: True náº¿u cÃ²n key khÃ¡c Ä‘á»ƒ thá»­, False náº¿u háº¿t key\n",
    "        \"\"\"\n",
    "        key_name = self.keys[self.current_index][0]\n",
    "        self.error_counts[key_name] += 1\n",
    "        \n",
    "        error_count = self.error_counts[key_name]\n",
    "        print(f\"âš  Lá»—i láº§n {error_count} vá»›i {key_name}\")\n",
    "        \n",
    "        # Náº¿u Ä‘Ã£ Ä‘áº¡t giá»›i háº¡n retry cho key nÃ y\n",
    "        if error_count >= self.MAX_RETRIES_PER_KEY:\n",
    "            print(f\"â›” {key_name} Ä‘Ã£ lá»—i {error_count}/{self.MAX_RETRIES_PER_KEY} láº§n\")\n",
    "            \n",
    "            # Thá»­ chuyá»ƒn sang key tiáº¿p theo\n",
    "            next_index = self.current_index + 1\n",
    "            if next_index < len(self.keys):\n",
    "                print(f\"ğŸ”„ Chuyá»ƒn sang key tiáº¿p theo...\")\n",
    "                self._activate_key(next_index)\n",
    "                return True\n",
    "            else:\n",
    "                print(f\"âŒ ÄÃ£ háº¿t táº¥t cáº£ {len(self.keys)} keys!\")\n",
    "                return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def reset_errors(self):\n",
    "        \"\"\"Reset error counters cho táº¥t cáº£ keys\"\"\"\n",
    "        self.error_counts = {name: 0 for name, _ in self.keys}\n",
    "        print(\"â™» ÄÃ£ reset error counters\")\n",
    "\n",
    "print(\"âœ“ ÄÃ£ Ä‘á»‹nh nghÄ©a APIKeyManager\")\n",
    "\n",
    "# Khá»Ÿi táº¡o API Manager\n",
    "api_manager = APIKeyManager()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851c0369",
   "metadata": {},
   "source": [
    "## ğŸ“ Prompt Templates\n",
    "\n",
    "### Entity Extraction Prompt:\n",
    "- **ÄÃ£ cáº£i tiáº¿n:** Bá» trÆ°á»ng `{group}` khÃ´ng tá»“n táº¡i trong dataset\n",
    "- **ThÃªm trÆ°á»ng:** `{stockCodes}` - mÃ£ cá»• phiáº¿u tá»« dataset\n",
    "- **Focus:** MÃ£ CP trong portfolio, ngÃ nh VN, thá»‹ trÆ°á»ng VN/ngoáº¡i\n",
    "- **Chi tiáº¿t:** Báº¯t buá»™c trÃ­ch dáº«n sá»‘ liá»‡u cá»¥ thá»ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af71d02d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ ÄÃ£ Ä‘á»‹nh nghÄ©a entity_extraction_template\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# PROMPT TEMPLATES - TrÃ­ch xuáº¥t thá»±c thá»ƒ vÃ  má»‘i quan há»‡\n",
    "# ============================================================\n",
    "\n",
    "# Template trÃ­ch xuáº¥t thá»±c thá»ƒ tá»« tin tá»©c\n",
    "entity_extraction_template = PromptTemplate.from_template(\"\"\"Báº¡n Ä‘ang phÃ¢n tÃ­ch tin tá»©c kinh táº¿ Viá»‡t Nam Ä‘á»ƒ dá»± Ä‘oÃ¡n biáº¿n Ä‘á»™ng thá»‹ trÆ°á»ng chá»©ng khoÃ¡n.\n",
    "Báº¡n Ä‘Æ°á»£c cho má»™t bÃ i bÃ¡o vá»›i tá»±a Ä‘á», mÃ´ táº£ ngáº¯n gá»n, vÃ  ngÃ y xuáº¥t báº£n.\n",
    "\n",
    "**Má»¥c tiÃªu:** TÃ¬m cÃ¡c thá»±c thá»ƒ (mÃ£ cá»• phiáº¿u, ngÃ nh nghá», cÃ´ng ty, thá»‹ trÆ°á»ng, quá»‘c gia, tá»‰nh thÃ nh) bá»‹ áº£nh hÆ°á»Ÿng trá»±c tiáº¿p bá»Ÿi tin tá»©c, theo hÆ°á»›ng TÃCH Cá»°C hoáº·c TIÃŠU Cá»°C.\n",
    "\n",
    "**NguyÃªn táº¯c Ä‘áº·t tÃªn thá»±c thá»ƒ:**\n",
    "1. **Æ¯u tiÃªn tá»‘i Ä‘a cÃ¡c mÃ£ cá»• phiáº¿u trong danh má»¥c:** {portfolio}\n",
    "   - Náº¿u tin tá»©c Ä‘á» cáº­p cÃ´ng ty/ngÃ nh cá»§a cÃ¡c mÃ£ nÃ y, Báº®T BUá»˜C ghi rÃµ mÃ£ cá»• phiáº¿u\n",
    "   - VÃ­ dá»¥: \"FPT-CÃ´ng nghá»‡\", \"VCB-NgÃ¢n hÃ ng\", \"HPG-ThÃ©p\"\n",
    "   \n",
    "2. **Vá»›i ngÃ nh nghá» Viá»‡t Nam:** Ghi rÃµ \"NgÃ nh [tÃªn ngÃ nh] Viá»‡t Nam\"\n",
    "   - VÃ­ dá»¥: \"NgÃ nh bÃ¡n láº» Viá»‡t Nam\", \"NgÃ nh cÃ´ng nghá»‡ Viá»‡t Nam\", \"NgÃ nh thÃ©p Viá»‡t Nam\"\n",
    "   \n",
    "3. **Vá»›i thá»‹ trÆ°á»ng/khu vá»±c:** Ghi rÃµ Ä‘á»‹a danh cá»¥ thá»ƒ\n",
    "   - VÃ­ dá»¥: \"Thá»‹ trÆ°á»ng chá»©ng khoÃ¡n Viá»‡t Nam\", \"Thá»‹ trÆ°á»ng Má»¹\", \"Khu vá»±c ÄÃ´ng Nam Ã\"\n",
    "   \n",
    "4. **Vá»›i cÃ´ng ty khÃ´ng trong portfolio:** Ghi \"CÃ´ng ty [tÃªn]-[ngÃ nh]\"\n",
    "   - VÃ­ dá»¥: \"CÃ´ng ty Meta-CÃ´ng nghá»‡\", \"CÃ´ng ty Vinamilk-TiÃªu dÃ¹ng\"\n",
    "\n",
    "5. **Háº¡n cháº¿ táº¡o má»›i:** Chá»‰ táº¡o tá»‘i Ä‘a 5 thá»±c thá»ƒ. Æ¯u tiÃªn liÃªn káº¿t vá»›i cÃ¡c thá»±c thá»ƒ Ä‘Ã£ cÃ³: {existing_entities}\n",
    "\n",
    "6. **TrÃ¡nh kÃ½ tá»± phá»©c táº¡p:** KhÃ´ng dÃ¹ng dáº¥u cháº¥m, gáº¡ch ngang (trá»« sau mÃ£ cá»• phiáº¿u), dáº¥u &, dáº¥u cháº¥m pháº©y\n",
    "\n",
    "**NguyÃªn táº¯c giáº£i thÃ­ch:**\n",
    "- Báº®T BUá»˜C trÃ­ch dáº«n sá»‘ liá»‡u cá»¥ thá»ƒ tá»« bÃ i bÃ¡o (tÄƒng/giáº£m bao nhiÃªu, gáº¥p máº¥y láº§n, %)\n",
    "- ÄÃ¡nh giÃ¡ tÃ¡c Ä‘á»™ng dá»±a trÃªn dá»¯ liá»‡u thá»±c táº¿, khÃ´ng suy diá»…n quÃ¡ xa\n",
    "- KhÃ´ng tá»± thÃªm sá»‘ liá»‡u khÃ´ng cÃ³ trong bÃ i bÃ¡o\n",
    "- KhÃ´ng dÃ¹ng dáº¥u hai cháº¥m trong pháº§n giáº£i thÃ­ch\n",
    "\n",
    "**Äá»‹nh dáº¡ng output:**\n",
    "[[POSITIVE]]\n",
    "[Entity 1]: [Explanation vá»›i sá»‘ liá»‡u cá»¥ thá»ƒ]\n",
    "...\n",
    "\n",
    "[[NEGATIVE]]\n",
    "[Entity A]: [Explanation vá»›i sá»‘ liá»‡u cá»¥ thá»ƒ]\n",
    "...\n",
    "\n",
    "---\n",
    "\n",
    "**VÃ Dá»¤ MINH Há»ŒA:**\n",
    "\n",
    "(Báº®T Äáº¦U VÃ Dá»¤)\n",
    "\n",
    "NgÃ y Ä‘Äƒng: 2025-01-01T00:00:00+07:00\n",
    "MÃ£ cá»• phiáº¿u liÃªn quan: (khÃ´ng cÃ³)\n",
    "Tá»±a Ä‘á»: Sá»‘ lÆ°á»£ng hÃ³a Ä‘Æ¡n khá»Ÿi táº¡o tá»« mÃ¡y tÃ­nh tiá»n tÄƒng gáº¥p 13 láº§n nÄƒm 2023\n",
    "\n",
    "MÃ´ táº£: Tá»· lá»‡ cÆ¡ sá»Ÿ kinh doanh sá»­ dá»¥ng hÃ³a Ä‘Æ¡n Ä‘iá»‡n tá»­ tÄƒng máº¡nh, vá»›i sá»‘ lÆ°á»£ng hÃ³a Ä‘Æ¡n tá»« mÃ¡y tÃ­nh tiá»n tÄƒng gáº¥p 13 láº§n so vá»›i nÄƒm trÆ°á»›c. NgÃ nh bÃ¡n láº» vÃ  dá»‹ch vá»¥ hÆ°á»Ÿng lá»£i lá»›n tá»« chuyá»ƒn Ä‘á»•i sá»‘ nÃ y.\n",
    "\n",
    "Danh sÃ¡ch thá»±c thá»ƒ sáº½ bá»‹ áº£nh hÆ°á»Ÿng:\n",
    "\n",
    "[[POSITIVE]]\n",
    "NgÃ nh bÃ¡n láº» Viá»‡t Nam: Sá»‘ lÆ°á»£ng hÃ³a Ä‘Æ¡n Ä‘iá»‡n tá»­ tá»« mÃ¡y tÃ­nh tiá»n tÄƒng gáº¥p 13 láº§n trong nÄƒm 2023, giÃºp tÄƒng hiá»‡u quáº£ quáº£n lÃ½ vÃ  giáº£m chi phÃ­ váº­n hÃ nh\n",
    "MWG-BÃ¡n láº»: LÃ  chuá»—i bÃ¡n láº» lá»›n, hÆ°á»Ÿng lá»£i trá»±c tiáº¿p tá»« viá»‡c sá»‘ hÃ³a hÃ³a Ä‘Æ¡n tÄƒng 13 láº§n, cáº£i thiá»‡n kháº£ nÄƒng quáº£n lÃ½ tá»“n kho vÃ  dÃ²ng tiá»n\n",
    "NgÃ nh cÃ´ng nghá»‡ Viá»‡t Nam: Cung cáº¥p giáº£i phÃ¡p hÃ³a Ä‘Æ¡n Ä‘iá»‡n tá»­ vÃ  mÃ¡y tÃ­nh tiá»n cho hÃ ng nghÃ¬n cÆ¡ sá»Ÿ kinh doanh, doanh thu dá»± kiáº¿n tÄƒng máº¡nh\n",
    "FPT-CÃ´ng nghá»‡: LÃ  nhÃ  cung cáº¥p giáº£i phÃ¡p chuyá»ƒn Ä‘á»•i sá»‘ hÃ ng Ä‘áº§u, hÆ°á»Ÿng lá»£i tá»« nhu cáº§u triá»ƒn khai hÃ³a Ä‘Æ¡n Ä‘iá»‡n tá»­ tÄƒng Ä‘á»™t biáº¿n\n",
    "\n",
    "[[NEGATIVE]]\n",
    "(KhÃ´ng cÃ³ thá»±c thá»ƒ bá»‹ áº£nh hÆ°á»Ÿng tiÃªu cá»±c rÃµ rÃ ng tá»« bÃ i bÃ¡o nÃ y)\n",
    "\n",
    "(Káº¾T THÃšC VÃ Dá»¤)\n",
    "\n",
    "---\n",
    "\n",
    "**BÃ€I BÃO Cáº¦N PHÃ‚N TÃCH:**\n",
    "\n",
    "NgÃ y Ä‘Äƒng: {date}\n",
    "MÃ£ cá»• phiáº¿u liÃªn quan: {stockCodes}\n",
    "Tá»±a Ä‘á»: {title}\n",
    "\n",
    "MÃ´ táº£: {description}\n",
    "\n",
    "Danh sÃ¡ch thá»±c thá»ƒ sáº½ bá»‹ áº£nh hÆ°á»Ÿng:\n",
    "\"\"\")\n",
    "\n",
    "print(\"âœ“ ÄÃ£ Ä‘á»‹nh nghÄ©a entity_extraction_template\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0bcc5e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ ÄÃ£ Ä‘á»‹nh nghÄ©a portfolio vÃ  constants\n"
     ]
    }
   ],
   "source": [
    "PORTFOLIO_STOCKS = [\"FPT\", \"SSI\", \"VCB\", \"VHM\", \"HPG\", \"GAS\", \"MSN\", \"MWG\", \"GVR\", \"VIC\"]\n",
    "PORTFOLIO_SECTOR = [\"CÃ´ng nghá»‡\", \"Chá»©ng khoÃ¡n\", \"NgÃ¢n hÃ ng\", \"Báº¥t Ä‘á»™ng sáº£n\", \"Váº­t liá»‡u cÆ¡ báº£n\", \n",
    "                     \"Dá»‹ch vá»¥ Háº¡ táº§ng\", \"TiÃªu dÃ¹ng cÆ¡ báº£n\", \"BÃ¡n láº»\", \"Cháº¿ biáº¿n\", \"Báº¥t Ä‘á»ng sáº£n\"]\n",
    "BASE_DELAY = 30\n",
    "MAX_RETRIES = 3\n",
    "\n",
    "def create_chains(api_manager):\n",
    "    \"\"\"\n",
    "    Táº¡o chains vá»›i models tá»« APIKeyManager\n",
    "    \"\"\"\n",
    "    model, model_more_temp, model_pro = api_manager.get_models()\n",
    "    \n",
    "    # Táº¡o chain trÃ­ch xuáº¥t thá»±c thá»ƒ\n",
    "    chain_entity = entity_extraction_template | model\n",
    "    \n",
    "    return chain_entity\n",
    "\n",
    "print(\"âœ“ ÄÃ£ Ä‘á»‹nh nghÄ©a portfolio vÃ  constants\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c320c01e",
   "metadata": {},
   "source": [
    "## ğŸ› ï¸ Utility Functions\n",
    "\n",
    "CÃ¡c hÃ m há»— trá»£:\n",
    "1. `invoke_chain_with_retry()` - Gá»i API vá»›i retry tá»± Ä‘á»™ng\n",
    "2. `parse_entity_response()` - Parse response tá»« LLM\n",
    "3. `merge_entity()` - Chuáº©n hÃ³a tÃªn entity\n",
    "4. `graph_entities_to_str()` - Convert graph entities thÃ nh string cho prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65b81888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ ÄÃ£ Ä‘á»‹nh nghÄ©a cÃ¡c hÃ m tiá»‡n Ã­ch\n"
     ]
    }
   ],
   "source": [
    "def invoke_chain_with_retry(chain, prompt, api_manager, base_delay=BASE_DELAY):\n",
    "    \"\"\"\n",
    "    Gá»i chain vá»›i cÆ¡ cháº¿ retry tá»± Ä‘á»™ng vÃ  tÃ­ch há»£p APIKeyManager\n",
    "    \"\"\"\n",
    "    total_attempts = 0\n",
    "    max_total_attempts = len(api_manager.keys) * api_manager.MAX_RETRIES_PER_KEY\n",
    "    \n",
    "    while total_attempts < max_total_attempts:\n",
    "        try:\n",
    "            # Thá»­ gá»i API\n",
    "            response = chain.invoke(prompt)\n",
    "            \n",
    "            # ThÃ nh cÃ´ng -> reset error count\n",
    "            api_manager.reset_error_count()\n",
    "            return response\n",
    "            \n",
    "        except Exception as e:\n",
    "            total_attempts += 1\n",
    "            error_msg = str(e)\n",
    "            \n",
    "            # BÃ¡o lá»—i cho API manager\n",
    "            switched = api_manager.on_error()\n",
    "            \n",
    "            if total_attempts >= max_total_attempts:\n",
    "                print(f\"âŒ ÄÃ£ thá»­ táº¥t cáº£ {len(api_manager.keys)} API keys ({total_attempts} láº§n) nhÆ°ng váº«n lá»—i\")\n",
    "                print(f\"   Lá»—i cuá»‘i: {error_msg}\")\n",
    "                return None\n",
    "            \n",
    "            # Chá» trÆ°á»›c khi retry\n",
    "            if switched:\n",
    "                delay = base_delay\n",
    "                print(f\"â³ Äá»£i {delay}s trÆ°á»›c khi thá»­ key má»›i...\")\n",
    "            else:\n",
    "                retry_num = api_manager.error_counts.get(api_manager.current_index, 0)\n",
    "                delay = base_delay * (1.5 ** (retry_num - 1))\n",
    "                print(f\"â³ Äá»£i {delay:.0f}s trÆ°á»›c khi retry ({retry_num}/{api_manager.MAX_RETRIES_PER_KEY})...\")\n",
    "            \n",
    "            time.sleep(delay)\n",
    "\n",
    "def parse_entity_response(response):\n",
    "    \"\"\"\n",
    "    PhÃ¢n tÃ­ch response tá»« entity extraction prompt\n",
    "    \n",
    "    Returns:\n",
    "        dict: {\"POSITIVE\": [(entity, explanation), ...], \"NEGATIVE\": [(entity, explanation), ...]}\n",
    "    \"\"\"\n",
    "    if response is None:\n",
    "        print(\"Response is None\")\n",
    "        return {\"POSITIVE\": [], \"NEGATIVE\": []}\n",
    "        \n",
    "    sections = {\"POSITIVE\": [], \"NEGATIVE\": []}\n",
    "    current_section = None\n",
    "    str_resp = response.content\n",
    "    \n",
    "    for line in str(str_resp).splitlines():\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        if \"[[POSITIVE]]\" in line.upper():\n",
    "            current_section = \"POSITIVE\"\n",
    "            continue\n",
    "        if \"[[NEGATIVE]]\" in line.upper():\n",
    "            current_section = \"NEGATIVE\"\n",
    "            continue\n",
    "        if current_section and ':' in line:\n",
    "            entity = line.split(\":\", 1)[0].strip()\n",
    "            # Skip invalid entities\n",
    "            if not entity or \"khÃ´ng cÃ³ thá»±c thá»ƒ nÃ o\" in entity.lower():\n",
    "                continue\n",
    "            # content = all line except entity\n",
    "            content = line.split(entity, 1)[-1].strip(':').strip()\n",
    "            sections[current_section].append((entity, content))\n",
    "\n",
    "    return sections\n",
    "\n",
    "def merge_entity(entity, canonical_set):\n",
    "    \"\"\"\n",
    "    Tráº£ vá» phiÃªn báº£n canonical cá»§a entity náº¿u Ä‘Ã£ tá»“n táº¡i (case-insensitive),\n",
    "    náº¿u khÃ´ng thÃ¬ thÃªm vÃ  tráº£ vá» entity má»›i.\n",
    "    \"\"\"\n",
    "    normalized_entity = str(entity).strip('[').strip(']').strip(' ').lower()\n",
    "    for exist in canonical_set:\n",
    "        if exist.lower() == normalized_entity:\n",
    "            return exist\n",
    "    canonical_set.add(normalized_entity)\n",
    "    return normalized_entity\n",
    "\n",
    "def graph_entities_to_str(G, max_entities=50):\n",
    "    \"\"\"\n",
    "    Chuyá»ƒn Ä‘á»•i cÃ¡c entities trong graph thÃ nh chuá»—i Ä‘á»ƒ Ä‘Æ°a vÃ o prompt\n",
    "    \"\"\"\n",
    "    entities = [node for node in G.nodes() if not node.startswith(\"Article_\")]\n",
    "    # Giá»›i háº¡n sá»‘ lÆ°á»£ng Ä‘á»ƒ khÃ´ng lÃ m prompt quÃ¡ dÃ i\n",
    "    entities = entities[:max_entities]\n",
    "    return \", \".join(entities) if entities else \"ChÆ°a cÃ³ thá»±c thá»ƒ nÃ o\"\n",
    "\n",
    "print(\"âœ“ ÄÃ£ Ä‘á»‹nh nghÄ©a cÃ¡c hÃ m tiá»‡n Ã­ch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf93f87",
   "metadata": {},
   "source": [
    "## ğŸ¯ Main Function: Extract Entities from News\n",
    "\n",
    "HÃ m chÃ­nh trÃ­ch xuáº¥t entities tá»« CSV tin tá»©c Ä‘Ã£ tÃ³m táº¯t.\n",
    "\n",
    "**Features:**\n",
    "- âœ… Validation dataset trÆ°á»›c khi cháº¡y\n",
    "- âœ… Error handling vá»›i thÃ´ng bÃ¡o chi tiáº¿t\n",
    "- âœ… Export to tuples CSV cho phÃ¢n tÃ­ch\n",
    "- âœ… Progress tracking vÃ  logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bec082aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ ÄÃ£ Ä‘á»‹nh nghÄ©a hÃ m extract_entities_from_news()\n"
     ]
    }
   ],
   "source": [
    "def extract_entities_from_news(\n",
    "    csv_path=\"summarized_news_with_stocks_merged.csv\",\n",
    "    output_path=\"entities_extracted.csv\",\n",
    "    start_date=None,\n",
    "    end_date=None,\n",
    "    max_articles=None\n",
    "):\n",
    "    \"\"\"\n",
    "    TrÃ­ch xuáº¥t thá»±c thá»ƒ tá»« tin tá»©c Ä‘Ã£ tÃ³m táº¯t\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    csv_path : str\n",
    "        ÄÆ°á»ng dáº«n Ä‘áº¿n file CSV chá»©a tin tá»©c Ä‘Ã£ tÃ³m táº¯t\n",
    "    output_path : str\n",
    "        ÄÆ°á»ng dáº«n file CSV káº¿t quáº£\n",
    "    start_date : str, optional\n",
    "        NgÃ y báº¯t Ä‘áº§u (format: YYYY-MM-DD)\n",
    "    end_date : str, optional\n",
    "        NgÃ y káº¿t thÃºc (format: YYYY-MM-DD)\n",
    "    max_articles : int, optional\n",
    "        Sá»‘ lÆ°á»£ng bÃ i bÃ¡o tá»‘i Ä‘a Ä‘á»ƒ xá»­ lÃ½ (Ä‘á»ƒ test)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple: (entities_df, graph, canonical_entities)\n",
    "        - entities_df: DataFrame chá»©a cÃ¡c thá»±c thá»ƒ Ä‘Ã£ trÃ­ch xuáº¥t\n",
    "        - graph: NetworkX graph chá»©a má»‘i quan há»‡\n",
    "        - canonical_entities: Set cÃ¡c thá»±c thá»ƒ canonical\n",
    "    \"\"\"\n",
    "    print(f\"ğŸ“– Äang Ä‘á»c dá»¯ liá»‡u tá»« {csv_path}...\")\n",
    "    \n",
    "    # Äá»c dá»¯ liá»‡u tin tá»©c Ä‘Ã£ tÃ³m táº¯t vá»›i error handling\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        print(f\"âœ“ ÄÃ£ Ä‘á»c {len(df)} tin tá»©c\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"âŒ KhÃ´ng tÃ¬m tháº¥y file: {csv_path}\")\n",
    "        print(f\"ğŸ’¡ CÃ¡c file CSV cÃ³ sáºµn:\")\n",
    "        csv_files = [f for f in os.listdir('.') if f.endswith('.csv') and 'summarized' in f]\n",
    "        for f in csv_files:\n",
    "            print(f\"   - {f}\")\n",
    "        raise\n",
    "    \n",
    "    # Validate required columns\n",
    "    required_cols = ['postID', 'stockCodes', 'title', 'description', 'date']\n",
    "    missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        print(f\"âŒ Dataset thiáº¿u cÃ¡c cá»™t: {missing_cols}\")\n",
    "        print(f\"ğŸ“‹ CÃ¡c cá»™t hiá»‡n cÃ³: {list(df.columns)}\")\n",
    "        raise ValueError(f\"Dataset khÃ´ng há»£p lá»‡: thiáº¿u cá»™t {missing_cols}\")\n",
    "    \n",
    "    print(f\"âœ“ Dataset há»£p lá»‡ vá»›i cÃ¡c cá»™t: {list(df.columns)}\")\n",
    "    \n",
    "    # Chuyá»ƒn Ä‘á»•i cá»™t date sang datetime\n",
    "    df['parsed_date'] = pd.to_datetime(df['date'])\n",
    "    df['only_date'] = df['parsed_date'].dt.date\n",
    "    \n",
    "    # Lá»c theo khoáº£ng thá»i gian\n",
    "    if start_date:\n",
    "        start_dt = pd.to_datetime(start_date).date()\n",
    "        df = df[df['only_date'] >= start_dt]\n",
    "        print(f\"âœ“ Lá»c tá»« ngÃ y {start_date}: cÃ²n {len(df)} tin\")\n",
    "    \n",
    "    if end_date:\n",
    "        end_dt = pd.to_datetime(end_date).date()\n",
    "        df = df[df['only_date'] <= end_dt]\n",
    "        print(f\"âœ“ Lá»c Ä‘áº¿n ngÃ y {end_date}: cÃ²n {len(df)} tin\")\n",
    "    \n",
    "    # Giá»›i háº¡n sá»‘ lÆ°á»£ng náº¿u cáº§n (Ä‘á»ƒ test)\n",
    "    if max_articles:\n",
    "        df = df.head(max_articles)\n",
    "        print(f\"âœ“ Giá»›i háº¡n xuá»‘ng {len(df)} tin Ä‘á»ƒ xá»­ lÃ½\")\n",
    "    \n",
    "    # Sáº¯p xáº¿p theo thá»i gian\n",
    "    df = df.sort_values('date')\n",
    "    \n",
    "    # Khá»Ÿi táº¡o graph vÃ  canonical entities\n",
    "    G = nx.DiGraph()\n",
    "    canonical_entities = set()\n",
    "    \n",
    "    # Build portfolio string\n",
    "    portfolio_str_full = \", \".join([f\"{stock}-{sector}\" for stock, sector in zip(PORTFOLIO_STOCKS, PORTFOLIO_SECTOR)])\n",
    "    \n",
    "    # Khá»Ÿi táº¡o chain vá»›i API manager\n",
    "    chain_entity = create_chains(api_manager)\n",
    "    \n",
    "    # Káº¿t quáº£\n",
    "    all_entities = []\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ğŸ” Báº®T Äáº¦U TRÃCH XUáº¤T THá»°C THá»‚\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Xá»­ lÃ½ tá»«ng bÃ i bÃ¡o\n",
    "    for idx, row in df.iterrows():\n",
    "        article_idx = idx + 1\n",
    "        article_node = f\"Article_{article_idx}: {row['title']}\"\n",
    "        article_timestamp = row['parsed_date']\n",
    "        \n",
    "        # ThÃªm node bÃ i bÃ¡o vÃ o graph\n",
    "        if not G.has_node(article_node):\n",
    "            G.add_node(article_node, type=\"article\", timestamp=article_timestamp)\n",
    "        \n",
    "        print(f\"[{article_idx}/{len(df)}] ğŸ“° {row['title'][:60]}...\")\n",
    "        \n",
    "        # Láº¥y thÃ´ng tin mÃ£ cá»• phiáº¿u tá»« dataset\n",
    "        stock_codes = row.get('stockCodes', '')\n",
    "        if not stock_codes or pd.isna(stock_codes):\n",
    "            stock_codes = \"(khÃ´ng cÃ³)\"\n",
    "        \n",
    "        # Phase 1: Extract initial entities\n",
    "        max_entity_retries = MAX_RETRIES\n",
    "        entity_retry_count = 0\n",
    "        entities_dict = {\"POSITIVE\": [], \"NEGATIVE\": []}\n",
    "        \n",
    "        while entity_retry_count < max_entity_retries:\n",
    "            prompt_text = {\n",
    "                \"portfolio\": portfolio_str_full,\n",
    "                \"date\": row['date'],\n",
    "                \"stockCodes\": stock_codes,\n",
    "                \"title\": row['title'],\n",
    "                \"description\": row['description'],\n",
    "                \"existing_entities\": graph_entities_to_str(G)\n",
    "            }\n",
    "            \n",
    "            response_text = invoke_chain_with_retry(chain_entity, prompt_text, api_manager)\n",
    "            time.sleep(1)  # Rate limiting\n",
    "            \n",
    "            if response_text is None:\n",
    "                print(f\"   âŒ Bá» qua tin {article_idx} do lá»—i API\")\n",
    "                break\n",
    "            \n",
    "            entities_dict = parse_entity_response(response_text)\n",
    "            \n",
    "            # Check if we got any entities\n",
    "            total_entities = len(entities_dict.get(\"POSITIVE\", [])) + len(entities_dict.get(\"NEGATIVE\", []))\n",
    "            if total_entities > 0:\n",
    "                print(f\"   âœ“ TrÃ­ch xuáº¥t Ä‘Æ°á»£c {total_entities} thá»±c thá»ƒ\")\n",
    "                break\n",
    "                \n",
    "            entity_retry_count += 1\n",
    "            print(f\"   âš  KhÃ´ng cÃ³ thá»±c thá»ƒ. Thá»­ láº¡i {entity_retry_count}/{max_entity_retries}\")\n",
    "            time.sleep(BASE_DELAY)\n",
    "        \n",
    "        if entity_retry_count == max_entity_retries and total_entities == 0:\n",
    "            print(f\"   âŒ Tháº¥t báº¡i sau {max_entity_retries} láº§n thá»­\")\n",
    "            continue\n",
    "        \n",
    "        # Process entities\n",
    "        for impact in [\"POSITIVE\", \"NEGATIVE\"]:\n",
    "            for ent, content in entities_dict.get(impact, []):\n",
    "                # Skip invalid entities\n",
    "                if not ent or \"khÃ´ng cÃ³ thá»±c thá»ƒ nÃ o\" in ent.lower():\n",
    "                    continue\n",
    "                \n",
    "                # Normalize entity\n",
    "                canon_ent = merge_entity(ent, canonical_entities)\n",
    "                \n",
    "                # Determine node type\n",
    "                node_type = \"stock\" if any(str(canon_ent).lower().find(stock.lower()) != -1 for stock in PORTFOLIO_STOCKS) else \"entity\"\n",
    "                \n",
    "                # Add node to graph\n",
    "                if not G.has_node(canon_ent):\n",
    "                    G.add_node(canon_ent, type=node_type, timestamp=article_timestamp)\n",
    "                \n",
    "                # Add edge from article to entity\n",
    "                if not G.has_edge(article_node, canon_ent):\n",
    "                    G.add_edge(article_node, canon_ent, impact=impact, timestamp=article_timestamp)\n",
    "                \n",
    "                # LÆ°u vÃ o káº¿t quáº£\n",
    "                all_entities.append({\n",
    "                    \"article_id\": article_idx,\n",
    "                    \"article_title\": row['title'],\n",
    "                    \"date\": row['date'],\n",
    "                    \"entity\": canon_ent,\n",
    "                    \"entity_type\": node_type,\n",
    "                    \"impact\": impact,\n",
    "                    \"explanation\": content\n",
    "                })\n",
    "    \n",
    "    # Táº¡o DataFrame káº¿t quáº£\n",
    "    entities_df = pd.DataFrame(all_entities)\n",
    "    \n",
    "    # LÆ°u file\n",
    "    entities_df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    # Export to tuples CSV for analysis\n",
    "    print(f\"\\nğŸ“¦ Äang export graph tuples...\")\n",
    "    tuples_data = []\n",
    "    for u, v, data in G.edges(data=True):\n",
    "        if not u.startswith(\"Article_\"):  # Only export non-article edges\n",
    "            tuples_data.append({\n",
    "                'date': data.get('timestamp', ''),\n",
    "                'source': u,\n",
    "                'impact': data.get('impact', ''),\n",
    "                'target': v\n",
    "            })\n",
    "    \n",
    "    if tuples_data:\n",
    "        tuples_df = pd.DataFrame(tuples_data)\n",
    "        tuples_path = output_path.replace('.csv', '_tuples.csv')\n",
    "        tuples_df.to_csv(tuples_path, index=False, encoding='utf-8-sig')\n",
    "        print(f\"âœ“ ÄÃ£ export {len(tuples_df)} tuples vÃ o: {tuples_path}\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"âœ… HOÃ€N THÃ€NH!\")\n",
    "    print(f\"ğŸ“Š Tá»•ng sá»‘ entities: {len(entities_df)}\")\n",
    "    print(f\"ğŸ”¹ Unique entities: {len(canonical_entities)}\")\n",
    "    print(f\"ğŸ“ˆ Graph nodes: {len(G.nodes())}\")\n",
    "    print(f\"ğŸ”— Graph edges: {len(G.edges())}\")\n",
    "    print(f\"ğŸ’¾ ÄÃ£ lÆ°u vÃ o: {output_path}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    return entities_df, G, canonical_entities\n",
    "\n",
    "print(\"âœ“ ÄÃ£ Ä‘á»‹nh nghÄ©a hÃ m extract_entities_from_news()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea251fe5",
   "metadata": {},
   "source": [
    "## ğŸš€ Cháº¡y trÃ­ch xuáº¥t thá»±c thá»ƒ\n",
    "\n",
    "### HÆ°á»›ng dáº«n sá»­ dá»¥ng:\n",
    "\n",
    "1. **Test vá»›i sá»‘ lÆ°á»£ng nhá»:** Thá»­ vá»›i `max_articles=10` Ä‘á»ƒ kiá»ƒm tra\n",
    "2. **Cháº¡y Ä‘áº§y Ä‘á»§:** Bá» `max_articles` Ä‘á»ƒ xá»­ lÃ½ toÃ n bá»™\n",
    "3. **Lá»c theo thá»i gian:** DÃ¹ng `start_date` vÃ  `end_date`\n",
    "\n",
    "### VÃ­ dá»¥:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb6e205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“– Äang Ä‘á»c dá»¯ liá»‡u tá»« summarized_news_with_stocks_merged.csv...\n",
      "âœ“ ÄÃ£ Ä‘á»c 24571 tin tá»©c\n",
      "âœ“ Dataset há»£p lá»‡ vá»›i cÃ¡c cá»™t: ['postID', 'stockCodes', 'title', 'description', 'date']\n",
      "âœ“ Giá»›i háº¡n xuá»‘ng 10 tin Ä‘á»ƒ xá»­ lÃ½\n",
      "\n",
      "============================================================\n",
      "ğŸ” Báº®T Äáº¦U TRÃCH XUáº¤T THá»°C THá»‚\n",
      "============================================================\n",
      "\n",
      "[1/10] ğŸ“° Sá»‘ lÆ°á»£ng hÃ³a Ä‘Æ¡n khá»Ÿi táº¡o tá»« mÃ¡y tÃ­nh tiá»n tÄƒng gáº¥p 13 láº§n n...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource exhausted. Please try again later. Please refer to https://cloud.google.com/vertex-ai/generative-ai/docs/error-code-429 for more details..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš  Lá»—i láº§n 1 vá»›i GOOGLE_API_KEY\n",
      "â³ Äá»£i 30s trÆ°á»›c khi thá»­ key má»›i...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource exhausted. Please try again later. Please refer to https://cloud.google.com/vertex-ai/generative-ai/docs/error-code-429 for more details..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš  Lá»—i láº§n 2 vá»›i GOOGLE_API_KEY\n",
      "â›” GOOGLE_API_KEY Ä‘Ã£ lá»—i 2/2 láº§n\n",
      "ğŸ”„ Chuyá»ƒn sang key tiáº¿p theo...\n",
      "ğŸ”‘ Äang sá»­ dá»¥ng: GOOGLE_API_KEY_2 (Key 2/4)\n",
      "â³ Äá»£i 30s trÆ°á»›c khi thá»­ key má»›i...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource exhausted. Please try again later. Please refer to https://cloud.google.com/vertex-ai/generative-ai/docs/error-code-429 for more details..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš  Lá»—i láº§n 1 vá»›i GOOGLE_API_KEY_2\n",
      "â³ Äá»£i 30s trÆ°á»›c khi thá»­ key má»›i...\n",
      "âš  Lá»—i láº§n 2 vá»›i GOOGLE_API_KEY_2\n",
      "â›” GOOGLE_API_KEY_2 Ä‘Ã£ lá»—i 2/2 láº§n\n",
      "ğŸ”„ Chuyá»ƒn sang key tiáº¿p theo...\n",
      "ğŸ”‘ Äang sá»­ dá»¥ng: GOOGLE_API_KEY_3 (Key 3/4)\n",
      "â³ Äá»£i 30s trÆ°á»›c khi thá»­ key má»›i...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource exhausted. Please try again later. Please refer to https://cloud.google.com/vertex-ai/generative-ai/docs/error-code-429 for more details..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš  Lá»—i láº§n 1 vá»›i GOOGLE_API_KEY_3\n",
      "â³ Äá»£i 30s trÆ°á»›c khi thá»­ key má»›i...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource exhausted. Please try again later. Please refer to https://cloud.google.com/vertex-ai/generative-ai/docs/error-code-429 for more details..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš  Lá»—i láº§n 2 vá»›i GOOGLE_API_KEY_3\n",
      "â›” GOOGLE_API_KEY_3 Ä‘Ã£ lá»—i 2/2 láº§n\n",
      "ğŸ”„ Chuyá»ƒn sang key tiáº¿p theo...\n",
      "ğŸ”‘ Äang sá»­ dá»¥ng: GOOGLE_API_KEY_4 (Key 4/4)\n",
      "â³ Äá»£i 30s trÆ°á»›c khi thá»­ key má»›i...\n"
     ]
    }
   ],
   "source": [
    "# Test vá»›i 10 tin tá»©c Ä‘áº§u tiÃªn\n",
    "entities_df, G, canonical_entities = extract_entities_from_news(\n",
    "    csv_path=\"summarized_news_with_stocks_merged.csv\",  # File merged\n",
    "    output_path=\"entities_extracted.csv\",\n",
    "    max_articles=10  # Test vá»›i 10 tin trÆ°á»›c\n",
    ")\n",
    "\n",
    "# Äá»ƒ cháº¡y Ä‘áº§y Ä‘á»§:\n",
    "# entities_df, G, canonical_entities = extract_entities_from_news(\n",
    "#     csv_path=\"summarized_news_with_stocks_merged.csv\",\n",
    "#     output_path=\"entities_extracted.csv\",\n",
    "#     start_date=\"2025-01-01\",  # TÃ¹y chá»n\n",
    "#     end_date=None\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a79ca8e",
   "metadata": {},
   "source": [
    "## ğŸ“Š PhÃ¢n tÃ­ch káº¿t quáº£\n",
    "\n",
    "Xem cÃ¡c thá»±c thá»ƒ Ä‘Ã£ trÃ­ch xuáº¥t:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6611160e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xem tá»•ng quan\n",
    "print(f\"ğŸ“Š Tá»”NG QUAN Káº¾T QUáº¢\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "print(f\"Tá»•ng sá»‘ entities trÃ­ch xuáº¥t: {len(entities_df)}\")\n",
    "print(f\"Unique entities: {len(canonical_entities)}\")\n",
    "print(f\"Graph nodes: {len(G.nodes())}\")\n",
    "print(f\"Graph edges: {len(G.edges())}\")\n",
    "\n",
    "print(f\"\\nğŸ“ˆ PHÃ‚N LOáº I THEO IMPACT:\\n\")\n",
    "print(entities_df['impact'].value_counts())\n",
    "\n",
    "print(f\"\\nğŸ“Œ PHÃ‚N LOáº I THEO ENTITY TYPE:\\n\")\n",
    "print(entities_df['entity_type'].value_counts())\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"MáºªU 10 ENTITIES Äáº¦U TIÃŠN:\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "for idx, row in entities_df.head(10).iterrows():\n",
    "    print(f\"[{row['article_id']}] ğŸ“° {row['article_title'][:50]}...\")\n",
    "    print(f\"ğŸ·ï¸  Entity: {row['entity']} ({row['entity_type']})\")\n",
    "    print(f\"{'âœ…' if row['impact'] == 'POSITIVE' else 'âŒ'} Impact: {row['impact']}\")\n",
    "    print(f\"ğŸ“ {row['explanation'][:100]}...\")\n",
    "    print(f\"{'-'*60}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c143bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xem cÃ¡c entity Ä‘Æ°á»£c Ä‘á» cáº­p nhiá»u nháº¥t\n",
    "from collections import Counter\n",
    "\n",
    "entity_counts = Counter(entities_df['entity'])\n",
    "\n",
    "print(f\"ğŸ”¥ TOP 15 ENTITIES ÄÆ¯á»¢C Äá»€ Cáº¬P NHIá»€U NHáº¤T:\\n\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "for i, (entity, count) in enumerate(entity_counts.most_common(15), 1):\n",
    "    # Äáº¿m positive vÃ  negative\n",
    "    pos_count = len(entities_df[(entities_df['entity'] == entity) & (entities_df['impact'] == 'POSITIVE')])\n",
    "    neg_count = len(entities_df[(entities_df['entity'] == entity) & (entities_df['impact'] == 'NEGATIVE')])\n",
    "    \n",
    "    print(f\"{i:2d}. {entity}\")\n",
    "    print(f\"    Tá»•ng: {count} láº§n | âœ… {pos_count} | âŒ {neg_count}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2d87c6",
   "metadata": {},
   "source": [
    "## ğŸ” TÃ¬m kiáº¿m vÃ  phÃ¢n tÃ­ch\n",
    "\n",
    "### TÃ¬m thÃ´ng tin vá» má»™t entity cá»¥ thá»ƒ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2a08a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_entity(entity_name, entities_df):\n",
    "    \"\"\"\n",
    "    TÃ¬m kiáº¿m táº¥t cáº£ thÃ´ng tin vá» má»™t entity\n",
    "    \"\"\"\n",
    "    # TÃ¬m kiáº¿m case-insensitive\n",
    "    results = entities_df[entities_df['entity'].str.lower().str.contains(entity_name.lower(), na=False)]\n",
    "    \n",
    "    if len(results) == 0:\n",
    "        print(f\"âŒ KhÃ´ng tÃ¬m tháº¥y entity: {entity_name}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"ğŸ” TÃŒM THáº¤Y {len(results)} MENTIONS Vá»€ '{entity_name.upper()}'\\n\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    for idx, row in results.iterrows():\n",
    "        print(f\"[{row['article_id']}] ğŸ“° {row['article_title']}\")\n",
    "        print(f\"ğŸ“… {row['date']}\")\n",
    "        print(f\"ğŸ·ï¸  {row['entity']} ({row['entity_type']})\")\n",
    "        print(f\"{'âœ…' if row['impact'] == 'POSITIVE' else 'âŒ'} {row['impact']}\")\n",
    "        print(f\"ğŸ“ {row['explanation']}\")\n",
    "        print(f\"{'-'*60}\\n\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# VÃ­ dá»¥: TÃ¬m thÃ´ng tin vá» FPT\n",
    "# search_entity(\"FPT\", entities_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505062d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PhÃ¢n tÃ­ch Graph: Xem cÃ¡c entities káº¿t ná»‘i vá»›i nhau\n",
    "def analyze_graph(G):\n",
    "    \"\"\"\n",
    "    PhÃ¢n tÃ­ch knowledge graph Ä‘Ã£ xÃ¢y dá»±ng\n",
    "    \"\"\"\n",
    "    print(f\"ğŸ“Š PHÃ‚N TÃCH KNOWLEDGE GRAPH\\n\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Thá»‘ng kÃª cÆ¡ báº£n\n",
    "    print(f\"Tá»•ng sá»‘ nodes: {len(G.nodes())}\")\n",
    "    print(f\"Tá»•ng sá»‘ edges: {len(G.edges())}\")\n",
    "    \n",
    "    # PhÃ¢n loáº¡i nodes\n",
    "    node_types = {}\n",
    "    for node, data in G.nodes(data=True):\n",
    "        node_type = data.get('type', 'unknown')\n",
    "        node_types[node_type] = node_types.get(node_type, 0) + 1\n",
    "    \n",
    "    print(f\"\\nğŸ“Œ PhÃ¢n loáº¡i nodes:\")\n",
    "    for ntype, count in node_types.items():\n",
    "        print(f\"   {ntype}: {count}\")\n",
    "    \n",
    "    # TÃ¬m nodes cÃ³ nhiá»u káº¿t ná»‘i nháº¥t (degree centrality)\n",
    "    entity_nodes = [n for n, d in G.nodes(data=True) if d.get('type') != 'article']\n",
    "    \n",
    "    if entity_nodes:\n",
    "        # In-degree: sá»‘ lÆ°á»£ng bÃ i bÃ¡o liÃªn káº¿t Ä‘áº¿n entity nÃ y\n",
    "        in_degrees = [(node, G.in_degree(node)) for node in entity_nodes]\n",
    "        in_degrees.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        print(f\"\\nğŸ”¥ TOP 10 ENTITIES ÄÆ¯á»¢C NHáº®C Äáº¾N NHIá»€U NHáº¤T (bá»Ÿi cÃ¡c bÃ i bÃ¡o):\\n\")\n",
    "        for i, (node, degree) in enumerate(in_degrees[:10], 1):\n",
    "            print(f\"{i:2d}. {node}: {degree} bÃ i bÃ¡o\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "\n",
    "# Cháº¡y phÃ¢n tÃ­ch\n",
    "analyze_graph(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b70144",
   "metadata": {},
   "source": [
    "## ğŸ’¾ LÆ°u Graph Ä‘á»ƒ sá»­ dá»¥ng sau\n",
    "\n",
    "LÆ°u graph vÃ o file pickle Ä‘á»ƒ sá»­ dá»¥ng cho cÃ¡c bÆ°á»›c tiáº¿p theo (relation extraction, attention mechanism...):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcc08b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# LÆ°u graph\n",
    "with open('knowledge_graph.pkl', 'wb') as f:\n",
    "    pickle.dump(G, f)\n",
    "print(\"âœ“ ÄÃ£ lÆ°u knowledge graph vÃ o 'knowledge_graph.pkl'\")\n",
    "\n",
    "# LÆ°u canonical entities\n",
    "with open('canonical_entities.pkl', 'wb') as f:\n",
    "    pickle.dump(canonical_entities, f)\n",
    "print(\"âœ“ ÄÃ£ lÆ°u canonical_entities vÃ o 'canonical_entities.pkl'\")\n",
    "\n",
    "# Äá»ƒ load láº¡i sau:\n",
    "# with open('knowledge_graph.pkl', 'rb') as f:\n",
    "#     G = pickle.load(f)\n",
    "# with open('canonical_entities.pkl', 'rb') as f:\n",
    "#     canonical_entities = pickle.load(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
