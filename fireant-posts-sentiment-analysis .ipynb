{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T00:47:25.459216Z",
     "iopub.status.busy": "2025-11-13T00:47:25.458661Z",
     "iopub.status.idle": "2025-11-13T00:47:25.779185Z",
     "shell.execute_reply": "2025-11-13T00:47:25.778447Z",
     "shell.execute_reply.started": "2025-11-13T00:47:25.459194Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T00:47:25.781124Z",
     "iopub.status.busy": "2025-11-13T00:47:25.780626Z",
     "iopub.status.idle": "2025-11-13T00:48:05.398526Z",
     "shell.execute_reply": "2025-11-13T00:48:05.397751Z",
     "shell.execute_reply.started": "2025-11-13T00:47:25.781102Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('fireant_data/cleaned_posts/all_posts.csv', nrows=1000000)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep posts with negative and positive sentiment only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T00:48:05.399933Z",
     "iopub.status.busy": "2025-11-13T00:48:05.399555Z",
     "iopub.status.idle": "2025-11-13T00:48:06.446501Z",
     "shell.execute_reply": "2025-11-13T00:48:06.445712Z",
     "shell.execute_reply.started": "2025-11-13T00:48:05.399911Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sentiment_df = df[df['sentiment'].isin([-1, 1])]\n",
    "sentiment_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T00:48:06.448669Z",
     "iopub.status.busy": "2025-11-13T00:48:06.448392Z",
     "iopub.status.idle": "2025-11-13T00:48:06.454748Z",
     "shell.execute_reply": "2025-11-13T00:48:06.454090Z",
     "shell.execute_reply.started": "2025-11-13T00:48:06.448651Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_links(text):\n",
    "    # Xóa link dạng http(s)://... hoặc www....\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)     # remove http:// hoặc https://\n",
    "    text = re.sub(r\"www\\.\\S+\", \"\", text)    # remove www...\n",
    "    text = re.sub(r\"\\S+\\.com\\S*\", \"\", text) # remove .com/.net/.vn...\n",
    "    return text.strip()\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()                          # lowercase\n",
    "    text = re.sub(r\"\\n+\", \". \", text)                # replace new line with period\n",
    "    text = remove_links(text)                         # remove links\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)                  # remove mentions (@abc)\n",
    "    text = re.sub(r\"#\\w+\", \"\", text)                  # remove hashtags\n",
    "    text = re.sub(\n",
    "        r\"[^0-9a-zA-Záàảãạăắằẳẵặâấầẩẫậéèẻẽẹêếềểễệ\"\n",
    "        r\"íìỉĩịóòỏõọôốồổỗộơớờởỡợúùủũụưứừửữự\"\n",
    "        r\"ýỳỷỹỵđ\\s!?,.]+\", \n",
    "        \" \", \n",
    "        text\n",
    "    )            # keep only letters, numbers and some punctuation\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()          # remove extra spaces\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove links in posts content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T00:48:06.455605Z",
     "iopub.status.busy": "2025-11-13T00:48:06.455410Z",
     "iopub.status.idle": "2025-11-13T00:48:33.993901Z",
     "shell.execute_reply": "2025-11-13T00:48:33.993020Z",
     "shell.execute_reply.started": "2025-11-13T00:48:06.455590Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sentiment_df['originalContent'] = sentiment_df['originalContent'].apply(clean_text)\n",
    "sentiment_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T00:48:33.995055Z",
     "iopub.status.busy": "2025-11-13T00:48:33.994800Z",
     "iopub.status.idle": "2025-11-13T00:48:44.869436Z",
     "shell.execute_reply": "2025-11-13T00:48:44.868616Z",
     "shell.execute_reply.started": "2025-11-13T00:48:33.995037Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer \n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T00:48:44.870885Z",
     "iopub.status.busy": "2025-11-13T00:48:44.870391Z",
     "iopub.status.idle": "2025-11-13T00:48:44.877875Z",
     "shell.execute_reply": "2025-11-13T00:48:44.877079Z",
     "shell.execute_reply.started": "2025-11-13T00:48:44.870858Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len=100, mode=\"train\",test_size=0.2, random_state=42):\n",
    "        # keep only necessary columns\n",
    "        df = df[['originalContent', 'sentiment']]\n",
    "        \n",
    "        # train/test split\n",
    "        train_df, test_df = train_test_split(\n",
    "            df, \n",
    "            test_size=test_size,\n",
    "            stratify=df['sentiment'],\n",
    "            random_state=random_state\n",
    "        )\n",
    "        \n",
    "        self.df = train_df if mode == \"train\" else test_df \n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.iloc[index]\n",
    "        text = row['originalContent']\n",
    "        label = 1 if row['sentiment'] == 1 else 0\n",
    "        \n",
    "        tokens = self.tokenizer(\n",
    "            text, \n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": tokens['input_ids'].squeeze(0),\n",
    "            \"attention_mask\": tokens['attention_mask'].squeeze(0),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long)\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T00:48:44.879199Z",
     "iopub.status.busy": "2025-11-13T00:48:44.878927Z",
     "iopub.status.idle": "2025-11-13T00:48:48.201116Z",
     "shell.execute_reply": "2025-11-13T00:48:48.200203Z",
     "shell.execute_reply.started": "2025-11-13T00:48:44.879175Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import WeightedRandomSampler\n",
    "# load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"5CD-AI/Vietnamese-Sentiment-visobert\")\n",
    "\n",
    "# Create dataset\n",
    "train_dataset = SentimentDataset(sentiment_df, tokenizer, mode='train')\n",
    "test_dataset = SentimentDataset(sentiment_df, tokenizer, mode='test')\n",
    "\n",
    "# compute class weights\n",
    "labels = train_dataset.df['sentiment'].map({-1:0, 1:1}).values\n",
    "class_counts = [sum(labels==0), sum(labels==1)] # [neg_count, pos_count]\n",
    "class_weights = [1.0 / count for count in class_counts]\n",
    "\n",
    "# assign weight to each sample\n",
    "sample_weights = [class_weights[label] for label in labels]\n",
    "\n",
    "# create WeightedRandomSampler\n",
    "sampler = WeightedRandomSampler(\n",
    "    weights=sample_weights,\n",
    "    num_samples=len(sample_weights),\n",
    "    replacement=True # allow sampling the same example multiple times\n",
    ")\n",
    "\n",
    "# create DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, sampler=sampler)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# check a batch\n",
    "batch = next(iter(train_loader))\n",
    "print(batch['input_ids'].shape)  # (batch_size, seq_len)\n",
    "print(batch['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T00:48:48.202252Z",
     "iopub.status.busy": "2025-11-13T00:48:48.202037Z",
     "iopub.status.idle": "2025-11-13T00:48:48.401936Z",
     "shell.execute_reply": "2025-11-13T00:48:48.401115Z",
     "shell.execute_reply.started": "2025-11-13T00:48:48.202235Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# check a batch\n",
    "batch = next(iter(train_loader))\n",
    "print(batch['input_ids'].shape)  # (batch_size, seq_len)\n",
    "print(batch['labels'])\n",
    "print(batch['labels'].bincount()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T00:48:48.404852Z",
     "iopub.status.busy": "2025-11-13T00:48:48.404618Z",
     "iopub.status.idle": "2025-11-13T00:49:12.621304Z",
     "shell.execute_reply": "2025-11-13T00:49:12.620513Z",
     "shell.execute_reply.started": "2025-11-13T00:48:48.404835Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers.models.xlm_roberta.modeling_xlm_roberta import XLMRobertaClassificationHead\n",
    "from transformers import AutoModelForSequenceClassification, AutoConfig\n",
    "model_name = \"5CD-AI/Vietnamese-Sentiment-visobert\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "# 1. Load config\n",
    "config = AutoConfig.from_pretrained(\"5CD-AI/Vietnamese-Sentiment-visobert\")\n",
    "config.num_labels = 2  # important\n",
    "\n",
    "# 2. Load model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    config=config,\n",
    "    ignore_mismatched_sizes=True   # avoid shape errors\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T00:49:12.622638Z",
     "iopub.status.busy": "2025-11-13T00:49:12.622058Z",
     "iopub.status.idle": "2025-11-13T00:49:12.629016Z",
     "shell.execute_reply": "2025-11-13T00:49:12.628135Z",
     "shell.execute_reply.started": "2025-11-13T00:49:12.622614Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def freeze_model_layers(model, unfreeze_last_n=4):\n",
    "    \"\"\"\n",
    "    Freeze all but the last `n` transformer layers of the encoder,\n",
    "    plus keep the classifier trainable.\n",
    "    \"\"\"\n",
    "    # 1. Freeze embeddings\n",
    "    for param in model.roberta.embeddings.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # 2. Freeze all encoder layers except the last `unfreeze_last_n`\n",
    "    for i, layer in enumerate(model.roberta.encoder.layer):\n",
    "        if i < len(model.roberta.encoder.layer) - unfreeze_last_n:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = False\n",
    "        else:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "    # 3. Always train classifier head\n",
    "    for param in model.classifier.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    # Print summary\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Trainable params: {trainable_params/1e6:.2f}M / {total_params/1e6:.2f}M total \"\n",
    "          f\"({100 * trainable_params/total_params:.1f}%)\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T00:49:12.631070Z",
     "iopub.status.busy": "2025-11-13T00:49:12.630039Z",
     "iopub.status.idle": "2025-11-13T00:49:12.695163Z",
     "shell.execute_reply": "2025-11-13T00:49:12.694413Z",
     "shell.execute_reply.started": "2025-11-13T00:49:12.631038Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "freeze_model_layers(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T00:49:12.696427Z",
     "iopub.status.busy": "2025-11-13T00:49:12.696109Z",
     "iopub.status.idle": "2025-11-13T00:49:12.735618Z",
     "shell.execute_reply": "2025-11-13T00:49:12.735062Z",
     "shell.execute_reply.started": "2025-11-13T00:49:12.696400Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.amp import autocast, GradScaler\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, data_loader, device='cuda'):\n",
    "    model.eval()\n",
    "    preds, true_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"Evaluating\", leave=False):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            with autocast('cuda'):  # Add this for mixed precision\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits \n",
    "            \n",
    "            batch_preds = torch.argmax(logits, dim=1)\n",
    "            preds.extend(batch_preds.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "    acc = accuracy_score(true_labels, preds)\n",
    "    f1 = f1_score(true_labels, preds)\n",
    "    precision = precision_score(true_labels, preds)\n",
    "    recall = recall_score(true_labels, preds)\n",
    "\n",
    "    print(f\"Eval | Acc: {acc:.4f} | F1: {f1:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f}\")\n",
    "    return acc, f1, precision, recall\n",
    "\n",
    "def train_model(model, train_loader, val_loader=None,\n",
    "                epochs=3, lr=2e-5, weight_decay=0.01,\n",
    "                warmup_ratio=0.1, max_grad_norm=1.0, device='cuda',\n",
    "                save_dir=\"./checkpoints\", save_every_n_epochs=2):\n",
    "    \n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    model.to(device)\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "    total_steps = len(train_loader) * epochs\n",
    "    warmup_steps = int(total_steps * warmup_ratio)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=warmup_steps,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    scaler = GradScaler()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            with autocast(device_type='cuda'):\n",
    "                outputs = model(input_ids=input_ids,\n",
    "                                attention_mask=attention_mask,\n",
    "                                labels=labels)\n",
    "                loss = outputs.loss\n",
    "                \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1} | Train loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        # Evaluate\n",
    "        if val_loader is not None:\n",
    "            evaluate_model(model, val_loader, device=device)\n",
    "        \n",
    "        # === SAVE EVERY `save_every_n_epochs` EPOCHS ===\n",
    "        if (epoch + 1) % save_every_n_epochs == 0:\n",
    "            save_path = os.path.join(save_dir, f\"model_epoch_{epoch+1}\")\n",
    "            model.save_pretrained(save_path)\n",
    "            tokenizer.save_pretrained(save_path)\n",
    "            print(f\"Checkpoint saved: {save_path}\")\n",
    "\n",
    "    # Save final model\n",
    "    final_path = os.path.join(save_dir, \"model_final\")\n",
    "    model.save_pretrained(final_path)\n",
    "    tokenizer.save_pretrained(final_path)\n",
    "    print(f\"Final model saved: {final_path}\")\n",
    "    \n",
    "    print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T00:49:12.736518Z",
     "iopub.status.busy": "2025-11-13T00:49:12.736306Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_model(model, train_loader, val_loader=test_loader, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply the Model for Neutral Sentiment Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.amp import autocast\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "@torch.no_grad()\n",
    "def analyze_zero_sentiment_posts(\n",
    "    df,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    text_column='originalContent',\n",
    "    sentiment_column='sentiment',\n",
    "    batch_size=32,\n",
    "    device='cuda',\n",
    "    label_map={0: -1, 1: 1}  # 0 in logits → negative, 1 → positive\n",
    "):\n",
    "    \"\"\"\n",
    "    Predict sentiment for rows where `sentiment == 0` (unknown/neutral).\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataframe\n",
    "        model: Trained HuggingFace model (on GPU/CPU)\n",
    "        tokenizer: Corresponding tokenizer\n",
    "        text_column: Name of column with cleaned text\n",
    "        sentiment_column: Name of column with labels (0, 1, -1)\n",
    "        batch_size: Inference batch size\n",
    "        device: 'cuda' or 'cpu'\n",
    "        label_map: How to map logit index → actual label\n",
    "    \n",
    "    Returns:\n",
    "        df_with_pred: DataFrame with new column `predicted_sentiment`\n",
    "    \"\"\"\n",
    "    # Filter rows with sentiment == 0\n",
    "    zero_df = df[df[sentiment_column] == 0].copy()\n",
    "    if zero_df.empty:\n",
    "        print(\"No rows with sentiment == 0 found.\")\n",
    "        df['predicted_sentiment'] = None\n",
    "        return df\n",
    "\n",
    "    print(f\"Analyzing {len(zero_df)} posts with sentiment == 0...\")\n",
    "\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    # Tokenize all texts\n",
    "    texts = zero_df[text_column].tolist()\n",
    "    encodings = tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=256,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    # Move to device\n",
    "    input_ids = encodings['input_ids'].to(device)\n",
    "    attention_mask = encodings['attention_mask'].to(device)\n",
    "\n",
    "    # Batch inference\n",
    "    dataset = torch.utils.data.TensorDataset(input_ids, attention_mask)\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "    predictions = []\n",
    "    for batch in tqdm(loader, desc=\"Predicting\", leave=False):\n",
    "        batch_ids, batch_mask = batch\n",
    "        with autocast(device_type='cuda' if 'cuda' in device else 'cpu'):\n",
    "            outputs = model(input_ids=batch_ids, attention_mask=batch_mask)\n",
    "        logits = outputs.logits\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        predictions.extend(preds.cpu().numpy())\n",
    "\n",
    "    # Map predictions: 0 → -1, 1 → 1\n",
    "    pred_labels = [label_map[p] for p in predictions]\n",
    "\n",
    "    # Add to original zero_df\n",
    "    zero_df = zero_df.copy()\n",
    "    zero_df['predicted_sentiment'] = pred_labels\n",
    "\n",
    "    # Merge back into original df\n",
    "    df_with_pred = df.copy()\n",
    "    df_with_pred = df_with_pred.merge(\n",
    "        zero_df[['predicted_sentiment']],\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # Fill NaN (non-zero sentiment rows) with None or keep original\n",
    "    df_with_pred['predicted_sentiment'] = df_with_pred['predicted_sentiment'].where(\n",
    "        df[sentiment_column] == 0, None\n",
    "    )\n",
    "\n",
    "    # Summary\n",
    "    pos = sum(p == 1 for p in pred_labels)\n",
    "    neg = sum(p == -1 for p in pred_labels)\n",
    "    print(f\"\\nResults for sentiment == 0 posts:\")\n",
    "    print(f\"  Positive: {pos} ({pos/len(pred_labels)*100:.1f}%)\")\n",
    "    print(f\"  Negative: {neg} ({neg/len(pred_labels)*100:.1f}%)\")\n",
    "\n",
    "    return df_with_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "new_df = df.copy()\n",
    "new_df['originalContent'] = new_df['originalContent'].apply(clean_text)\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "new_df = analyze_zero_sentiment_posts(\n",
    "    df=new_df,           # original dataframe\n",
    "    model=model,               # trained model\n",
    "    tokenizer=tokenizer,       # same tokenizer\n",
    "    text_column='originalContent',\n",
    "    sentiment_column='sentiment',\n",
    "    batch_size=32,\n",
    "    device='cuda'\n",
    ")\n",
    "\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "new_df.to_csv('posts_sentiment.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8722562,
     "sourceId": 13711106,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
