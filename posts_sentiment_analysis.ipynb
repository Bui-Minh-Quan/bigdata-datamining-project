{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42d288ff",
   "metadata": {},
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c7a5d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a415dde9",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "eeb79f05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>postID</th>\n",
       "      <th>originalContent</th>\n",
       "      <th>date</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>taggedSymbols</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3118126</td>\n",
       "      <td>V√†o NKG ng√†y mai ·ªïn kh√¥ng c√°c b√°c? D√†i h·∫°n v√†o...</td>\n",
       "      <td>2021-09-30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>['NKG']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3118104</td>\n",
       "      <td>C√≥ m·ªôt ƒëi·ªÅu th·∫•y bu·ªìn c∆∞·ªùi nh·∫•t l√† nh·ªØng ng∆∞·ªùi...</td>\n",
       "      <td>2021-09-30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>['ART']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3118086</td>\n",
       "      <td>L·∫°i l√† ƒêTC</td>\n",
       "      <td>2021-09-30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>['DHA']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3118058</td>\n",
       "      <td>Ch√†o anh em, m√¨nh g·ª≠i anh ch·ªã em chi·∫øn l∆∞·ª£c ƒë√°...</td>\n",
       "      <td>2021-09-30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3118053</td>\n",
       "      <td>CTC v√†o 8.3 c√≥ d√≠nh b√¥ kh√¥ng m·ªçi ng∆∞·ªùi?</td>\n",
       "      <td>2021-09-30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>['CTC']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    postID                                    originalContent        date  \\\n",
       "0  3118126  V√†o NKG ng√†y mai ·ªïn kh√¥ng c√°c b√°c? D√†i h·∫°n v√†o...  2021-09-30   \n",
       "1  3118104  C√≥ m·ªôt ƒëi·ªÅu th·∫•y bu·ªìn c∆∞·ªùi nh·∫•t l√† nh·ªØng ng∆∞·ªùi...  2021-09-30   \n",
       "2  3118086                                         L·∫°i l√† ƒêTC  2021-09-30   \n",
       "3  3118058  Ch√†o anh em, m√¨nh g·ª≠i anh ch·ªã em chi·∫øn l∆∞·ª£c ƒë√°...  2021-09-30   \n",
       "4  3118053            CTC v√†o 8.3 c√≥ d√≠nh b√¥ kh√¥ng m·ªçi ng∆∞·ªùi?  2021-09-30   \n",
       "\n",
       "   sentiment taggedSymbols  \n",
       "0        0.0       ['NKG']  \n",
       "1        0.0       ['ART']  \n",
       "2        0.0       ['DHA']  \n",
       "3        0.0            []  \n",
       "4        0.0       ['CTC']  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('fireant_data/cleaned_posts/all_posts.csv', nrows=200000)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9dae06",
   "metadata": {},
   "source": [
    "## Keep posts with negative and positive sentiment only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8f282eba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>postID</th>\n",
       "      <th>originalContent</th>\n",
       "      <th>date</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>taggedSymbols</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3117847</td>\n",
       "      <td>N√äN MUA G√å CHO B√ÅO C√ÅO QU√ù 3?\\n\\nTh√°ng 10 l√† g...</td>\n",
       "      <td>2021-09-30</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['DCM', 'DPM', 'GMD', 'HPG', 'KSB', 'NTL', 'PV...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>3117591</td>\n",
       "      <td>D·ª± √°n Akari City v·ªõi quy m√¥ 5.000 cƒÉn h·ªô n·∫±m t...</td>\n",
       "      <td>2021-09-30</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['NLG']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>3117571</td>\n",
       "      <td>PNJ - H√ÄNH TR√åNH M·ªöI üî•üî•\\n\\nPNJ + 5,79% üî•\\n\\nT·∫°...</td>\n",
       "      <td>2021-09-30</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['MSN', 'MWG', 'PC1', 'PNJ']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>3117327</td>\n",
       "      <td>Ti·∫øc qu√° kh√¥ng CE\\nNay ƒÉn ƒë∆∞·ª£c PNJ em vui qu√° ...</td>\n",
       "      <td>2021-09-30</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['PNJ']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>3117230</td>\n",
       "      <td>Gi√° kh√≠ ƒë·ªët l·∫°i tƒÉng d·ª±ng ƒë·ª©ng mai l·∫°i tr·∫ßn ti...</td>\n",
       "      <td>2021-09-30</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['ASP']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     postID                                    originalContent        date  \\\n",
       "11  3117847  N√äN MUA G√å CHO B√ÅO C√ÅO QU√ù 3?\\n\\nTh√°ng 10 l√† g...  2021-09-30   \n",
       "29  3117591  D·ª± √°n Akari City v·ªõi quy m√¥ 5.000 cƒÉn h·ªô n·∫±m t...  2021-09-30   \n",
       "31  3117571  PNJ - H√ÄNH TR√åNH M·ªöI üî•üî•\\n\\nPNJ + 5,79% üî•\\n\\nT·∫°...  2021-09-30   \n",
       "48  3117327  Ti·∫øc qu√° kh√¥ng CE\\nNay ƒÉn ƒë∆∞·ª£c PNJ em vui qu√° ...  2021-09-30   \n",
       "56  3117230  Gi√° kh√≠ ƒë·ªët l·∫°i tƒÉng d·ª±ng ƒë·ª©ng mai l·∫°i tr·∫ßn ti...  2021-09-30   \n",
       "\n",
       "    sentiment                                      taggedSymbols  \n",
       "11        1.0  ['DCM', 'DPM', 'GMD', 'HPG', 'KSB', 'NTL', 'PV...  \n",
       "29        1.0                                            ['NLG']  \n",
       "31        1.0                       ['MSN', 'MWG', 'PC1', 'PNJ']  \n",
       "48        1.0                                            ['PNJ']  \n",
       "56        1.0                                            ['ASP']  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_df = df[df['sentiment'].isin([-1, 1])]\n",
    "sentiment_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd12102",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b86a40b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_links(text):\n",
    "    # X√≥a link d·∫°ng http(s)://... ho·∫∑c www....\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)     # remove http:// ho·∫∑c https://\n",
    "    text = re.sub(r\"www\\.\\S+\", \"\", text)    # remove www...\n",
    "    text = re.sub(r\"\\S+\\.com\\S*\", \"\", text) # remove .com/.net/.vn...\n",
    "    return text.strip()\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()                          # lowercase\n",
    "    text = re.sub(r\"\\n+\", \". \", text)                # replace new line with period\n",
    "    text = remove_links(text)                         # remove links\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)                  # remove mentions (@abc)\n",
    "    text = re.sub(r\"#\\w+\", \"\", text)                  # remove hashtags\n",
    "    text = re.sub(\n",
    "        r\"[^0-9a-zA-Z√°√†·∫£√£·∫°ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠√©√®·∫ª·∫Ω·∫π√™·∫ø·ªÅ·ªÉ·ªÖ·ªá\"\n",
    "        r\"√≠√¨·ªâƒ©·ªã√≥√≤·ªè√µ·ªç√¥·ªë·ªì·ªï·ªó·ªô∆°·ªõ·ªù·ªü·ª°·ª£√∫√π·ªß≈©·ª•∆∞·ª©·ª´·ª≠·ªØ·ª±\"\n",
    "        r\"√Ω·ª≥·ª∑·ªπ·ªµƒë\\s!?,.]+\", \n",
    "        \" \", \n",
    "        text\n",
    "    )            # keep only letters, numbers and some punctuation\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()          # remove extra spaces\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98396909",
   "metadata": {},
   "source": [
    "### Remove links in posts content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1aa5fe12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_16400\\952562065.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sentiment_df['originalContent'] = sentiment_df['originalContent'].apply(clean_text)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>postID</th>\n",
       "      <th>originalContent</th>\n",
       "      <th>date</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>taggedSymbols</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3117847</td>\n",
       "      <td>n√™n mua g√¨ cho b√°o c√°o qu√Ω 3?. th√°ng 10 l√† gia...</td>\n",
       "      <td>2021-09-30</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['DCM', 'DPM', 'GMD', 'HPG', 'KSB', 'NTL', 'PV...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>3117591</td>\n",
       "      <td>d·ª± √°n akari city v·ªõi quy m√¥ 5.000 cƒÉn h·ªô n·∫±m t...</td>\n",
       "      <td>2021-09-30</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['NLG']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>3117571</td>\n",
       "      <td>pnj h√†nh tr√¨nh m·ªõi . pnj 5,79 . t·∫°i sao n√≥i h√†...</td>\n",
       "      <td>2021-09-30</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['MSN', 'MWG', 'PC1', 'PNJ']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>3117327</td>\n",
       "      <td>ti·∫øc qu√° kh√¥ng ce. nay ƒÉn ƒë∆∞·ª£c pnj em vui qu√° ...</td>\n",
       "      <td>2021-09-30</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['PNJ']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>3117230</td>\n",
       "      <td>gi√° kh√≠ ƒë·ªët l·∫°i tƒÉng d·ª±ng ƒë·ª©ng mai l·∫°i tr·∫ßn ti...</td>\n",
       "      <td>2021-09-30</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['ASP']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     postID                                    originalContent        date  \\\n",
       "11  3117847  n√™n mua g√¨ cho b√°o c√°o qu√Ω 3?. th√°ng 10 l√† gia...  2021-09-30   \n",
       "29  3117591  d·ª± √°n akari city v·ªõi quy m√¥ 5.000 cƒÉn h·ªô n·∫±m t...  2021-09-30   \n",
       "31  3117571  pnj h√†nh tr√¨nh m·ªõi . pnj 5,79 . t·∫°i sao n√≥i h√†...  2021-09-30   \n",
       "48  3117327  ti·∫øc qu√° kh√¥ng ce. nay ƒÉn ƒë∆∞·ª£c pnj em vui qu√° ...  2021-09-30   \n",
       "56  3117230  gi√° kh√≠ ƒë·ªët l·∫°i tƒÉng d·ª±ng ƒë·ª©ng mai l·∫°i tr·∫ßn ti...  2021-09-30   \n",
       "\n",
       "    sentiment                                      taggedSymbols  \n",
       "11        1.0  ['DCM', 'DPM', 'GMD', 'HPG', 'KSB', 'NTL', 'PV...  \n",
       "29        1.0                                            ['NLG']  \n",
       "31        1.0                       ['MSN', 'MWG', 'PC1', 'PNJ']  \n",
       "48        1.0                                            ['PNJ']  \n",
       "56        1.0                                            ['ASP']  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_df['originalContent'] = sentiment_df['originalContent'].apply(clean_text)\n",
    "sentiment_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831e0ec6",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e6f3fa53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer \n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "68aaaf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len=100, mode=\"train\",test_size=0.2, random_state=42):\n",
    "        # keep only necessary columns\n",
    "        df = df[['originalContent', 'sentiment']]\n",
    "        \n",
    "        # train/test split\n",
    "        train_df, test_df = train_test_split(\n",
    "            df, \n",
    "            test_size=test_size,\n",
    "            stratify=df['sentiment'],\n",
    "            random_state=random_state\n",
    "        )\n",
    "        \n",
    "        self.df = train_df if mode == \"train\" else test_df \n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.iloc[index]\n",
    "        text = row['originalContent']\n",
    "        label = 1 if row['sentiment'] == 1 else 0\n",
    "        \n",
    "        tokens = self.tokenizer(\n",
    "            text, \n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": tokens['input_ids'].squeeze(0),\n",
    "            \"attention_mask\": tokens['attention_mask'].squeeze(0),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long)\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9db1a633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 100])\n",
      "tensor([0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,\n",
      "        0, 0, 1, 0, 1, 1, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import WeightedRandomSampler\n",
    "# load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"5CD-AI/Vietnamese-Sentiment-visobert\")\n",
    "\n",
    "# Create dataset\n",
    "train_dataset = SentimentDataset(sentiment_df, tokenizer, mode='train')\n",
    "test_dataset = SentimentDataset(sentiment_df, tokenizer, mode='test')\n",
    "\n",
    "# compute class weights\n",
    "labels = train_dataset.df['sentiment'].map({-1:0, 1:1}).values\n",
    "class_counts = [sum(labels==0), sum(labels==1)] # [neg_count, pos_count]\n",
    "class_weights = [1.0 / count for count in class_counts]\n",
    "\n",
    "# assign weight to each sample\n",
    "sample_weights = [class_weights[label] for label in labels]\n",
    "\n",
    "# create WeightedRandomSampler\n",
    "sampler = WeightedRandomSampler(\n",
    "    weights=sample_weights,\n",
    "    num_samples=len(sample_weights),\n",
    "    replacement=True # allow sampling the same example multiple times\n",
    ")\n",
    "\n",
    "# create DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, sampler=sampler)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# check a batch\n",
    "batch = next(iter(train_loader))\n",
    "print(batch['input_ids'].shape)  # (batch_size, seq_len)\n",
    "print(batch['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ff2e3ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 100])\n",
      "tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,\n",
      "        0, 1, 1, 0, 0, 0, 0, 1])\n",
      "tensor([14, 18])\n"
     ]
    }
   ],
   "source": [
    "# check a batch\n",
    "batch = next(iter(train_loader))\n",
    "print(batch['input_ids'].shape)  # (batch_size, seq_len)\n",
    "print(batch['labels'])\n",
    "print(batch['labels'].bincount()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452ff56a",
   "metadata": {},
   "source": [
    "## Load pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "76609009",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at 5CD-AI/Vietnamese-Sentiment-visobert and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "- classifier.out_proj.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers.models.xlm_roberta.modeling_xlm_roberta import XLMRobertaClassificationHead\n",
    "from transformers import AutoModelForSequenceClassification, AutoConfig\n",
    "model_name = \"5CD-AI/Vietnamese-Sentiment-visobert\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "# 1. Load config\n",
    "config = AutoConfig.from_pretrained(\"5CD-AI/Vietnamese-Sentiment-visobert\")\n",
    "config.num_labels = 2  # important\n",
    "\n",
    "# 2. Load model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    config=config,\n",
    "    ignore_mismatched_sizes=True   # avoid shape errors\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "9db6a03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_model_layers(model, unfreeze_last_n=2):\n",
    "    \"\"\"\n",
    "    Freeze all but the last `n` transformer layers of the encoder,\n",
    "    plus keep the classifier trainable.\n",
    "    \"\"\"\n",
    "    # 1. Freeze embeddings\n",
    "    for param in model.roberta.embeddings.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # 2. Freeze all encoder layers except the last `unfreeze_last_n`\n",
    "    for i, layer in enumerate(model.roberta.encoder.layer):\n",
    "        if i < len(model.roberta.encoder.layer) - unfreeze_last_n:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = False\n",
    "        else:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "    # 3. Always train classifier head\n",
    "    for param in model.classifier.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    # Print summary\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Trainable params: {trainable_params/1e6:.2f}M / {total_params/1e6:.2f}M total \"\n",
    "          f\"({100 * trainable_params/total_params:.1f}%)\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3a8c4f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 14.77M / 97.57M total (15.1%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XLMRobertaForSequenceClassification(\n",
       "  (roberta): XLMRobertaModel(\n",
       "    (embeddings): XLMRobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(15004, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): XLMRobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x XLMRobertaLayer(\n",
       "          (attention): XLMRobertaAttention(\n",
       "            (self): XLMRobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): XLMRobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): XLMRobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): XLMRobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): XLMRobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freeze_model_layers(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36595be1",
   "metadata": {},
   "source": [
    "## Training and Evaluating Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d197bd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.amp import autocast, GradScaler\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, data_loader, device='cuda'):\n",
    "    model.eval()\n",
    "    preds, true_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"Evaluating\", leave=False):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            with autocast('cuda'):  # Add this for mixed precision\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits \n",
    "            \n",
    "            batch_preds = torch.argmax(logits, dim=1)\n",
    "            preds.extend(batch_preds.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "    acc = accuracy_score(true_labels, preds)\n",
    "    f1 = f1_score(true_labels, preds)\n",
    "    precision = precision_score(true_labels, preds)\n",
    "    recall = recall_score(true_labels, preds)\n",
    "\n",
    "    print(f\"Eval | Acc: {acc:.4f} | F1: {f1:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f}\")\n",
    "    return acc, f1, precision, recall\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, val_loader=None,\n",
    "                epochs=3, lr=2e-5,weight_decay=0.01, \n",
    "                warmup_ratio=0.1, max_grad_norm=1.0, device='cuda'):\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    # optimizer + weight decay\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "    # LR Scheduler with warmup\n",
    "    total_steps = len(train_loader) * epochs\n",
    "    warmup_steps = int(total_steps * warmup_ratio)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=warmup_steps,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    scaler = GradScaler('cuda')  # mixed precision scaler\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with autocast('cuda'):\n",
    "                outputs = model(input_ids=input_ids,\n",
    "                                attention_mask=attention_mask,\n",
    "                                labels=labels)\n",
    "                loss = outputs.loss\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            # Gradient clipping\n",
    "            scaler.unscale_(optimizer)\n",
    "            clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1} | Train loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # Evaluate after each epoch\n",
    "        if val_loader is not None:\n",
    "            evaluate_model(model, val_loader, device=device)\n",
    "\n",
    "    print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e664fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 732/732 [20:32<00:00,  1.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train loss: 0.5516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval | Acc: 0.7789 | F1: 0.8518 | Precision: 0.9086 | Recall: 0.8017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 732/732 [16:06<00:00,  1.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Train loss: 0.4712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval | Acc: 0.7567 | F1: 0.8303 | Precision: 0.9288 | Recall: 0.7506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 732/732 [15:07<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Train loss: 0.4277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval | Acc: 0.7497 | F1: 0.8231 | Precision: 0.9360 | Recall: 0.7345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 674/732 [13:12<01:34,  1.63s/it]"
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader, val_loader=test_loader, epochs=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
