import logging 
from datetime import date, datetime, timedelta
import time 
import sys
import os
import re # Import regex ƒë·ªÉ x·ª≠ l√Ω text t·ª´ LLM

from database.db import get_database

# --- 1. CRAWLING IMPORTS ---
from crawling.news_daily_crawl import main_news_crawling
from crawling.posts_daily_crawl import main_posts_crawling

# --- 2. ETL IMPORTS ---
try:
    from etl.summarizer import run_summarization
except ImportError:
    print("‚ö†Ô∏è Warning: Ch∆∞a t√¨m th·∫•y module 'etl.summarizer'. B∆∞·ªõc t√≥m t·∫Øt c√≥ th·ªÉ b·ªã l·ªói.")
    def run_summarization(date): pass 

from etl.graph_loader import save_graph
from etl.memory_attention import TRRMemoryAttention
from etl.reasoning import StockPredictor
from etl.extractor import build_daily_knowledge_graph_batch

# Danh m·ª•c ƒë·∫ßu t∆∞
PORTFOLIO_STOCKS = ["FPT", "SSI", "VCB", "VHM", "HPG", "GAS", "MSN", "MWG", "GVR", "VIC"]

def parse_llm_response(response_text):
    """
    H√†m t√°ch chu·ªói text c·ªßa Gemini th√†nh c√°c tr∆∞·ªùng d·ªØ li·ªáu c√≥ c·∫•u tr√∫c
    Input format:
    [TREND]: INCREASE
    [CONFIDENCE]: 85%
    [REASONING]: ...
    """
    if not response_text:
        return "UNKNOWN", "0%", "Kh√¥ng c√≥ d·ªØ li·ªáu ph√¢n t√≠ch"
    
    # S·ª≠ d·ª•ng Regex ƒë·ªÉ b·∫Øt c√°c pattern
    trend_match = re.search(r"\[TREND\]:\s*(.*)", response_text, re.IGNORECASE)
    conf_match = re.search(r"\[CONFIDENCE\]:\s*(.*)", response_text, re.IGNORECASE)
    
    # Reasoning th∆∞·ªùng l√† ph·∫ßn c√≤n l·∫°i ho·∫∑c n·∫±m trong tag
    reason_match = re.search(r"\[REASONING\]:\s*((?:.|\n)*)", response_text, re.IGNORECASE)
    
    trend = trend_match.group(1).strip().upper() if trend_match else "UNKNOWN"
    confidence = conf_match.group(1).strip() if conf_match else "0%"
    reasoning = reason_match.group(1).strip() if reason_match else response_text
    
    return trend, confidence, reasoning

def save_predictions_to_db(results, target_date_str):
    """L∆∞u k·∫øt qu·∫£ d·ª± ƒëo√°n v√†o MongoDB"""
    print("\nüíæ ƒêang l∆∞u k·∫øt qu·∫£ d·ª± ƒëo√°n v√†o MongoDB...")
    db = get_database()
    collection = db['stock_predictions']
    
    timestamp = datetime.now()
    count = 0
    
    for ticker, full_text in results.items():
        if not full_text: continue
        
        # Parse d·ªØ li·ªáu
        trend, confidence, reasoning = parse_llm_response(full_text)
        
        record = {
            "date": target_date_str,
            "symbol": ticker,
            "full_analysis": full_text, # L∆∞u vƒÉn b·∫£n g·ªëc
            "trend": trend,             # INCREASE/DECREASE/SIDEWAYS
            "confidence": confidence,   # VD: 85%
            "reasoning": reasoning,     # L√Ω do chi ti·∫øt
            "created_at": timestamp
        }
        
        # Upsert v√†o MongoDB
        collection.update_one(
            {"created_at": timestamp, "symbol": ticker},
            {"$set": record},
            upsert=True
        )
        count += 1
        
    print(f"‚úÖ ƒê√£ l∆∞u th√†nh c√¥ng {count} d·ª± ƒëo√°n v√†o collection 'stock_predictions'")

def run_full_pipeline(target_date_input):
    # --- CHU·∫®N H√ìA NG√ÄY TH√ÅNG ---
    if isinstance(target_date_input, str):
        try:
            target_date_str = target_date_input
            target_date_obj = datetime.strptime(target_date_input, "%Y-%m-%d").date()
        except ValueError:
            print(f"‚ùå L·ªói: Ng√†y '{target_date_input}' kh√¥ng ƒë√∫ng ƒë·ªãnh d·∫°ng YYYY-MM-DD")
            return
    elif isinstance(target_date_input, (date, datetime)):
        target_date_obj = target_date_input
        target_date_str = target_date_input.strftime("%Y-%m-%d")
    else:
        print("‚ùå L·ªói: Input ng√†y kh√¥ng h·ª£p l·ªá.")
        return

    print(f"\n{'='*50}")
    print(f"üöÄ B·∫ÆT ƒê·∫¶U PIPELINE NG√ÄY {target_date_str}")
    print(f"{'='*50}\n")
    
    # ---------------------------------------------------------
    # 1. Crawling news and posts
    # ---------------------------------------------------------
    print("üì∞ B∆∞·ªõc 1: Thu th·∫≠p d·ªØ li·ªáu t·ª´ m·∫°ng x√£ h·ªôi v√† tin t·ª©c...")
    try:
        main_posts_crawling(target_date_obj)
        main_news_crawling(target_date_obj)
    except Exception as e:
        print(f"‚ö†Ô∏è L·ªói Crawling (c√≥ th·ªÉ b·ªè qua n·∫øu ƒë√£ c√≥ data): {e}")
    
    # ---------------------------------------------------------
    # 2. Summarization and Graph Construction
    # ---------------------------------------------------------
    print("\nüß† B∆∞·ªõc 2: T√≥m t·∫Øt v√† x√¢y d·ª±ng ƒë·ªì th·ªã tri th·ª©c...")
    
    run_summarization() 
    
    # Check data
    db = get_database()
    summ_count = db['summarized_news'].count_documents({"date": target_date_str + " 00:00:00"})
    print(f"üìä Ki·ªÉm tra MongoDB: T√¨m th·∫•y {summ_count} b√†i b√°o ƒë√£ t√≥m t·∫Øt cho ng√†y {target_date_str + " 00:00:00"}")
    
    # Extractor
    # target_date_str += " 00:00:00"
    daily_graph = build_daily_knowledge_graph_batch(target_date_str + " 00:00:00")
    
    # ---------------------------------------------------------
    # 3. Integrate with historical graph from Neo4j
    # ---------------------------------------------------------
    print("\nüíæ B∆∞·ªõc 3: L∆∞u tr·ªØ v√† T√≠ch h·ª£p ƒë·ªì th·ªã tri th·ª©c L·ªãch s·ª≠")
    if daily_graph.number_of_nodes() > 0:
        save_graph(daily_graph)
    else:
        print("‚ö†Ô∏è ƒê·ªì th·ªã tri th·ª©c h√¥m nay r·ªóng. V·∫´n ti·∫øp t·ª•c ƒë·ªÉ l·∫•y d·ªØ li·ªáu qu√° kh·ª©.")
    
    # ---------------------------------------------------------
    # 4. MEMORY & ATTENTION (TRR)
    # ---------------------------------------------------------
    print("\nüîç B∆∞·ªõc 4: K√≠ch ho·∫°t TRR Memory & Attention...")
    trr = TRRMemoryAttention()
    
    # Fetch graph qu√° kh·ª©
    G_full = trr.fetch_historical_graph(target_date_str)
    
    # √Åp d·ª•ng PageRank ƒë·ªÉ l·ªçc
    G_attention = trr.apply_attention_mechanism(G_full, PORTFOLIO_STOCKS)
    
    # Format ra text cho LLM
    graph_context = trr.format_graph_for_llm(G_attention)
    trr.close()
    
    if not graph_context:
        graph_context = "Kh√¥ng c√≥ s·ª± ki·ªán quan tr·ªçng n√†o trong ƒë·ªì th·ªã tri th·ª©c."
        print("   -> Context r·ªóng.")
    else:
        print(f"   -> Context generated ({len(graph_context)} chars).")
    
    # ---------------------------------------------------------
    # 5. Prediction
    # ---------------------------------------------------------
    print("\nüîÆ B∆∞·ªõc 5: D·ª± ƒëo√°n xu h∆∞·ªõng (Predictor)...")
    predictor = StockPredictor()
    results = {}
    
    print(f"\nüìù Context m·∫´u g·ª≠i cho LLM:\n{graph_context[:300]}...\n")

    for ticker in PORTFOLIO_STOCKS:
        print(f"--- Ph√¢n t√≠ch {ticker} ---")
        prediction = predictor.predict(ticker, target_date_str, graph_context)
        
        print(f"   üëâ K·∫øt qu·∫£: {prediction}\n")
        results[ticker] = prediction
        
        # Ngh·ªâ nh·∫π ƒë·ªÉ tr√°nh rate limit
        time.sleep(1)

    # ---------------------------------------------------------
    # 6. SAVE RESULTS TO DATABASE (NEW STEP)
    # ---------------------------------------------------------
    save_predictions_to_db(results, target_date_str)

    print(f"\n‚úÖ HO√ÄN TH√ÄNH PIPELINE NG√ÄY {target_date_str}")
    return results

if __name__ == "__main__":
    # Ch·∫°y th·ª≠ v·ªõi ng√†y hi·ªán t·∫°i ho·∫∑c ng√†y b·∫°n mu·ªën test
    today = datetime.now().strftime("%Y-%m-%d")
    # run_full_pipeline("2023-11-20") # Uncomment ƒë·ªÉ test ng√†y c≈©
    run_full_pipeline(today)