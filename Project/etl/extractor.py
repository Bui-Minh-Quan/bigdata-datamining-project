import networkx as nx 
from concurrent.futures import ThreadPoolExecutor, as_completed
import pandas as pd
from threading import Lock
import time
import os 
import sys 
import re
from datetime import datetime

ROOT_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(ROOT_DIR)

from database.db import get_database
db = get_database()

# Import modules in project
try:
    from etl.llm_client import APIKeyManager, get_llm_chain
    from etl.prompts import BATCH_RELATION_EXTRACTION_PROMPT, ENTITY_EXTRACTION_PROMPT, RELATION_EXTRACTION_PROMPT, BATCH_ARTICLE_EXTRACTION_PROMPT
    from etl.graph_loader import save_graph
except ImportError:
    # Fallback for direct script execution
    from llm_client import APIKeyManager, get_llm_chain
    from prompts import BATCH_RELATION_EXTRACTION_PROMPT, ENTITY_EXTRACTION_PROMPT, RELATION_EXTRACTION_PROMPT, BATCH_ARTICLE_EXTRACTION_PROMPT
    from graph_loader import save_graph



graph_lock = Lock()
canonical_lock = Lock()
results_lock = Lock()

PORTFOLIO_STOCKS = ["FPT", "SSI", "VCB", "VHM", "HPG", "GAS", "MSN", "MWG", "GVR", "VIC"]
PORTFOLIO_SECTOR = ["C√¥ng ngh·ªá", "Ch·ª©ng kho√°n", "Ng√¢n h√†ng", "B·∫•t ƒë·ªông s·∫£n", "V·∫≠t li·ªáu c∆° b·∫£n", 
                     "D·ªãch v·ª• H·∫° t·∫ßng", "Ti√™u d√πng c∆° b·∫£n", "B√°n l·∫ª", "Ch·∫ø bi·∫øn", "B·∫•t ƒë·ªông s·∫£n"]
ALIAS_MAP = {
    "TH·∫æ GI·ªöI DI ƒê·ªòNG": "MWG",
    "T·∫¨P ƒêO√ÄN H√íA PH√ÅT": "HPG",
    "H√íA PH√ÅT": "HPG",
    "VINGROUP": "VIC",
    "VINHOMES": "VHM",
    "VIETCOMBANK": "VCB",
    "NG√ÇN H√ÄNG NGO·∫†I TH∆Ø∆†NG": "VCB",
    "PETROVIETNAM GAS": "GAS",
    "PV GAS": "GAS",
    "MASAN": "MSN",
    "FPT CORP": "FPT",
    "T·∫¨P ƒêO√ÄN CAO SU": "GVR"
}

PORTFOLIO_STR = ", ".join([f"{s}-{sec}" for s, sec in zip(PORTFOLIO_STOCKS, PORTFOLIO_SECTOR)])

BASE_DELAY = 30
MAX_RETRIES = 3
MAX_WORKERS = 1  

def classify_entity(entity_name):
    entity_upper = entity_name.upper().strip()
    
    # 1. Check Alias Map tr∆∞·ªõc
    if entity_upper in ALIAS_MAP:
        return "Stock", ALIAS_MAP[entity_upper]
        
    # 2. Check containment nh∆∞ ·ªü gi·∫£i ph√°p 1
    for stock in PORTFOLIO_STOCKS:
        if stock == entity_upper or f"({stock})" in entity_upper:
             return "Stock", stock
             
    return "Entity", entity_name




# ------------------------
#    CACHE FUNCTIONS 
# ------------------------
cache_collection = db['graph_extraction_cache']
cache_collection.create_index("article_id", unique=True)

def get_cached_extractions(article_ids):
    """
    L·∫•y c√°c k·∫øt qu·∫£ ƒë√£ tr√≠ch xu·∫•t t·ª´ cache
    Input: List of article_ids (strings)
    Output: Dict {article_id: data}
    """
    cursor = cache_collection.find({"article_id": {"$in": article_ids}})
    return {doc["article_id"]: doc for doc in cursor}

def save_extraction_to_cache(article_id, entities, relations):
    """
    L∆∞u k·∫øt qu·∫£ tr√≠ch xu·∫•t v√†o MongoDB ƒë·ªÉ d√πng l·∫°i sau n√†y
    """
    doc = {
        "article_id": article_id,
        "entities": list(entities),     # List of (name, type, reason)
        "relations": list(relations),   # List of (source, target, impact, reason)
        "processed_at": datetime.now()
    }
    try:
        cache_collection.update_one(
            {"article_id": article_id}, 
            {"$set": doc}, 
            upsert=True
        )
    except Exception as e:
        print(f"‚ö†Ô∏è L·ªói l∆∞u cache cho b√†i {article_id}: {e}")

# ------------------------
#    Helper functions 
# ------------------------
def parse_entity_response(response):
    """
    Ph√¢n t√≠ch response t·ª´ entity extraction prompt
    
    Returns:
        dict: {"POSITIVE": [(entity, explanation), ...], "NEGATIVE": [(entity, explanation), ...]}
    """
    if response is None:
        print("Response is None")
        return {"POSITIVE": [], "NEGATIVE": []}
        
    sections = {"POSITIVE": [], "NEGATIVE": []}
    current_section = None
    str_resp = response.content
    
    for line in str(str_resp).splitlines():
        line = line.strip()
        if not line:
            continue
        if "[[POSITIVE]]" in line.upper():
            current_section = "POSITIVE"
            continue
        if "[[NEGATIVE]]" in line.upper():
            current_section = "NEGATIVE"
            continue
        if current_section and ':' in line:
            entity = line.split(":", 1)[0].strip()
            # Skip invalid entities
            if not entity or "kh√¥ng c√≥ th·ª±c th·ªÉ n√†o" in entity.lower():
                continue
            # content = all line except entity
            content = line.split(entity, 1)[-1].strip(':').strip()
            sections[current_section].append((entity, content))

    return sections

def parse_batch_articles_response(response_content):
    if response_content is None:
        return {}
    
    content = str(response_content)
    results = {}
    # Regex linh ho·∫°t h∆°n: B·∫Øt ƒë∆∞·ª£c c·∫£ "ARTICLE_ID: 1", "Article ID: 1", v.v.
    # Flag (?i) ƒë·ªÉ kh√¥ng ph√¢n bi·ªát hoa th∆∞·ªùng
    parts = re.split(r"\[\[(?:ARTICLE_)?ID:\s*(.+?)\s*\]\]", content, flags=re.IGNORECASE)
    
    # Parts[0] l√† text th·ª´a tr∆∞·ªõc tag ƒë·∫ßu ti√™n
    for i in range(1, len(parts), 2):
        article_id = parts[i].strip().strip('"').strip("'") # B·ªè c·∫£ d·∫•u nh√°y n·∫øu c√≥
        body = parts[i+1].strip()
        results[article_id] = body 
        
    return results
    
    


def parse_batch_entity_response(response):
    """
    Ph√¢n t√≠ch response t·ª´ batch relation extraction prompt
    
    Returns:
        list: [(source_entity, impact, target_entity, content), ...]
    """
    if response is None:
        print("Response is None")
        return []
        
    results = []
    current_source = None
    current_impact = None
    current_section = None
    
    str_resp = str(response.content)
    lines = str_resp.splitlines()
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
            
        # Check for source entity marker
        if line.startswith("[[SOURCE:") or "[[SOURCE:" in line:
            source_text = line.replace("[[SOURCE:", "").replace("]]", "").strip()
            if source_text and "kh√¥ng c√≥ th·ª±c th·ªÉ n√†o" not in source_text.lower():
                current_source = source_text
            else:
                current_source = None
            continue
            
        # Check for impact marker
        if line.startswith("[[IMPACT:") or "[[IMPACT:" in line:
            impact_str = line.replace("[[IMPACT:", "").replace("]]", "").strip()
            current_impact = impact_str.upper()
            continue
            
        # Check for positive/negative section markers
        if "[[POSITIVE]]" in line.upper():
            current_section = "POSITIVE"
            continue
            
        if "[[NEGATIVE]]" in line.upper():
            current_section = "NEGATIVE"
            continue
            
        # Process entity and explanation
        if current_source and current_section and ':' in line:
            try:
                entity, *content_parts = line.split(":", 1)
                entity = entity.strip().strip('[]')
                
                if not entity or "kh√¥ng c√≥ th·ª±c th·ªÉ n√†o" in entity.lower():
                    continue
                    
                if entity and content_parts:
                    content = content_parts[0].strip()
                    actual_impact = current_impact if current_impact else current_section
                    results.append((current_source, actual_impact, entity, content))
            except Exception as e:
                print(f"Error parsing line: {line}. Error: {e}")
                continue
    
    if not results:
        print("Warning: No relationships were parsed from the response")
        
    return results

def merge_entity(entity, canonical_set):
    """
    Tr·∫£ v·ªÅ phi√™n b·∫£n canonical c·ªßa entity n·∫øu ƒë√£ t·ªìn t·∫°i (case-insensitive),
    n·∫øu kh√¥ng th√¨ th√™m v√† tr·∫£ v·ªÅ entity m·ªõi.
    
    FIX: ƒê·∫£m b·∫£o t√™n entity ƒë∆∞·ª£c l∆∞u v√† tr·∫£ v·ªÅ nh·∫•t qu√°n (gi·ªØ nguy√™n vi·∫øt hoa/th∆∞·ªùng c·ªßa l·∫ßn ƒë·∫ßu)
    """
    # Chu·∫©n h√≥a: b·ªè d·∫•u ngo·∫∑c vu√¥ng, kho·∫£ng tr·∫Øng th·ª´a
    cleaned_entity = str(entity).strip('[').strip(']').strip()
    normalized_lower = cleaned_entity.lower()
    
    # T√¨m entity ƒë√£ t·ªìn t·∫°i (case-insensitive)
    for existing_entity in canonical_set:
        if existing_entity.lower() == normalized_lower:
            # Tr·∫£ v·ªÅ entity ƒë√£ t·ªìn t·∫°i (gi·ªØ nguy√™n vi·∫øt hoa/th∆∞·ªùng c·ªßa l·∫ßn ƒë·∫ßu)
            return existing_entity
    
    # Ch∆∞a t·ªìn t·∫°i ‚Üí th√™m entity m·ªõi (GI·ªÆ NGUY√äN vi·∫øt hoa/th∆∞·ªùng g·ªëc)
    canonical_set.add(cleaned_entity)
    return cleaned_entity

def get_graph_context(G, max_entities=50):
    """
    Chuy·ªÉn ƒë·ªïi c√°c entities trong graph th√†nh chu·ªói ƒë·ªÉ ƒë∆∞a v√†o prompt
    """
    entities = [node for node in G.nodes() if not node.startswith("Article_")]
    # Gi·ªõi h·∫°n s·ªë l∆∞·ª£ng ƒë·ªÉ kh√¥ng l√†m prompt qu√° d√†i
    entities = entities[:max_entities]
    return ", ".join(entities) if entities else "Ch∆∞a c√≥ th·ª±c th·ªÉ n√†o"



def invoke_chain_with_retry(prompt_template, prompt_inputs, api_manager, base_delay=BASE_DELAY,
                            temperature=0.15, model_name="gemini-2.0-flash"):
    """
    G·ªçi LLM v·ªõi c∆° ch·∫ø retry:
    Thay ƒë·ªïi quan tr·ªçng: Nh·∫≠n v√†o prompt_template ch·ª© KH√îNG ph·∫£i chain.
    M·ªói l·∫ßn retry s·∫Ω T·∫†O L·∫†I CHAIN M·ªöI v·ªõi key m·ªõi.
    """
    total_attempts = 0
    max_total_attempts = len(api_manager.keys) * api_manager.MAX_RETRIES_PER_KEY

    while total_attempts < max_total_attempts:
        try:
            # --- ƒêI·ªÇM KH√ÅC BI·ªÜT: T·∫°o chain M·ªöI ngay trong v√≤ng l·∫∑p ---
            # ƒêi·ªÅu n√†y ƒë·∫£m b·∫£o chain lu√¥n d√πng Key hi·ªán t·∫°i c·ªßa api_manager
            chain = get_llm_chain(api_manager, prompt_template, temperature, model_name)
            # ---------------------------------------------------------
            
            response = chain.invoke(prompt_inputs)
            api_manager.reset_errors() # N·∫øu th√†nh c√¥ng th√¨ reset l·ªói
            return response

        except Exception as e:
            total_attempts += 1
            print(f"‚ö†Ô∏è L·ªói (L·∫ßn {total_attempts}): {str(e)[:100]}...")

            # B√°o l·ªói cho manager ƒë·ªÉ n√≥ chuy·ªÉn index key
            key_changed = api_manager.on_error()

            if total_attempts >= max_total_attempts:
                print("‚ùå ƒê√£ th·ª≠ t·∫•t c·∫£ keys nh∆∞ng v·∫´n l·ªói.")
                return None

            if key_changed:
                print("üîÅ ƒê√£ chuy·ªÉn API key m·ªõi -> S·∫Ω t·∫°o l·∫°i Chain ·ªü v√≤ng l·∫∑p sau.")
                delay = base_delay
            else:
                # N·∫øu ch∆∞a ƒë·ªïi key (retry l·∫°i key c≈©)
                delay = base_delay * 1.5

            print(f"‚è≥ ƒê·ª£i {delay}s...")
            time.sleep(delay)
            
    return None

            
# -----------------------------
#       Core functions
# -----------------------------
def process_batch_relations(batch, G, canonical_entities, api_manager, article_timestamp):
    # Process a single batch of entities for relation extraction
    input_text = ""
    for entity, impact, content in batch:
        input_text += f"Th·ª±c th·ªÉ: {entity}\n·∫¢nh h∆∞·ªüng nh·∫≠n ƒë∆∞·ª£c: {impact} - {content}\n---\n"
    
    with graph_lock:
        existing_entities = get_graph_context(G)
        
    prompt_inputs = {
        "input_entities": input_text,
        "portfolio": PORTFOLIO_STR,
        "existing_entities": existing_entities
    }
    
    # G·ªçi LLM
    response = invoke_chain_with_retry(BATCH_RELATION_EXTRACTION_PROMPT, prompt_inputs, api_manager)
    
    relations = parse_batch_entity_response(response)
    new_entities = []
    
    for source, impact, target, content in relations:
        with canonical_lock:
            raw_source = merge_entity(source, canonical_entities)
            raw_target = merge_entity(target, canonical_entities)
        
        # --- ƒêO·∫†N S·ª¨A QUAN TR·ªåNG: Ph√¢n lo·∫°i l·∫°i Source v√† Target ---
        type_source, canon_source = classify_entity(raw_source)
        type_target, canon_target = classify_entity(raw_target)
            
        with graph_lock:
            # Add nodes (D√πng t√™n ƒë√£ chu·∫©n h√≥a)
            if not G.has_node(canon_source):
                G.add_node(canon_source, type=type_source, updated_at=article_timestamp)
            
            if not G.has_node(canon_target):
                G.add_node(canon_target, type=type_target, updated_at=article_timestamp)
                
            # Add edge
            if not G.has_edge(canon_source, canon_target):
                G.add_edge(canon_source, canon_target, impact=impact, description=content, date=article_timestamp)
        
        new_entities.append((canon_target, impact, content))
        
    return new_entities

def process_single_article(article, G, canonical_entities, api_manager):
    # Process a single article to extract entities and relations
    article_id = article['_id']
    title = article.get("title", "")
    description = article.get("description", "")
    date_str = article.get("date", "")
    stock_codes = ", ".join(article.get("stockCodes", []))
    
    print(f"Processing: {title[:30]}...")
    
    # Node article 
    article_node = f"Article_{article_id}"
    
    with graph_lock:
        if not G.has_node(article_node):
            G.add_node(article_node, type="Article", title=title, date=date_str)
    
    # Phase 1: Entity Extraction
    with graph_lock:
        existing = get_graph_context(G)
    
    prompt_inputs = {
        "date": date_str,
        "stockCodes": stock_codes,
        "title": title,
        "description": description,
        "portfolio": PORTFOLIO_STR,
        "existing_entities": existing
    }
    
    # --- S·ª¨A: ƒê·ªïi BATCH_RELATION... th√†nh ENTITY_EXTRACTION_PROMPT ---
    response = invoke_chain_with_retry(ENTITY_EXTRACTION_PROMPT, prompt_inputs, api_manager)
    # -----------------------------------------------------------------

    entities_dict = parse_entity_response(response)
    
    frontier = [] # List off entities to run Phase 2
    
    # Process the results from Phase 1
    for impact_type, items in entities_dict.items():
        for entity_name, reason in items:
            with canonical_lock:
                canon_entity = merge_entity(entity_name, canonical_entities)
            
            with graph_lock:
                # create node Entity
                if not G.has_node(canon_entity):
                    node_type = "Stock" if canon_entity in PORTFOLIO_STOCKS else "Entity"
                    G.add_node(canon_entity, type=node_type, updated_at=date_str)
                
                # create edge Article -> Entity
                G.add_edge(article_node, canon_entity, impact=impact_type, description=reason, date=date_str)
                
                frontier.append((canon_entity, impact_type, reason))
    
    # Phase 2: Relation Extraction for entities in frontier
    BATCH_SIZE = 50
    if frontier: 
        for i in range(0, len(frontier), BATCH_SIZE):
            batch = frontier[i: i + BATCH_SIZE]
            # call process_batch_relations
            process_batch_relations(batch, G, canonical_entities, api_manager, date_str)
            
            
# Process a batch of articles            
def process_article_batch2(articles_batch, G, canonical_entities, api_manager):
    batch_content = ""
    
    id_map = {}
    
    for article in articles_batch:
        # S·ª¨A 1: √âp ki·ªÉu _id v·ªÅ string ngay l·∫≠p t·ª©c ƒë·ªÉ l√†m key th·ªëng nh·∫•t
        article_id = str(article['_id'])
        
        title = article.get("title", "Kh√¥ng c√≥ ti√™u ƒë·ªÅ")
        description = article.get("summary", article.get("description", "Kh√¥ng c√≥ m√¥ t·∫£"))
        
        # S·ª¨A 2: Ch·ªâ ƒë∆∞a article_id v√†o prompt, kh√¥ng ƒë∆∞a c·∫£ object article
        batch_content += f"[ID: {article_id}] Ti√™u ƒë·ªÅ: {title} | N·ªôi dung t√≥m t·∫Øt: {description}\n\n"
        
        # L∆∞u v√†o map v·ªõi key l√† string
        id_map[article_id] = article
        
    # get context from graph
    with graph_lock:
        existing = get_graph_context(G)
    
    prompt_inputs = {
        "batch_content": batch_content,
        "existing_entities": existing,
        "portfolio": PORTFOLIO_STR
    }
    
    # Call LLM
    try:
        # ƒê·∫£m b·∫£o b·∫°n ƒë√£ import BATCH_ARTICLE_EXTRACTION_PROMPT
        response = invoke_chain_with_retry(BATCH_ARTICLE_EXTRACTION_PROMPT, prompt_inputs, api_manager)
        if not response: return
    except Exception as e:
        print(f"Error invoking LLM for batch: {e}")
        return

    # Parse response
    parsed_results = parse_batch_articles_response(response.content)
    print(f"‚úì Batch {len(articles_batch)} b√†i -> Tr√≠ch xu·∫•t ƒë∆∞·ª£c cho {len(parsed_results)} b√†i.")
    
    # Process each article's results
    frontier = [] 
    
    for article_id_str, extraction_text in parsed_results.items():
        # S·ª¨A 3: Strip kho·∫£ng tr·∫Øng th·ª´a c·ªßa ID do LLM sinh ra
        clean_id = article_id_str.strip()
        
        # L√∫c n√†y id_map key l√† string, clean_id c≈©ng l√† string => T√¨m th·∫•y nhau
        original_article = id_map.get(clean_id)
        
        if not original_article:
            # Debug: In ra ƒë·ªÉ bi·∫øt t·∫°i sao kh√¥ng kh·ªõp
            # print(f"Warning: Kh√¥ng t√¨m th·∫•y ID '{clean_id}' trong map g·ªëc.") 
            continue
        
        # Mock response object for parse_entity_response
        class MockResponse:
            content = extraction_text
            
        entities_dict = parse_entity_response(MockResponse())
        date_str = original_article.get("date", "")
        
        # create article node
        article_node = f"Article_{clean_id}"
        with graph_lock:
            if not G.has_node(article_node):
                G.add_node(article_node, type="Article", title=original_article.get("title", ""), date=date_str)
        
        # Save entities and edges
        for impact_type, items in entities_dict.items():
            for entity_name, reason in items:
                with canonical_lock:
                    canon_entity = merge_entity(entity_name, canonical_entities)
                
                with graph_lock:
                    if not G.has_node(canon_entity):
                        is_stock = any(stock in canon_entity for stock in PORTFOLIO_STOCKS)
                        node_type = "Stock" if is_stock else "Entity"
                        # node_type = "Stock" if canon_entity in PORTFOLIO_STOCKS else "Entity"
                        G.add_node(canon_entity, type=node_type, updated_at=date_str)
                    
                    # Add edge
                    if not G.has_edge(article_node, canon_entity):
                        G.add_edge(article_node, canon_entity, impact=impact_type, description=reason, date=date_str)
                        
                        # Add to frontier for Phase 2
                        frontier.append((canon_entity, impact_type, reason))
                        
    # Phase 2: Relation Extraction
    BATCH_SIZE = 50
    if frontier:
        unique_frontier = list(set(frontier))
        for i in range(0, len(unique_frontier), BATCH_SIZE):
            batch = unique_frontier[i: i + BATCH_SIZE]
            process_batch_relations(batch, G, canonical_entities, api_manager, articles_batch[0].get('date', ''))
            

# H√ÄM X·ª¨ L√ù BATCH (ƒê√É FIX L·ªñI ID + FIX L·ªñI STOCK)
def process_article_batch(articles_batch, G, canonical_entities, api_manager):
    # 1. T√°ch b√†i n√†o ƒê√É L√ÄM v√† CH∆ØA L√ÄM
    article_ids = [str(a['_id']) for a in articles_batch]
    cached_data = get_cached_extractions(article_ids)
    
    articles_to_process = [] 
    articles_done = []       
    
    for article in articles_batch:
        a_id = str(article['_id'])
        if a_id in cached_data:
            articles_done.append(article)
        else:
            articles_to_process.append(article)
            
    # 2. D·ª±ng ƒë·ªì th·ªã cho b√†i ƒê√É L√ÄM (T·ª´ Cache)
    if articles_done:
        build_graph_from_cache(articles_done, cached_data, G, canonical_entities)

    # 3. X·ª≠ l√Ω b√†i CH∆ØA L√ÄM (G·ªçi LLM)
    if not articles_to_process:
        return 

    print(f"‚ö° ƒêang x·ª≠ l√Ω {len(articles_to_process)} b√†i m·ªõi b·∫±ng LLM...")
    
    batch_content = ""
    index_map = {} # Map ID gi·∫£ (1,2,3) -> Article th·∫≠t
    
    for idx, article in enumerate(articles_to_process, 1):
        simulated_id = str(idx)
        index_map[simulated_id] = article
        
        title = article.get("title", "Kh√¥ng c√≥ ti√™u ƒë·ªÅ")
        description = article.get("description") or article.get("summary") or ""
        if not description and article.get("content"):
             description = article.get("content")[:200] + "..."
             
        batch_content += f"[ID: {simulated_id}] Ti√™u ƒë·ªÅ: {title} | N·ªôi dung: {description}\n\n"
        
    with graph_lock:
        existing = get_graph_context(G)
    
    prompt_inputs = {
        "batch_content": batch_content,
        "existing_entities": existing,
        "portfolio": PORTFOLIO_STR
    }
    
    try:
        response = invoke_chain_with_retry(BATCH_ARTICLE_EXTRACTION_PROMPT, prompt_inputs, api_manager)
        if not response: return
    except Exception as e:
        print(f"Error invoking LLM for batch: {e}")
        return

    # Parse response (H√†m parse n√†y c·∫ßn regex linh ho·∫°t nh∆∞ ƒë√£ b√†n)
    parsed_results = parse_batch_articles_response(response.content)
    
    frontier = [] 
    
    for simulated_id, extraction_text in parsed_results.items():
        clean_id = simulated_id.strip().strip('"').strip("'")
        original_article = index_map.get(clean_id)
        
        if not original_article:
            continue
        
        real_mongo_id = str(original_article['_id'])
        
        class MockResponse: content = extraction_text
        entities_dict = parse_entity_response(MockResponse())
        date_str = original_article.get("date", "")
        
        # Create article node
        article_node = f"Article_{real_mongo_id}"
        with graph_lock:
            if not G.has_node(article_node):
                G.add_node(article_node, type="Article", title=original_article.get("title", ""), date=date_str)
        
        cache_entities = [] 
        
        for impact_type, items in entities_dict.items():
            for entity_name, reason in items:
                with canonical_lock:
                    raw_entity = merge_entity(entity_name, canonical_entities)
                
                # --- ƒêO·∫†N S·ª¨A QUAN TR·ªåNG: √âP KI·ªÇU V·ªÄ STOCK ---
                node_type, canon_entity = classify_entity(raw_entity)
                
                with graph_lock:
                    # T·∫°o node v·ªõi t√™n ƒë√£ chu·∫©n h√≥a (VD: HPG thay v√¨ T·∫≠p ƒëo√†n H√≤a Ph√°t)
                    if not G.has_node(canon_entity):
                        G.add_node(canon_entity, type=node_type, updated_at=date_str)
                    
                    if not G.has_edge(article_node, canon_entity):
                        G.add_edge(article_node, canon_entity, impact=impact_type, description=reason, date=date_str)
                        frontier.append((canon_entity, impact_type, reason))
                
                cache_entities.append((canon_entity, impact_type, reason))
        
        # L∆∞u cache v·ªõi ID th·∫≠t
        save_extraction_to_cache(real_mongo_id, cache_entities, []) 

    # Phase 2
    BATCH_SIZE = 50
    if frontier:
        unique_frontier = list(set(frontier))
        for i in range(0, len(unique_frontier), BATCH_SIZE):
            batch = unique_frontier[i: i + BATCH_SIZE]
            process_batch_relations(batch, G, canonical_entities, api_manager, articles_to_process[0].get('date', ''))

# Function to build graph from cached data
def build_graph_from_cache(articles, cached_data, G, canonical_entities):
    """
    D·ª±ng ƒë·ªì th·ªã t·ª´ d·ªØ li·ªáu ƒë√£ cache (kh√¥ng t·ªën API)
    """
    count = 0
    for article in articles:
        a_id = str(article['_id'])
        cache = cached_data.get(a_id)
        if not cache: continue
        
        count += 1
        date_str = article.get('date', '')
        
        # 1. T·∫°o node Article
        article_node = f"Article_{a_id}"
        with graph_lock:
            if not G.has_node(article_node):
                G.add_node(article_node, type="Article", title=article.get("title", ""), date=date_str)
                
        # 2. T√°i t·∫°o Nodes & Edges t·ª´ cache
        for ent_name, ent_type, reason in cache.get('entities', []):
            with canonical_lock:
                canon_entity = merge_entity(ent_name, canonical_entities)
            with graph_lock:
                if not G.has_node(canon_entity):
                    # Check l·∫°i type cho ch·∫Øc
                    is_stock = any(stock in canon_entity for stock in PORTFOLIO_STOCKS)
                    final_type = "Stock" if is_stock else "Entity"
                    G.add_node(canon_entity, type=final_type, updated_at=date_str)
                # Edge Article -> Entity
                if not G.has_edge(article_node, canon_entity):
                    impact = "RELATED" # Default if missing
                    # Logic c≈© cache l∆∞u (name, type, reason), impact ·ªü ƒë√¢u?
                    # N·∫øu c·∫•u tr√∫c cache c≈© thi·∫øu, ta t·∫°m ƒë·ªÉ RELATED ho·∫∑c l·∫•y t·ª´ reason
                    G.add_edge(article_node, canon_entity, description=reason, date=date_str)
                    
    print(f"‚úÖ ƒê√£ t√°i t·∫°o {count} b√†i b√°o t·ª´ Cache.")

# ---- MAIN FUNCTION ----
def build_daily_knowledge_graph(target_date):
    """ 
    Input: Target date to build knowledge graph (YYYY-MM-DD)
    Output: networkx graph object
    """
    print(f"Building knowledge graph for date: {target_date}")
    
    # 1. Get summarized articles in target date from mongoDB
    cursor = db['summarized_news'].find({"date": target_date})
    articles = list(cursor)
    articles = articles[:20]  # Limit to first 20 articles for performance during testing
    
    if not articles:
        print("No articles found for the target date.")
        return nx.DiGraph()
    
    print(f"Found {len(articles)} articles for date {target_date}")
    
    # 2. Initialize graph and canonical entity set
    G = nx.DiGraph()
    canonical_entities = set()
    api_manager = APIKeyManager()
    
    # 3. Process articles with ThreadPoolExecutor
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = [
            executor.submit(process_single_article, article, G, canonical_entities, api_manager)
            for article in articles
        ]
        
        # wait for all to complete
        for future in as_completed(futures):
            try:
                future.result()
            except Exception as e:
                print(f"Error processing article: {e}")
    
    print(f"Knowledge graph construction completed. Total nodes: {G.number_of_nodes()}, Total edges: {G.number_of_edges()}")
    return G

def build_daily_knowledge_graph_batch(target_date):
    """ 
    Input: Target date to build knowledge graph (YYYY-MM-DD)
    Output: networkx graph object
    """
    print(f"Building knowledge graph for date: {target_date}")
    
    # 1. Get summarized articles in target date from mongoDB
    cursor = db['summarized_news'].find({"date": target_date})
    articles = list(cursor)
    # articles = articles[:40]  # Limit to first 20 articles for performance during testing
    
    if not articles:
        print("No articles found for the target date.")
        return nx.DiGraph()
    
    print(f"Found {len(articles)} articles for date {target_date}")
    
    # 2. Initialize graph and canonical entity set
    G = nx.DiGraph()
    canonical_entities = set()
    api_manager = APIKeyManager()
    
    # 3. Process articles in batches
    BATCH_SIZE = 20
    for i in range(0, len(articles), BATCH_SIZE):
        batch = articles[i: i + BATCH_SIZE]
        process_article_batch(batch, G, canonical_entities, api_manager)
        # wait for 4 seconds
        time.sleep(4)
        
        
    
    print(f"Knowledge graph construction completed. Total nodes: {G.number_of_nodes()}, Total edges: {G.number_of_edges()}")
    return G

if __name__ == "__main__":
    # Example run
    # G = build_daily_knowledge_graph("2025-11-19 00:00:00")
    # print("Sample nodes:", list(G.nodes(data=True))[:5])
    
    G = build_daily_knowledge_graph_batch("2025-11-19 00:00:00")
    # print("Sample nodes:", list(G.nodes(data=True))[:5])
    if G.number_of_nodes() > 0:
        try:
            save_graph(G)
        except Exception as e:
            print(f"Error saving graph to Neo4j: {e}") 
    else :
        print("Graph is empty, not saving to Neo4j.")