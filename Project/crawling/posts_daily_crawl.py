import os
import time
import random
import logging
from datetime import datetime, timedelta, timezone
from typing import List, Dict, Optional

import requests
import pandas as pd
from dateutil import parser

# Import h√†m preprocessing t·ª´ file c√πng th∆∞ m·ª•c
try:
    from .data_preprocessing import preprocess_posts_df
except ImportError:
    from data_preprocessing import preprocess_posts_df

# ===========================
# CONFIG
# ===========================
API_URL = "https://api.fireant.vn/posts"
# L·∫•y Bearer Token t·ª´ bi·∫øn m√¥i tr∆∞·ªùng ho·∫∑c d√πng token m·∫∑c ƒë·ªãnh
AUTH_BEARER = os.getenv("FIREANT_BEARER")
if not AUTH_BEARER:
    AUTH_BEARER = "eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiIsIng1dCI6IkdYdExONzViZlZQakdvNERWdjV4QkRITHpnSSIsImtpZCI6IkdYdExONzViZlZQakdvNERWdjV4QkRITHpnSSJ9.eyJpc3MiOiJodHRwczovL2FjY291bnRzLmZpcmVhbnQudm4iLCJhdWQiOiJodHRwczovL2FjY291bnRzLmZpcmVhbnQudm4vcmVzb3VyY2VzIiwiZXhwIjoyMDYyMTA0NDA5LCJuYmYiOjE3NjIxMDQ0MDksImNsaWVudF9pZCI6ImZpcmVhbnQud2ViIiwic2NvcGUiOlsib3BlbmlkIiwicHJvZmlsZSIsInJvbGVzIiwiZW1haWwiLCJhY2NvdW50cy1yZWFkIiwiYWNjb3VudHMtd3JpdGUiLCJvcmRlcnMtcmVhZCIsIm9yZGVycy13cml0ZSIsImNvbXBhbmllcy1yZWFkIiwiaW5kaXZpZHVhbHMtcmVhZCIsImZpbmFuY2UtcmVhZCIsInBvc3RzLXdyaXRlIiwicG9zdHMtcmVhZCIsInN5bWJvbHMtcmVhZCIsInVzZXItZGF0YS1yZWFkIiwidXNlci1kYXRhLXdyaXRlIiwidXNlcnMtcmVhZCIsInNlYXJjaCIsImFjYWRlbXktcmVhZCIsImFjYWRlbXktd3JpdGUiLCJibG9nLXJlYWQiLCJpbnZlc3RvcGVkaWEtcmVhZCJdLCJzdWIiOiIyMWU5OTg0NC03ODljLTQxMGMtYWU4ZC00MmE0N2MwOGM4NDUiLCJhdXRoX3RpbWUiOjE3NjIxMDQ0MDksImlkcCI6Ikdvb2dsZSIsIm5hbWUiOiJidWltaW5ocXVhbmR6MjAwNUBnbWFpbC5jb20iLCJzZWN1cml0eV9zdGFtcCI6IjQ5ODExNzUxLWE5YWMtNDliMy1hNzBmLWFiNWVlMzkyZTcwZCIsImp0aSI6IjVhNDEzYTFhY2U1YjYyZGMyMWUwNjc0NWQ3MzFkMjQyIiwiYW1yIjpbImV4dGVybmFsIl19.cVnpsLaA50c-xTXx-5xJRr0TldZrH3Owu1z7i6qyq6eHR7aFIHyXa4ooMU12E6tg8fQYZR01kU2OBvgc_wQTqzQaFLX6d3eZmB8rA4b8RD1s2DzIoo3f1BlK_gDQRcE_iLRTfmDHwRZk4GvPLGKPkm24wAYd0gAwB9RcrtA4wz77w9IOHSVpqgrDKX_ww-U947cYeiGLBbgaSz7IIM0j30f_ZFPDUOGnVrGnS4bQZCyuqsZiMizUr5A3ivJuz8mXMlxMIaJbS_LwMTwsBY2FBn-hPBXrVozjYZxh22C9PfJ6U0kjV_UO4nUeKWMen3kbRdZW7lzR3TohDQlU9DXDXw"

# C·∫•u h√¨nh Crawl
TARGET_DAY_DEFAULT = (datetime.now() - timedelta(days=1)).date()  # M·∫∑c ƒë·ªãnh l√† h√¥m qua
LIMIT = 1000         # S·ªë l∆∞·ª£ng b√†i vi·∫øt m·ªói l·∫ßn request
PROBE_LIMIT = 1      # S·ªë l∆∞·ª£ng b√†i vi·∫øt thƒÉm d√≤ ƒë·ªÉ ki·ªÉm tra ng√†y
OFFSET_STEP = LIMIT  # B∆∞·ªõc nh·∫£y offset
MAX_RETRY = 5        # S·ªë l·∫ßn th·ª≠ l·∫°i khi l·ªói m·∫°ng
REQUEST_TIMEOUT = 20 # Gi√¢y
MIN_DELAY = 0.3      # Gi√¢y
MAX_DELAY = 1.2      # Gi√¢y

# C·∫•u h√¨nh Logging
logging.basicConfig(
    level=logging.INFO,
    format="[%(asctime)s] %(levelname)s - %(message)s",
)
logger = logging.getLogger("posts_crawler")

# =========================
# Helpers: API calls
# =========================
def _headers() -> Dict[str, str]:
    return {
        "Authorization": f"Bearer {AUTH_BEARER}",
        "User-Agent": "Mozilla/5.0",
        "Accept": "application/json",
    }
    
def api_get(params: Dict[str, object], max_retry: int = MAX_RETRY) -> Optional[List[Dict]]:
    """G·ªçi API c√≥ c∆° ch·∫ø th·ª≠ l·∫°i (Retry) v√† ch·ªù (Backoff)"""
    retry = 0
    backoff = 1.0
    while retry < max_retry:
        try:
            r = requests.get(API_URL, headers=_headers(), params=params, timeout=REQUEST_TIMEOUT)
            if r.status_code == 200:
                return r.json()
            
            logger.warning(f"API {API_URL} status={r.status_code}, retry={retry+1}/{max_retry}")
        except requests.RequestException as e:
            logger.warning(f"Request exception: {e} (retry {retry+1}/{max_retry})")
            
        time.sleep(backoff + random.uniform(0, 0.5))
        backoff *= 2
        retry += 1
        
    logger.error("Max retry reached -> returning None")
    return None

# =========================
# Core crawl logic (REWRITTEN)
# =========================
def crawl_posts(target_day) -> List[Dict]:
    """
    H√†m crawl posts ch√≠nh.
    Input: target_day (str "YYYY-MM-DD" ho·∫∑c datetime.date object)
    Output: List[Dict] (Danh s√°ch c√°c b√†i raw posts)
    """
    
    # 1. Chu·∫©n h√≥a ng√†y th√°ng (S·ª¨A L·ªñI TYPE ERROR T·∫†I ƒê√ÇY)
    if isinstance(target_day, str):
        try:
            target_day = datetime.strptime(target_day, "%Y-%m-%d").date()
        except ValueError:
            logger.error(f"‚ùå ƒê·ªãnh d·∫°ng ng√†y kh√¥ng h·ª£p l·ªá: {target_day}. D√πng YYYY-MM-DD.")
            return []
            
    if target_day is None:
        target_day = TARGET_DAY_DEFAULT
    
    # 2. X√°c ƒë·ªãnh khung th·ªùi gian (Time Window) theo UTC
    # Fireant d√πng gi·ªù UTC, ta l·∫•y t·ª´ 00:00:00 ng√†y target ƒë·∫øn 00:00:00 ng√†y h√¥m sau
    day_start = datetime.combine(target_day, datetime.min.time()).replace(tzinfo=timezone.utc)
    day_end = day_start + timedelta(days=1)
    
    logger.info(f"üöÄ B·∫Øt ƒë·∫ßu crawl Posts cho ng√†y: {target_day} (Window UTC: {day_start} -> {day_end})")
    
    offset = 0
    collected_raw: List[Dict] = []
    
    while True:
        # --- B∆∞·ªõc A: ThƒÉm d√≤ (Probe) ---
        # Ki·ªÉm tra nhanh 1 b√†i t·∫°i v·ªã tr√≠ offset hi·ªán t·∫°i xem ƒëang ·ªü ng√†y n√†o
        probe_params = {"type": 0, "offset": offset, "limit": PROBE_LIMIT}
        probe = api_get(probe_params)
        
        if probe is None: # L·ªói m·∫°ng fatal
            break 
        
        if len(probe) == 0:
            logger.info("Probe tr·∫£ v·ªÅ r·ªóng -> ƒê√£ h·∫øt d·ªØ li·ªáu tr√™n server -> D·ª´ng.")
            break
        
        # L·∫•y ng√†y c·ªßa b√†i vi·∫øt thƒÉm d√≤
        try:
            probe_date = parser.isoparse(probe[0].get("date"))
        except Exception:
            logger.warning("Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c ng√†y c·ªßa b√†i probe -> B·ªè qua batch n√†y.")
            offset += OFFSET_STEP
            continue
            
        # logger.info(f"Probe t·∫°i offset {offset} -> Ng√†y b√†i vi·∫øt: {probe_date}")

        # --- B∆∞·ªõc B: Quy·∫øt ƒë·ªãnh d·ª±a tr√™n ng√†y thƒÉm d√≤ ---
        
        # Tr∆∞·ªùng h·ª£p 1: B√†i thƒÉm d√≤ n·∫±m trong T∆Ø∆†NG LAI so v·ªõi ng√†y c·∫ßn l·∫•y
        # (V√≠ d·ª•: C·∫ßn l·∫•y ng√†y 19, probe ƒëang th·∫•y ng√†y 20) -> Nh·∫£y c√≥c
        if probe_date >= day_end:
            # logger.info(f"ƒêang ·ªü v√πng d·ªØ li·ªáu m·ªõi ({probe_date}) -> Nh·∫£y offset ƒë·ªÉ t√¨m ng√†y {target_day}...")
            offset += OFFSET_STEP
            time.sleep(random.uniform(MIN_DELAY, 0.5)) # Delay ng·∫Øn ƒë·ªÉ l∆∞·ªõt nhanh
            continue
            
        # Tr∆∞·ªùng h·ª£p 2: B√†i thƒÉm d√≤ n·∫±m trong QU√Å KH·ª® so v·ªõi ng√†y c·∫ßn l·∫•y
        # (V√≠ d·ª•: C·∫ßn l·∫•y ng√†y 19, probe th·∫•y ng√†y 18) -> ƒê√£ ƒëi qu√° xa -> D·ª´ng
        if probe_date < day_start:
            logger.info(f"ƒê√£ ch·∫°m v√πng d·ªØ li·ªáu c≈© ({probe_date}) -> D·ª´ng crawl.")
            break
            
        # Tr∆∞·ªùng h·ª£p 3: B√†i thƒÉm d√≤ n·∫±m TRONG ng√†y c·∫ßn l·∫•y (ho·∫∑c g·∫ßn ƒë√≥) -> T·∫£i th·∫≠t
        # logger.info(f"üéØ T√¨m th·∫•y d·ªØ li·ªáu m·ª•c ti√™u. T·∫£i batch t·∫°i offset {offset}...")
        
        batch_params = {"type": 0, "offset": offset, "limit": LIMIT}
        batch = api_get(batch_params)
        
        if not batch:
            break
            
        # L·ªçc chi ti·∫øt t·ª´ng b√†i trong batch
        items_in_batch = 0
        for p in batch:
            try:
                dt = parser.isoparse(p.get("date"))
            except:
                continue

            # N·∫øu b√†i vi·∫øt > day_end (v·∫´n c√≤n m·ªõi qu√°, do batch c√≥ th·ªÉ tr·ªôn l·∫´n): B·ªè qua
            if dt >= day_end:
                continue
                
            # N·∫øu b√†i vi·∫øt < day_start (ƒë√£ sang ng√†y h√¥m tr∆∞·ªõc): D·ª´ng to√†n b·ªô
            if dt < day_start:
                logger.info(f"G·∫∑p b√†i vi·∫øt c≈© ({dt}) trong batch -> Ho√†n t·∫•t.")
                # L∆∞u √Ω: Return lu√¥n t·∫°i ƒë√¢y v√¨ Fireant s·∫Øp x·∫øp theo th·ªùi gian gi·∫£m d·∫ßn
                return collected_raw 

            # N·∫øu ƒë√∫ng ng√†y
            collected_raw.append(p)
            items_in_batch += 1
            
        logger.info(f"   + ƒê√£ l·∫•y {items_in_batch} b√†i t·ª´ batch (Offset: {offset})")
        
        # TƒÉng offset ƒë·ªÉ l·∫•y batch ti·∫øp theo
        offset += OFFSET_STEP
        time.sleep(random.uniform(MIN_DELAY, MAX_DELAY))
        
    logger.info(f"‚úÖ Ho√†n th√†nh crawl Posts. T·ªïng s·ªë b√†i raw: {len(collected_raw)}")
    return collected_raw

# =========================
# Post Processing & Saving
# =========================
def process_post(raw: dict) -> Optional[Dict]:
    """L√†m s·∫°ch m·ªôt b√†i post raw"""
    try:
        postID = raw.get("postID")
        if not postID: return None
        
        # Clean Date
        date_raw = raw.get("date")
        try: 
            date_str = parser.isoparse(date_raw).isoformat() if date_raw else None
        except:
            date_str = date_raw
            
        # Clean Symbols
        tagged = raw.get("taggedSymbols") or []
        cleaned_symbols = [s.get("symbol") for s in tagged if isinstance(s, dict) and "symbol" in s]
        
        return {
            "postID": postID,
            "date": date_str,
            "originalContent": raw.get("originalContent"),
            "sentiment": raw.get("sentiment"),
            "taggedSymbols": cleaned_symbols
        }
    except Exception:
        return None

def save_posts_to_db(processed: List[Dict]):
    """Chuy·ªÉn list dict th√†nh DataFrame v√† g·ªçi h√†m preprocess ƒë·ªÉ l∆∞u DB"""
    if not processed:
        return
    
    df = pd.DataFrame(processed)
    # G·ªçi h√†m t·ª´ data_preprocessing.py (ƒë√£ c√≥ logic l∆∞u MongoDB)
    preprocess_posts_df(df)

# =========================
# Main Entry Point
# =========================
def main_posts_crawling(target_day=None):
    """H√†m ch√≠nh ƒë·ªÉ g·ªçi t·ª´ b√™n ngo√†i pipeline"""
    # 1. Crawl
    raw_posts = crawl_posts(target_day)
    
    if not raw_posts:
        logger.warning("Kh√¥ng t√¨m th·∫•y b√†i vi·∫øt n√†o.")
        return

    # 2. Process
    logger.info("ƒêang x·ª≠ l√Ω d·ªØ li·ªáu th√¥...")
    processed_posts = []
    seen_ids = set()

    for r in raw_posts:
        out = process_post(r)
        if out:
            pid = out.get("postID")
            if pid not in seen_ids:
                seen_ids.add(pid)
                processed_posts.append(out)
    
    # 3. Save
    if processed_posts:
        logger.info(f"L∆∞u {len(processed_posts)} b√†i vi·∫øt v√†o Database...")
        save_posts_to_db(processed_posts)
    else:
        logger.warning("Kh√¥ng c√≥ b√†i vi·∫øt h·ª£p l·ªá sau khi x·ª≠ l√Ω.")

if __name__ == "__main__":
    main_posts_crawling()