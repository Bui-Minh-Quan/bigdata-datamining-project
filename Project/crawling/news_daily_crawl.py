import os
import time
import random
import logging
from datetime import datetime, timedelta, timezone
from typing import List, Dict, Optional

import requests
import pandas as pd
from dateutil import parser

# Import h√†m preprocessing
try:
    from .data_preprocessing import preprocess_news_df
except ImportError:
    from data_preprocessing import preprocess_news_df

# =========================
# CONFIG
# =========================
API_URL = "https://api.fireant.vn/posts"
AUTH_BEARER = os.getenv("FIREANT_BEARER")

if not AUTH_BEARER:
    # Token m·∫∑c ƒë·ªãnh
    AUTH_BEARER = "eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiIsIng1dCI6IkdYdExONzViZlZQakdvNERWdjV4QkRITHpnSSIsImtpZCI6IkdYdExONzViZlZQakdvNERWdjV4QkRITHpnSSJ9.eyJpc3MiOiJodHRwczovL2FjY291bnRzLmZpcmVhbnQudm4iLCJhdWQiOiJodHRwczovL2FjY291bnRzLmZpcmVhbnQudm4vcmVzb3VyY2VzIiwiZXhwIjoyMDYyMTA0NDA5LCJuYmYiOjE3NjIxMDQ0MDksImNsaWVudF9pZCI6ImZpcmVhbnQud2ViIiwic2NvcGUiOlsib3BlbmlkIiwicHJvZmlsZSIsInJvbGVzIiwiZW1haWwiLCJhY2NvdW50cy1yZWFkIiwiYWNjb3VudHMtd3JpdGUiLCJvcmRlcnMtcmVhZCIsIm9yZGVycy13cml0ZSIsImNvbXBhbmllcy1yZWFkIiwiaW5kaXZpZHVhbHMtcmVhZCIsImZpbmFuY2UtcmVhZCIsInBvc3RzLXdyaXRlIiwicG9zdHMtcmVhZCIsInN5bWJvbHMtcmVhZCIsInVzZXItZGF0YS1yZWFkIiwidXNlci1kYXRhLXdyaXRlIiwidXNlcnMtcmVhZCIsInNlYXJjaCIsImFjYWRlbXktcmVhZCIsImFjYWRlbXktd3JpdGUiLCJibG9nLXJlYWQiLCJpbnZlc3RvcGVkaWEtcmVhZCJdLCJzdWIiOiIyMWU5OTg0NC03ODljLTQxMGMtYWU4ZC00MmE0N2MwOGM4NDUiLCJhdXRoX3RpbWUiOjE3NjIxMDQ0MDksImlkcCI6Ikdvb2dsZSIsIm5hbWUiOiJidWltaW5ocXVhbmR6MjAwNUBnbWFpbC5jb20iLCJzZWN1cml0eV9zdGFtcCI6IjQ5ODExNzUxLWE5YWMtNDliMy1hNzBmLWFiNWVlMzkyZTcwZCIsImp0aSI6IjVhNDEzYTFhY2U1YjYyZGMyMWUwNjc0NWQ3MzFkMjQyIiwiYW1yIjpbImV4dGVybmFsIl19.cVnpsLaA50c-xTXx-5xJRr0TldZrH3Owu1z7i6qyq6eHR7aFIHyXa4ooMU12E6tg8fQYZR01kU2OBvgc_wQTqzQaFLX6d3eZmB8rA4b8RD1s2DzIoo3f1BlK_gDQRcE_iLRTfmDHwRZk4GvPLGKPkm24wAYd0gAwB9RcrtA4wz77w9IOHSVpqgrDKX_ww-U947cYeiGLBbgaSz7IIM0j30f_ZFPDUOGnVrGnS4bQZCyuqsZiMizUr5A3ivJuz8mXMlxMIaJbS_LwMTwsBY2FBn-hPBXrVozjYZxh22C9PfJ6U0kjV_UO4nUeKWMen3kbRdZW7lzR3TohDQlU9DXDXw"

# C·∫•u h√¨nh Crawl
LIMIT = 500          
PROBE_LIMIT = 1
OFFSET_STEP = LIMIT
MAX_RETRY = 5
REQUEST_TIMEOUT = 20
MIN_DELAY = 0.5
MAX_DELAY = 1.5

# C·∫•u h√¨nh Logging
logging.basicConfig(level=logging.INFO, format="[%(asctime)s] %(levelname)s - %(message)s")
logger = logging.getLogger("news_crawler")

# =========================
# Helpers
# =========================
def _headers() -> Dict[str, str]:
    return {
        "Authorization": f"Bearer {AUTH_BEARER}",
        "User-Agent": "Mozilla/5.0",
        "Accept": "application/json",
    }

def api_get(url: str, params: Dict[str, object], max_retry: int = MAX_RETRY) -> Optional[List[Dict]]:
    retry = 0
    backoff = 1.0
    while retry < max_retry:
        try:
            r = requests.get(url, headers=_headers(), params=params, timeout=REQUEST_TIMEOUT)
            if r.status_code == 200:
                return r.json()
            logger.warning(f"API {url} status={r.status_code}, retry={retry+1}")
        except Exception as e:
            logger.warning(f"Request exception: {e}")
        time.sleep(backoff + random.uniform(0, 0.5))
        backoff *= 2
        retry += 1
    return None

# =========================
# CORE LOGIC (ƒê√É S·ª¨A THEO Y√äU C·∫¶U)
# =========================
def crawl_news(target_day=None) -> List[Dict]:
    """
    Crawl tin t·ª©c t·ª´ th·ªùi ƒëi·ªÉm hi·ªán t·∫°i l√πi v·ªÅ qu√° kh·ª©.
    ƒêi·ªÉm d·ª´ng: 00:00:00 c·ªßa 'target_day' (ho·∫∑c h√¥m qua n·∫øu kh√¥ng truy·ªÅn).
    Logic: Qu√©t t·ª´ Offset 0 (M·ªõi nh·∫•t) -> C≈© d·∫ßn.
    """
    # 1. X√°c ƒë·ªãnh ng√†y m·ªëc (Cutoff Date)
    if target_day is None:
        # M·∫∑c ƒë·ªãnh l√† 00:00:00 h√¥m qua
        target_day = (datetime.now() - timedelta(days=1)).date()
    elif isinstance(target_day, str):
        target_day = datetime.strptime(target_day, "%Y-%m-%d").date()

    # 2. Chuy·ªÉn ƒë·ªïi sang th·ªùi gian c√≥ m√∫i gi·ªù (UTC+7 cho Vi·ªát Nam)
    # M·∫πo: Fireant d√πng UTC. 00:00:00 VN = 17:00:00 h√¥m tr∆∞·ªõc (UTC)
    # ƒê·ªÉ an to√†n v√† ƒë∆°n gi·∫£n, ta d√πng timestamp so s√°nh
    
    # M·ªëc d·ª´ng: 00:00:00 c·ªßa target_day (Gi·ªù Vi·ªát Nam)
    # => ƒê·ªïi sang UTC ƒë·ªÉ so s√°nh v·ªõi d·ªØ li·ªáu API
    vn_tz = timezone(timedelta(hours=7))
    cutoff_time_vn = datetime.combine(target_day, datetime.min.time()).replace(tzinfo=vn_tz)
    cutoff_time_utc = cutoff_time_vn.astimezone(timezone.utc)

    logger.info(f"üöÄ B·∫Øt ƒë·∫ßu crawl News t·ª´ Hi·ªán t·∫°i l√πi v·ªÅ {cutoff_time_vn} (VN) / {cutoff_time_utc} (UTC)")
    
    offset = 0
    collected_posts: List[Dict] = []
    
    while True:
        # --- B∆∞·ªõc A: L·∫•y Batch tin t·ª©c ---
        batch_params = {"type": 1, "offset": offset, "limit": LIMIT}
        batch = api_get(API_URL, batch_params)
        
        if not batch: 
            logger.info("Kh√¥ng l·∫•y ƒë∆∞·ª£c batch ho·∫∑c h·∫øt d·ªØ li·ªáu -> D·ª´ng.")
            break
        
        items_in_batch = 0
        stop_signal = False
        
        for p in batch:
            try:
                post_date = parser.isoparse(p.get("date"))
            except:
                continue
            
            # --- QUAN TR·ªåNG: ƒêi·ªÅu ki·ªán d·ª´ng ---
            # N·∫øu g·∫∑p b√†i vi·∫øt C≈® H∆†N m·ªëc cutoff -> D·ª´ng to√†n b·ªô vi·ªác crawl
            if post_date < cutoff_time_utc:
                logger.info(f"üõë G·∫∑p tin c≈© ({post_date}) < Cutoff ({cutoff_time_utc}) -> D·ª´ng.")
                stop_signal = True
                break # Tho√°t v√≤ng for

            # N·∫øu b√†i vi·∫øt >= Cutoff -> L·∫•y chi ti·∫øt
            post_id = p.get("postID")
            if post_id:
                # G·ªçi API Detail ƒë·ªÉ l·∫•y n·ªôi dung full
                detail = api_get(f"{API_URL}/{post_id}", {})
                if detail:
                    collected_posts.append(detail)
                    items_in_batch += 1
        
        logger.info(f"   + Batch {offset}: L·∫•y ƒë∆∞·ª£c {items_in_batch} tin m·ªõi.")
        
        # N·∫øu g·∫∑p t√≠n hi·ªáu d·ª´ng trong batch v·ª´a r·ªìi -> Tho√°t v√≤ng while
        if stop_signal:
            break
            
        # N·∫øu batch tr·∫£ v·ªÅ √≠t h∆°n limit (t·ª©c l√† trang cu·ªëi) -> C≈©ng d·ª´ng
        if len(batch) < LIMIT:
            logger.info("ƒê√£ ƒë·∫øn trang cu·ªëi c√πng c·ªßa API -> D·ª´ng.")
            break

        # TƒÉng offset ƒë·ªÉ l·∫•y trang ti·∫øp theo (l√πi xa h∆°n v·ªÅ qu√° kh·ª©)
        offset += OFFSET_STEP
        time.sleep(random.uniform(MIN_DELAY, MAX_DELAY))

    logger.info(f"‚úÖ Ho√†n th√†nh. T·ªïng s·ªë tin thu th·∫≠p: {len(collected_posts)}")
    return collected_posts

# =========================
# Processing & Saving
# =========================
def process_post(post: Dict) -> Optional[Dict]:
    """L√†m s·∫°ch d·ªØ li·ªáu"""
    try:
        postID = post.get("postID")
        if not postID: return None
        
        date_raw = post.get("date")
        try:
            date_str = parser.isoparse(date_raw).isoformat() if date_raw else None
        except:
            date_str = date_raw
            
        tagged_symbols = post.get("taggedSymbols")
        if not isinstance(tagged_symbols, list):
            cleaned_symbols = []
        else:
            cleaned_symbols = [sym['symbol'] for sym in tagged_symbols if isinstance(sym, dict) and 'symbol' in sym]
            
        return {
            "postID": postID,
            "date": date_str,
            "title": post.get("title"),
            "description": post.get("description"),
            "originalContent": post.get("content"),
            "sentiment": post.get("sentiment"),
            "taggedSymbols": cleaned_symbols,
            "totalLikes": post.get("totalLikes", 0),
            "totalReplies": post.get("totalReplies", 0),
            "totalShares": post.get("totalShares", 0),
        }
    except Exception:
        return None

def save_news_to_db(processed: List[Dict]):
    if not processed: return
    df = pd.DataFrame(processed)
    preprocess_news_df(df)

# =========================
# Main Entry Point
# =========================
def main_news_crawling(target_day=None):
    raw_posts = crawl_news(target_day)
    
    if not raw_posts:
        logger.warning("Kh√¥ng t√¨m th·∫•y tin t·ª©c n√†o trong kho·∫£ng th·ªùi gian y√™u c·∫ßu.")
        return

    logger.info(f"ƒêang x·ª≠ l√Ω v√† l∆∞u {len(raw_posts)} tin t·ª©c...")
    processed_list = []
    seen_ids = set()
    
    for p in raw_posts:
        out = process_post(p)
        if out:
            pid = out.get("postID")
            if pid not in seen_ids:
                seen_ids.add(pid)
                processed_list.append(out)
            
    if processed_list:
        save_news_to_db(processed_list)
        logger.info("L∆∞u DB th√†nh c√¥ng.")

if __name__ == "__main__":
    # Test ch·∫°y
    target_date_str = (datetime.now() - timedelta(days=600)).strftime("%Y-%m-%d")
    main_news_crawling(target_date_str)