import os
import time
import random
import logging
from datetime import datetime, timedelta, timezone
from typing import List, Dict, Optional

import requests
import pandas as pd
from dateutil import parser

# Import h√†m preprocessing t·ª´ file c√πng th∆∞ m·ª•c
try:
    from .data_preprocessing import preprocess_news_df
except ImportError:
    from data_preprocessing import preprocess_news_df

# =========================
# CONFIG
# =========================
API_URL = "https://api.fireant.vn/posts"
AUTH_BEARER = os.getenv("FIREANT_BEARER")

if not AUTH_BEARER:
    # Token m·∫∑c ƒë·ªãnh (N√™n c·∫≠p nh·∫≠t th∆∞·ªùng xuy√™n)
    AUTH_BEARER = "eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiIsIng1dCI6IkdYdExONzViZlZQakdvNERWdjV4QkRITHpnSSIsImtpZCI6IkdYdExONzViZlZQakdvNERWdjV4QkRITHpnSSJ9.eyJpc3MiOiJodHRwczovL2FjY291bnRzLmZpcmVhbnQudm4iLCJhdWQiOiJodHRwczovL2FjY291bnRzLmZpcmVhbnQudm4vcmVzb3VyY2VzIiwiZXhwIjoyMDYyMTA0NDA5LCJuYmYiOjE3NjIxMDQ0MDksImNsaWVudF9pZCI6ImZpcmVhbnQud2ViIiwic2NvcGUiOlsib3BlbmlkIiwicHJvZmlsZSIsInJvbGVzIiwiZW1haWwiLCJhY2NvdW50cy1yZWFkIiwiYWNjb3VudHMtd3JpdGUiLCJvcmRlcnMtcmVhZCIsIm9yZGVycy13cml0ZSIsImNvbXBhbmllcy1yZWFkIiwiaW5kaXZpZHVhbHMtcmVhZCIsImZpbmFuY2UtcmVhZCIsInBvc3RzLXdyaXRlIiwicG9zdHMtcmVhZCIsInN5bWJvbHMtcmVhZCIsInVzZXItZGF0YS1yZWFkIiwidXNlci1kYXRhLXdyaXRlIiwidXNlcnMtcmVhZCIsInNlYXJjaCIsImFjYWRlbXktcmVhZCIsImFjYWRlbXktd3JpdGUiLCJibG9nLXJlYWQiLCJpbnZlc3RvcGVkaWEtcmVhZCJdLCJzdWIiOiIyMWU5OTg0NC03ODljLTQxMGMtYWU4ZC00MmE0N2MwOGM4NDUiLCJhdXRoX3RpbWUiOjE3NjIxMDQ0MDksImlkcCI6Ikdvb2dsZSIsIm5hbWUiOiJidWltaW5ocXVhbmR6MjAwNUBnbWFpbC5jb20iLCJzZWN1cml0eV9zdGFtcCI6IjQ5ODExNzUxLWE5YWMtNDliMy1hNzBmLWFiNWVlMzkyZTcwZCIsImp0aSI6IjVhNDEzYTFhY2U1YjYyZGMyMWUwNjc0NWQ3MzFkMjQyIiwiYW1yIjpbImV4dGVybmFsIl19.cVnpsLaA50c-xTXx-5xJRr0TldZrH3Owu1z7i6qyq6eHR7aFIHyXa4ooMU12E6tg8fQYZR01kU2OBvgc_wQTqzQaFLX6d3eZmB8rA4b8RD1s2DzIoo3f1BlK_gDQRcE_iLRTfmDHwRZk4GvPLGKPkm24wAYd0gAwB9RcrtA4wz77w9IOHSVpqgrDKX_ww-U947cYeiGLBbgaSz7IIM0j30f_ZFPDUOGnVrGnS4bQZCyuqsZiMizUr5A3ivJuz8mXMlxMIaJbS_LwMTwsBY2FBn-hPBXrVozjYZxh22C9PfJ6U0kjV_UO4nUeKWMen3kbRdZW7lzR3TohDQlU9DXDXw"

# C·∫•u h√¨nh Crawl
TARGET_DAY_DEFAULT = (datetime.now() - timedelta(days=1)).date()
LIMIT = 500          # Gi·∫£m limit xu·ªëng 500 v√¨ news n·∫∑ng h∆°n posts
PROBE_LIMIT = 1
OFFSET_STEP = LIMIT
MAX_RETRY = 5
REQUEST_TIMEOUT = 20
MIN_DELAY = 0.5
MAX_DELAY = 1.5

# C·∫•u h√¨nh Logging
logging.basicConfig(
    level=logging.INFO,
    format="[%(asctime)s] %(levelname)s - %(message)s",
)
logger = logging.getLogger("news_crawler")

# =========================
# Helpers: API calls
# =========================
def _headers() -> Dict[str, str]:
    return {
        "Authorization": f"Bearer {AUTH_BEARER}",
        "User-Agent": "Mozilla/5.0",
        "Accept": "application/json",
    }

def api_get(url: str, params: Dict[str, object], max_retry: int = MAX_RETRY) -> Optional[List[Dict]]:
    """G·ªçi API c√≥ retry"""
    retry = 0
    backoff = 1.0
    while retry < max_retry:
        try:
            r = requests.get(url, headers=_headers(), params=params, timeout=REQUEST_TIMEOUT)
            if r.status_code == 200:
                return r.json()
            logger.warning(f"API {url} status={r.status_code}, retry={retry+1}/{max_retry}")
        except requests.RequestException as e:
            logger.warning(f"Request exception: {e} (retry {retry+1}/{max_retry})")

        time.sleep(backoff + random.uniform(0, 0.5))
        backoff *= 2
        retry += 1

    logger.error(f"Max retries reached for {url}")
    return None

# =========================
# Core crawl logic (SMART PROBE)
# =========================
def crawl_news(target_day) -> List[Dict]:
    """
    Crawl News (type=1) cho m·ªôt ng√†y c·ª• th·ªÉ.
    S·ª≠ d·ª•ng logic thƒÉm d√≤ ƒë·ªÉ nh·∫£y c√≥c qua c√°c ng√†y m·ªõi h∆°n.
    """
    # 1. Chu·∫©n h√≥a ng√†y th√°ng
    if isinstance(target_day, str):
        try:
            target_day = datetime.strptime(target_day, "%Y-%m-%d").date()
        except ValueError:
            logger.error(f"‚ùå ƒê·ªãnh d·∫°ng ng√†y kh√¥ng h·ª£p l·ªá: {target_day}")
            return []
            
    if target_day is None:
        target_day = TARGET_DAY_DEFAULT
        
    # 2. X√°c ƒë·ªãnh khung th·ªùi gian (Time Window UTC)
    day_start = datetime.combine(target_day, datetime.min.time()).replace(tzinfo=timezone.utc)
    day_end = day_start + timedelta(days=1)
    
    logger.info(f"üöÄ B·∫Øt ƒë·∫ßu crawl News cho ng√†y: {target_day} (Window UTC: {day_start} -> {day_end})")
    
    offset = 0
    collected_posts: List[Dict] = []
    
    while True:
        # --- B∆∞·ªõc A: ThƒÉm d√≤ (Probe) ---
        # type=1 l√† News (Tin t·ª©c)
        probe_params = {"type": 1, "offset": offset, "limit": PROBE_LIMIT}
        probe = api_get(API_URL, probe_params)
        
        if probe is None: break
        if len(probe) == 0:
            logger.info("Probe r·ªóng -> H·∫øt d·ªØ li·ªáu -> D·ª´ng.")
            break
            
        try:
            probe_date = parser.isoparse(probe[0].get("date"))
        except:
            logger.warning("L·ªói parse date b√†i probe -> B·ªè qua batch.")
            offset += OFFSET_STEP
            continue

        # --- B∆∞·ªõc B: Quy·∫øt ƒë·ªãnh ---
        
        # 1. N·∫øu tin c√≤n m·ªõi qu√° (T∆∞∆°ng lai) -> Nh·∫£y c√≥c
        if probe_date >= day_end:
            # logger.info(f"ƒêang ·ªü v√πng tin m·ªõi ({probe_date}) -> Nh·∫£y offset...")
            offset += OFFSET_STEP
            time.sleep(random.uniform(MIN_DELAY, 0.5))
            continue
            
        # 2. N·∫øu tin ƒë√£ c≈© qu√° (Qu√° kh·ª©) -> D·ª´ng
        if probe_date < day_start:
            logger.info(f"ƒê√£ ch·∫°m v√πng tin c≈© ({probe_date}) -> D·ª´ng crawl.")
            break
            
        # 3. N·∫øu ƒë√∫ng v√πng c·∫ßn l·∫•y -> T·∫£i chi ti·∫øt
        # logger.info(f"üéØ T√¨m th·∫•y tin m·ª•c ti√™u. T·∫£i batch t·∫°i offset {offset}...")
        
        batch_params = {"type": 1, "offset": offset, "limit": LIMIT}
        batch = api_get(API_URL, batch_params)
        
        if not batch: break
        
        items_in_batch = 0
        for p in batch:
            try:
                post_date = parser.isoparse(p.get("date"))
            except:
                continue
                
            # L·ªçc tin trong batch (v√¨ batch c√≥ th·ªÉ ch·ª©a l·∫´n l·ªôn)
            if post_date >= day_end: continue
            if post_date < day_start: 
                # V√¨ ƒë√£ sort gi·∫£m d·∫ßn, g·∫∑p tin c≈© l√† d·ª´ng lu√¥n c·∫£ h√†m
                logger.info(f"G·∫∑p tin c≈© trong batch ({post_date}) -> Ho√†n t·∫•t.")
                return collected_posts

            # --- QUAN TR·ªåNG: L·∫•y chi ti·∫øt b√†i b√°o ---
            # Tin t·ª©c th∆∞·ªùng b·ªã c·∫Øt ng·∫Øn ·ªü danh s√°ch, c·∫ßn g·ªçi API detail
            post_id = p.get("postID")
            if post_id:
                detail = api_get(f"{API_URL}/{post_id}", {})
                if detail:
                    collected_posts.append(detail)
                    items_in_batch += 1
                    
        logger.info(f"   + ƒê√£ l·∫•y {items_in_batch} tin chi ti·∫øt t·ª´ batch (Offset: {offset})")
        
        offset += OFFSET_STEP
        time.sleep(random.uniform(MIN_DELAY, MAX_DELAY))

    logger.info(f"‚úÖ Ho√†n th√†nh crawl News. T·ªïng s·ªë tin: {len(collected_posts)}")
    return collected_posts

# =========================
# Processing & Saving
# =========================
def process_post(post: Dict) -> Optional[Dict]:
    """L√†m s·∫°ch d·ªØ li·ªáu tin t·ª©c"""
    try:
        postID = post.get("postID")
        if not postID: return None
        
        # Clean Date
        date_raw = post.get("date")
        try:
            date_str = parser.isoparse(date_raw).isoformat() if date_raw else None
        except:
            date_str = date_raw
            
        # Clean Symbols
        tagged_symbols = post.get("taggedSymbols")
        if not isinstance(tagged_symbols, list):
            cleaned_symbols = []
        else:
            cleaned_symbols = [sym['symbol'] for sym in tagged_symbols if isinstance(sym, dict) and 'symbol' in sym]
            
        return {
            "postID": postID,
            "date": date_str,
            "title": post.get("title"),
            "description": post.get("description"),
            "originalContent": post.get("content"), # Content tin t·ª©c
            "sentiment": post.get("sentiment"),
            "taggedSymbols": cleaned_symbols,
            "totalLikes": post.get("totalLikes", 0),
            "totalReplies": post.get("totalReplies", 0),
            "totalShares": post.get("totalShares", 0),
        }
    except Exception:
        return None

def save_news_to_db(processed: List[Dict]):
    """L∆∞u v√†o DB th√¥ng qua preprocess_news_df"""
    if not processed: return
    
    df = pd.DataFrame(processed)
    # H√†m n√†y trong data_preprocessing.py ƒë√£ c√≥ logic l∆∞u v√†o collection 'news' v√† 'processed_news'
    preprocess_news_df(df)

# =========================
# Main Entry Point
# =========================
def main_news_crawling(target_day=None):
    """H√†m g·ªçi ch√≠nh"""
    raw_posts = crawl_news(target_day)
    
    if not raw_posts:
        logger.warning("Kh√¥ng t√¨m th·∫•y tin t·ª©c n√†o.")
        return

    logger.info("ƒêang x·ª≠ l√Ω d·ªØ li·ªáu tin t·ª©c...")
    processed_list = []
    seen_ids = set()
    
    for p in raw_posts:
        out = process_post(p)
        if out:
            pid = out.get("postID")
            if pid not in seen_ids:
                seen_ids.add(pid)
                processed_list.append(out)
            
    if processed_list:
        logger.info(f"L∆∞u {len(processed_list)} tin t·ª©c v√†o Database...")
        save_news_to_db(processed_list)
    else:
        logger.warning("Kh√¥ng c√≥ tin t·ª©c h·ª£p l·ªá sau khi x·ª≠ l√Ω.")

if __name__ == "__main__":
    main_news_crawling()